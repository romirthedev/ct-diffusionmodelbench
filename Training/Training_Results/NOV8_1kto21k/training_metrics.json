[
  {
    "step": 10,
    "epoch": 0.004425757911042266,
    "loss": 1.2934,
    "grad_norm": 13.5625,
    "learning_rate": 9e-06
  },
  {
    "step": 20,
    "epoch": 0.008851515822084531,
    "loss": 0.9158,
    "grad_norm": 6.40625,
    "learning_rate": 1.9e-05
  },
  {
    "step": 30,
    "epoch": 0.013277273733126798,
    "loss": 0.5775,
    "grad_norm": 6.5,
    "learning_rate": 2.9e-05
  },
  {
    "step": 40,
    "epoch": 0.017703031644169063,
    "loss": 0.6717,
    "grad_norm": 6.78125,
    "learning_rate": 3.9000000000000006e-05
  },
  {
    "step": 50,
    "epoch": 0.02212878955521133,
    "loss": 0.669,
    "grad_norm": 1.75,
    "learning_rate": 4.9e-05
  },
  {
    "step": 60,
    "epoch": 0.026554547466253596,
    "loss": 0.4828,
    "grad_norm": 3.859375,
    "learning_rate": 4.999992104320636e-05
  },
  {
    "step": 70,
    "epoch": 0.03098030537729586,
    "loss": 0.6765,
    "grad_norm": 3.4375,
    "learning_rate": 4.999964810678219e-05
  },
  {
    "step": 80,
    "epoch": 0.035406063288338126,
    "loss": 0.8503,
    "grad_norm": 3.25,
    "learning_rate": 4.999918021808019e-05
  },
  {
    "step": 90,
    "epoch": 0.03983182119938039,
    "loss": 0.6825,
    "grad_norm": 3.21875,
    "learning_rate": 4.999851738074904e-05
  },
  {
    "step": 100,
    "epoch": 0.04425757911042266,
    "loss": 0.6117,
    "grad_norm": 4.0,
    "learning_rate": 4.999765959995769e-05
  },
  {
    "step": 110,
    "epoch": 0.04868333702146493,
    "loss": 0.5297,
    "grad_norm": 4.84375,
    "learning_rate": 4.999660688239527e-05
  },
  {
    "step": 120,
    "epoch": 0.05310909493250719,
    "loss": 0.6454,
    "grad_norm": 0.7578125,
    "learning_rate": 4.999535923627109e-05
  },
  {
    "step": 130,
    "epoch": 0.05753485284354946,
    "loss": 0.6737,
    "grad_norm": 4.1875,
    "learning_rate": 4.999391667131455e-05
  },
  {
    "step": 140,
    "epoch": 0.06196061075459172,
    "loss": 1.4242,
    "grad_norm": 3.09375,
    "learning_rate": 4.999227919877506e-05
  },
  {
    "step": 150,
    "epoch": 0.066386368665634,
    "loss": 0.3733,
    "grad_norm": 1.7421875,
    "learning_rate": 4.9990446831421955e-05
  },
  {
    "step": 160,
    "epoch": 0.07081212657667625,
    "loss": 0.911,
    "grad_norm": 4.84375,
    "learning_rate": 4.99884195835444e-05
  },
  {
    "step": 170,
    "epoch": 0.07523788448771852,
    "loss": 0.6132,
    "grad_norm": 38.75,
    "learning_rate": 4.998619747095129e-05
  },
  {
    "step": 180,
    "epoch": 0.07966364239876078,
    "loss": 0.4411,
    "grad_norm": 3.375,
    "learning_rate": 4.998378051097111e-05
  },
  {
    "step": 190,
    "epoch": 0.08408940030980305,
    "loss": 0.7032,
    "grad_norm": 0.4140625,
    "learning_rate": 4.9981168722451776e-05
  },
  {
    "step": 200,
    "epoch": 0.08851515822084532,
    "loss": 0.945,
    "grad_norm": 3.71875,
    "learning_rate": 4.997836212576057e-05
  },
  {
    "step": 210,
    "epoch": 0.09294091613188758,
    "loss": 0.6697,
    "grad_norm": 3.375,
    "learning_rate": 4.997536074278387e-05
  },
  {
    "step": 220,
    "epoch": 0.09736667404292985,
    "loss": 0.8257,
    "grad_norm": 2.046875,
    "learning_rate": 4.997216459692709e-05
  },
  {
    "step": 230,
    "epoch": 0.10179243195397211,
    "loss": 0.7429,
    "grad_norm": 4.03125,
    "learning_rate": 4.996877371311439e-05
  },
  {
    "step": 240,
    "epoch": 0.10621818986501438,
    "loss": 0.9043,
    "grad_norm": 6.90625,
    "learning_rate": 4.996518811778858e-05
  },
  {
    "step": 250,
    "epoch": 0.11064394777605666,
    "loss": 0.6731,
    "grad_norm": 1.7578125,
    "learning_rate": 4.996140783891085e-05
  },
  {
    "step": 260,
    "epoch": 0.11506970568709891,
    "loss": 0.7399,
    "grad_norm": 4.34375,
    "learning_rate": 4.995743290596057e-05
  },
  {
    "step": 270,
    "epoch": 0.11949546359814119,
    "loss": 0.614,
    "grad_norm": 4.21875,
    "learning_rate": 4.9953263349935074e-05
  },
  {
    "step": 280,
    "epoch": 0.12392122150918344,
    "loss": 0.6552,
    "grad_norm": 3.09375,
    "learning_rate": 4.9948899203349384e-05
  },
  {
    "step": 290,
    "epoch": 0.12834697942022572,
    "loss": 0.4627,
    "grad_norm": 1.875,
    "learning_rate": 4.994434050023601e-05
  },
  {
    "step": 300,
    "epoch": 0.132772737331268,
    "loss": 0.5967,
    "grad_norm": 2.703125,
    "learning_rate": 4.9939587276144616e-05
  },
  {
    "step": 310,
    "epoch": 0.13719849524231026,
    "loss": 0.6033,
    "grad_norm": 2.59375,
    "learning_rate": 4.993463956814181e-05
  },
  {
    "step": 320,
    "epoch": 0.1416242531533525,
    "loss": 0.8968,
    "grad_norm": 3.0,
    "learning_rate": 4.99294974148108e-05
  },
  {
    "step": 330,
    "epoch": 0.14605001106439477,
    "loss": 0.5832,
    "grad_norm": 3.90625,
    "learning_rate": 4.992416085625115e-05
  },
  {
    "step": 340,
    "epoch": 0.15047576897543705,
    "loss": 0.6253,
    "grad_norm": 5.09375,
    "learning_rate": 4.99186299340784e-05
  },
  {
    "step": 350,
    "epoch": 0.15490152688647932,
    "loss": 0.7126,
    "grad_norm": 4.875,
    "learning_rate": 4.99129046914238e-05
  },
  {
    "step": 360,
    "epoch": 0.15932728479752156,
    "loss": 0.3637,
    "grad_norm": 1.9453125,
    "learning_rate": 4.990698517293395e-05
  },
  {
    "step": 370,
    "epoch": 0.16375304270856383,
    "loss": 0.9688,
    "grad_norm": 4.125,
    "learning_rate": 4.9900871424770424e-05
  },
  {
    "step": 380,
    "epoch": 0.1681788006196061,
    "loss": 0.5631,
    "grad_norm": 3.40625,
    "learning_rate": 4.989456349460947e-05
  },
  {
    "step": 390,
    "epoch": 0.17260455853064838,
    "loss": 0.5734,
    "grad_norm": 5.65625,
    "learning_rate": 4.988806143164159e-05
  },
  {
    "step": 400,
    "epoch": 0.17703031644169065,
    "loss": 0.4508,
    "grad_norm": 5.65625,
    "learning_rate": 4.988136528657118e-05
  },
  {
    "step": 410,
    "epoch": 0.1814560743527329,
    "loss": 0.6466,
    "grad_norm": 3.234375,
    "learning_rate": 4.987447511161612e-05
  },
  {
    "step": 420,
    "epoch": 0.18588183226377517,
    "loss": 0.5995,
    "grad_norm": 3.09375,
    "learning_rate": 4.98673909605074e-05
  },
  {
    "step": 430,
    "epoch": 0.19030759017481744,
    "loss": 0.6089,
    "grad_norm": 3.859375,
    "learning_rate": 4.986011288848863e-05
  },
  {
    "step": 440,
    "epoch": 0.1947333480858597,
    "loss": 0.4396,
    "grad_norm": 3.34375,
    "learning_rate": 4.9852640952315674e-05
  },
  {
    "step": 450,
    "epoch": 0.19915910599690198,
    "loss": 0.4306,
    "grad_norm": 2.5,
    "learning_rate": 4.9844975210256217e-05
  },
  {
    "step": 460,
    "epoch": 0.20358486390794422,
    "loss": 1.1133,
    "grad_norm": 2.6875,
    "learning_rate": 4.983711572208924e-05
  },
  {
    "step": 470,
    "epoch": 0.2080106218189865,
    "loss": 0.4944,
    "grad_norm": 4.375,
    "learning_rate": 4.982906254910459e-05
  },
  {
    "step": 480,
    "epoch": 0.21243637973002877,
    "loss": 0.6098,
    "grad_norm": 6.90625,
    "learning_rate": 4.982081575410256e-05
  },
  {
    "step": 490,
    "epoch": 0.21686213764107104,
    "loss": 0.5146,
    "grad_norm": 2.984375,
    "learning_rate": 4.981237540139331e-05
  },
  {
    "step": 500,
    "epoch": 0.2212878955521133,
    "loss": 0.9437,
    "grad_norm": 4.375,
    "learning_rate": 4.980374155679639e-05
  },
  {
    "step": 510,
    "epoch": 0.22571365346315556,
    "loss": 0.6195,
    "grad_norm": 4.78125,
    "learning_rate": 4.979491428764026e-05
  },
  {
    "step": 520,
    "epoch": 0.23013941137419783,
    "loss": 0.7266,
    "grad_norm": 2.984375,
    "learning_rate": 4.978589366276174e-05
  },
  {
    "step": 530,
    "epoch": 0.2345651692852401,
    "loss": 0.4264,
    "grad_norm": 2.78125,
    "learning_rate": 4.9776679752505476e-05
  },
  {
    "step": 540,
    "epoch": 0.23899092719628237,
    "loss": 0.6434,
    "grad_norm": 3.328125,
    "learning_rate": 4.9767272628723396e-05
  },
  {
    "step": 550,
    "epoch": 0.24341668510732464,
    "loss": 0.6188,
    "grad_norm": 2.53125,
    "learning_rate": 4.975767236477413e-05
  },
  {
    "step": 560,
    "epoch": 0.2478424430183669,
    "loss": 0.8973,
    "grad_norm": 3.75,
    "learning_rate": 4.974787903552247e-05
  },
  {
    "step": 570,
    "epoch": 0.25226820092940916,
    "loss": 0.2785,
    "grad_norm": 2.140625,
    "learning_rate": 4.9737892717338774e-05
  },
  {
    "step": 580,
    "epoch": 0.25669395884045143,
    "loss": 0.827,
    "grad_norm": 17.875,
    "learning_rate": 4.9727713488098335e-05
  },
  {
    "step": 590,
    "epoch": 0.2611197167514937,
    "loss": 0.8517,
    "grad_norm": 1.890625,
    "learning_rate": 4.971734142718085e-05
  },
  {
    "step": 600,
    "epoch": 0.265545474662536,
    "loss": 0.5262,
    "grad_norm": 3.734375,
    "learning_rate": 4.9706776615469716e-05
  },
  {
    "step": 610,
    "epoch": 0.26997123257357825,
    "loss": 0.5274,
    "grad_norm": 1.7265625,
    "learning_rate": 4.969601913535148e-05
  },
  {
    "step": 620,
    "epoch": 0.2743969904846205,
    "loss": 0.4583,
    "grad_norm": 3.515625,
    "learning_rate": 4.9685069070715106e-05
  },
  {
    "step": 630,
    "epoch": 0.27882274839566273,
    "loss": 0.3668,
    "grad_norm": 2.9375,
    "learning_rate": 4.9673926506951404e-05
  },
  {
    "step": 640,
    "epoch": 0.283248506306705,
    "loss": 0.772,
    "grad_norm": 4.375,
    "learning_rate": 4.966259153095235e-05
  },
  {
    "step": 650,
    "epoch": 0.2876742642177473,
    "loss": 0.4722,
    "grad_norm": 2.703125,
    "learning_rate": 4.965106423111033e-05
  },
  {
    "step": 660,
    "epoch": 0.29210002212878955,
    "loss": 0.9055,
    "grad_norm": 3.515625,
    "learning_rate": 4.963934469731756e-05
  },
  {
    "step": 670,
    "epoch": 0.2965257800398318,
    "loss": 0.783,
    "grad_norm": 4.9375,
    "learning_rate": 4.9627433020965314e-05
  },
  {
    "step": 680,
    "epoch": 0.3009515379508741,
    "loss": 0.6873,
    "grad_norm": 3.203125,
    "learning_rate": 4.961532929494325e-05
  },
  {
    "step": 690,
    "epoch": 0.30537729586191636,
    "loss": 0.5395,
    "grad_norm": 2.15625,
    "learning_rate": 4.9603033613638626e-05
  },
  {
    "step": 700,
    "epoch": 0.30980305377295864,
    "loss": 0.503,
    "grad_norm": 1.140625,
    "learning_rate": 4.959054607293567e-05
  },
  {
    "step": 710,
    "epoch": 0.3142288116840009,
    "loss": 0.5976,
    "grad_norm": 6.78125,
    "learning_rate": 4.957786677021471e-05
  },
  {
    "step": 720,
    "epoch": 0.3186545695950431,
    "loss": 0.7938,
    "grad_norm": 5.125,
    "learning_rate": 4.95649958043515e-05
  },
  {
    "step": 730,
    "epoch": 0.3230803275060854,
    "loss": 0.5677,
    "grad_norm": 0.98828125,
    "learning_rate": 4.955193327571642e-05
  },
  {
    "step": 740,
    "epoch": 0.32750608541712767,
    "loss": 0.6532,
    "grad_norm": 2.328125,
    "learning_rate": 4.9538679286173696e-05
  },
  {
    "step": 750,
    "epoch": 0.33193184332816994,
    "loss": 0.5648,
    "grad_norm": 3.953125,
    "learning_rate": 4.952523393908059e-05
  },
  {
    "step": 760,
    "epoch": 0.3363576012392122,
    "loss": 1.1309,
    "grad_norm": 4.65625,
    "learning_rate": 4.951159733928663e-05
  },
  {
    "step": 770,
    "epoch": 0.3407833591502545,
    "loss": 0.5138,
    "grad_norm": 1.625,
    "learning_rate": 4.949776959313275e-05
  },
  {
    "step": 780,
    "epoch": 0.34520911706129676,
    "loss": 0.7102,
    "grad_norm": 1.5390625,
    "learning_rate": 4.94837508084505e-05
  },
  {
    "step": 790,
    "epoch": 0.349634874972339,
    "loss": 0.5397,
    "grad_norm": 3.40625,
    "learning_rate": 4.946954109456118e-05
  },
  {
    "step": 800,
    "epoch": 0.3540606328833813,
    "loss": 0.5821,
    "grad_norm": 5.34375,
    "learning_rate": 4.9455140562274995e-05
  },
  {
    "step": 810,
    "epoch": 0.35848639079442357,
    "loss": 0.5712,
    "grad_norm": 4.5,
    "learning_rate": 4.9440549323890176e-05
  },
  {
    "step": 820,
    "epoch": 0.3629121487054658,
    "loss": 0.6578,
    "grad_norm": 1.8359375,
    "learning_rate": 4.9425767493192144e-05
  },
  {
    "step": 830,
    "epoch": 0.36733790661650806,
    "loss": 0.7413,
    "grad_norm": 4.0625,
    "learning_rate": 4.941079518545258e-05
  },
  {
    "step": 840,
    "epoch": 0.37176366452755033,
    "loss": 0.9012,
    "grad_norm": 2.6875,
    "learning_rate": 4.939563251742855e-05
  },
  {
    "step": 850,
    "epoch": 0.3761894224385926,
    "loss": 0.8828,
    "grad_norm": 2.5625,
    "learning_rate": 4.9380279607361575e-05
  },
  {
    "step": 860,
    "epoch": 0.3806151803496349,
    "loss": 0.6084,
    "grad_norm": 7.125,
    "learning_rate": 4.9364736574976736e-05
  },
  {
    "step": 870,
    "epoch": 0.38504093826067715,
    "loss": 0.6381,
    "grad_norm": 2.984375,
    "learning_rate": 4.934900354148173e-05
  },
  {
    "step": 880,
    "epoch": 0.3894666961717194,
    "loss": 0.6026,
    "grad_norm": 4.09375,
    "learning_rate": 4.933308062956591e-05
  },
  {
    "step": 890,
    "epoch": 0.3938924540827617,
    "loss": 0.691,
    "grad_norm": 0.60546875,
    "learning_rate": 4.9316967963399335e-05
  },
  {
    "step": 900,
    "epoch": 0.39831821199380396,
    "loss": 0.3295,
    "grad_norm": 1.6015625,
    "learning_rate": 4.930066566863182e-05
  },
  {
    "step": 910,
    "epoch": 0.4027439699048462,
    "loss": 0.4376,
    "grad_norm": 1.984375,
    "learning_rate": 4.9284173872391925e-05
  },
  {
    "step": 920,
    "epoch": 0.40716972781588845,
    "loss": 0.5539,
    "grad_norm": 2.1875,
    "learning_rate": 4.9267492703286e-05
  },
  {
    "step": 930,
    "epoch": 0.4115954857269307,
    "loss": 0.6525,
    "grad_norm": 3.96875,
    "learning_rate": 4.925062229139714e-05
  },
  {
    "step": 940,
    "epoch": 0.416021243637973,
    "loss": 0.7617,
    "grad_norm": 4.4375,
    "learning_rate": 4.9233562768284225e-05
  },
  {
    "step": 950,
    "epoch": 0.42044700154901526,
    "loss": 0.6689,
    "grad_norm": 4.84375,
    "learning_rate": 4.9216314266980824e-05
  },
  {
    "step": 960,
    "epoch": 0.42487275946005754,
    "loss": 0.4037,
    "grad_norm": 2.015625,
    "learning_rate": 4.919887692199423e-05
  },
  {
    "step": 970,
    "epoch": 0.4292985173710998,
    "loss": 0.3785,
    "grad_norm": 1.9453125,
    "learning_rate": 4.918125086930435e-05
  },
  {
    "step": 980,
    "epoch": 0.4337242752821421,
    "loss": 0.7043,
    "grad_norm": 2.515625,
    "learning_rate": 4.916343624636269e-05
  },
  {
    "step": 990,
    "epoch": 0.43815003319318435,
    "loss": 0.8222,
    "grad_norm": 88.5,
    "learning_rate": 4.914543319209126e-05
  },
  {
    "step": 1000,
    "epoch": 0.4425757911042266,
    "loss": 0.6712,
    "grad_norm": 0.6015625,
    "learning_rate": 4.912724184688149e-05
  },
  {
    "step": 1000,
    "epoch": 0.4425757911042266,
    "eval_loss": 0.6048827171325684,
    "eval_runtime": 102.7582,
    "eval_samples_per_second": 10.997,
    "eval_steps_per_second": 10.997
  },
  {
    "step": 1010,
    "epoch": 0.44700154901526884,
    "loss": 0.934,
    "grad_norm": 5.125,
    "learning_rate": 4.910886235259314e-05
  },
  {
    "step": 1020,
    "epoch": 0.4514273069263111,
    "loss": 0.9106,
    "grad_norm": 1.5234375,
    "learning_rate": 4.909029485255321e-05
  },
  {
    "step": 1030,
    "epoch": 0.4558530648373534,
    "loss": 0.5588,
    "grad_norm": 2.53125,
    "learning_rate": 4.907153949155479e-05
  },
  {
    "step": 1040,
    "epoch": 0.46027882274839566,
    "loss": 0.8757,
    "grad_norm": 2.375,
    "learning_rate": 4.905259641585594e-05
  },
  {
    "step": 1050,
    "epoch": 0.4647045806594379,
    "loss": 0.6782,
    "grad_norm": 4.78125,
    "learning_rate": 4.903346577317859e-05
  },
  {
    "step": 1060,
    "epoch": 0.4691303385704802,
    "loss": 0.8556,
    "grad_norm": 2.609375,
    "learning_rate": 4.9014147712707316e-05
  },
  {
    "step": 1070,
    "epoch": 0.47355609648152247,
    "loss": 0.3929,
    "grad_norm": 2.609375,
    "learning_rate": 4.899464238508825e-05
  },
  {
    "step": 1080,
    "epoch": 0.47798185439256474,
    "loss": 0.665,
    "grad_norm": 3.15625,
    "learning_rate": 4.897494994242785e-05
  },
  {
    "step": 1090,
    "epoch": 0.482407612303607,
    "loss": 0.7532,
    "grad_norm": 3.4375,
    "learning_rate": 4.8955070538291735e-05
  },
  {
    "step": 1100,
    "epoch": 0.4868333702146493,
    "loss": 0.3301,
    "grad_norm": 1.578125,
    "learning_rate": 4.893500432770349e-05
  },
  {
    "step": 1110,
    "epoch": 0.4912591281256915,
    "loss": 0.6679,
    "grad_norm": 12.5625,
    "learning_rate": 4.891475146714347e-05
  },
  {
    "step": 1120,
    "epoch": 0.4956848860367338,
    "loss": 1.1315,
    "grad_norm": 7.96875,
    "learning_rate": 4.8894312114547535e-05
  },
  {
    "step": 1130,
    "epoch": 0.500110643947776,
    "loss": 0.6458,
    "grad_norm": 2.875,
    "learning_rate": 4.887368642930588e-05
  },
  {
    "step": 1140,
    "epoch": 0.5045364018588183,
    "loss": 0.6615,
    "grad_norm": 21.5,
    "learning_rate": 4.885287457226172e-05
  },
  {
    "step": 1150,
    "epoch": 0.5089621597698606,
    "loss": 0.7704,
    "grad_norm": 4.9375,
    "learning_rate": 4.88318767057101e-05
  },
  {
    "step": 1160,
    "epoch": 0.5133879176809029,
    "loss": 0.5789,
    "grad_norm": 3.75,
    "learning_rate": 4.881069299339662e-05
  },
  {
    "step": 1170,
    "epoch": 0.5178136755919451,
    "loss": 0.5492,
    "grad_norm": 0.546875,
    "learning_rate": 4.8789323600516104e-05
  },
  {
    "step": 1180,
    "epoch": 0.5222394335029874,
    "loss": 0.6929,
    "grad_norm": 7.96875,
    "learning_rate": 4.876776869371139e-05
  },
  {
    "step": 1190,
    "epoch": 0.5266651914140297,
    "loss": 0.9062,
    "grad_norm": 3.03125,
    "learning_rate": 4.8746028441071943e-05
  },
  {
    "step": 1200,
    "epoch": 0.531090949325072,
    "loss": 0.7186,
    "grad_norm": 5.75,
    "learning_rate": 4.872410301213265e-05
  },
  {
    "step": 1210,
    "epoch": 0.5355167072361142,
    "loss": 0.518,
    "grad_norm": 5.46875,
    "learning_rate": 4.87019925778724e-05
  },
  {
    "step": 1220,
    "epoch": 0.5399424651471565,
    "loss": 0.5368,
    "grad_norm": 1.9375,
    "learning_rate": 4.867969731071279e-05
  },
  {
    "step": 1230,
    "epoch": 0.5443682230581988,
    "loss": 0.7395,
    "grad_norm": 7.28125,
    "learning_rate": 4.86572173845168e-05
  },
  {
    "step": 1240,
    "epoch": 0.548793980969241,
    "loss": 0.8483,
    "grad_norm": 5.0625,
    "learning_rate": 4.8634552974587414e-05
  },
  {
    "step": 1250,
    "epoch": 0.5532197388802832,
    "loss": 0.4454,
    "grad_norm": 2.234375,
    "learning_rate": 4.861170425766625e-05
  },
  {
    "step": 1260,
    "epoch": 0.5576454967913255,
    "loss": 0.5222,
    "grad_norm": 4.25,
    "learning_rate": 4.858867141193219e-05
  },
  {
    "step": 1270,
    "epoch": 0.5620712547023677,
    "loss": 0.6353,
    "grad_norm": 9.3125,
    "learning_rate": 4.8565454617e-05
  },
  {
    "step": 1280,
    "epoch": 0.56649701261341,
    "loss": 0.5342,
    "grad_norm": 2.0,
    "learning_rate": 4.85420540539189e-05
  },
  {
    "step": 1290,
    "epoch": 0.5709227705244523,
    "loss": 0.4798,
    "grad_norm": 7.3125,
    "learning_rate": 4.851846990517118e-05
  },
  {
    "step": 1300,
    "epoch": 0.5753485284354946,
    "loss": 0.8402,
    "grad_norm": 3.59375,
    "learning_rate": 4.849470235467078e-05
  },
  {
    "step": 1310,
    "epoch": 0.5797742863465368,
    "loss": 0.4068,
    "grad_norm": 1.703125,
    "learning_rate": 4.847075158776183e-05
  },
  {
    "step": 1320,
    "epoch": 0.5842000442575791,
    "loss": 0.4868,
    "grad_norm": 3.9375,
    "learning_rate": 4.844661779121723e-05
  },
  {
    "step": 1330,
    "epoch": 0.5886258021686214,
    "loss": 0.5585,
    "grad_norm": 4.6875,
    "learning_rate": 4.8422301153237145e-05
  },
  {
    "step": 1340,
    "epoch": 0.5930515600796636,
    "loss": 0.8546,
    "grad_norm": 13.875,
    "learning_rate": 4.8397801863447635e-05
  },
  {
    "step": 1350,
    "epoch": 0.5974773179907059,
    "loss": 0.6543,
    "grad_norm": 3.78125,
    "learning_rate": 4.837312011289907e-05
  },
  {
    "step": 1360,
    "epoch": 0.6019030759017482,
    "loss": 0.6184,
    "grad_norm": 1.4453125,
    "learning_rate": 4.8348256094064695e-05
  },
  {
    "step": 1370,
    "epoch": 0.6063288338127905,
    "loss": 0.3177,
    "grad_norm": 1.4921875,
    "learning_rate": 4.8323210000839124e-05
  },
  {
    "step": 1380,
    "epoch": 0.6107545917238327,
    "loss": 0.6066,
    "grad_norm": 7.15625,
    "learning_rate": 4.8297982028536826e-05
  },
  {
    "step": 1390,
    "epoch": 0.615180349634875,
    "loss": 1.2746,
    "grad_norm": 8.25,
    "learning_rate": 4.82725723738906e-05
  },
  {
    "step": 1400,
    "epoch": 0.6196061075459173,
    "loss": 1.1295,
    "grad_norm": 14.875,
    "learning_rate": 4.824698123505004e-05
  },
  {
    "step": 1410,
    "epoch": 0.6240318654569595,
    "loss": 0.6071,
    "grad_norm": 0.9921875,
    "learning_rate": 4.822120881157998e-05
  },
  {
    "step": 1420,
    "epoch": 0.6284576233680018,
    "loss": 0.726,
    "grad_norm": 4.84375,
    "learning_rate": 4.8195255304458945e-05
  },
  {
    "step": 1430,
    "epoch": 0.6328833812790441,
    "loss": 0.6274,
    "grad_norm": 2.828125,
    "learning_rate": 4.816912091607762e-05
  },
  {
    "step": 1440,
    "epoch": 0.6373091391900862,
    "loss": 0.479,
    "grad_norm": 9.0625,
    "learning_rate": 4.814280585023721e-05
  },
  {
    "step": 1450,
    "epoch": 0.6417348971011285,
    "loss": 0.7193,
    "grad_norm": 4.71875,
    "learning_rate": 4.811631031214786e-05
  },
  {
    "step": 1460,
    "epoch": 0.6461606550121708,
    "loss": 0.6733,
    "grad_norm": 1.3984375,
    "learning_rate": 4.808963450842713e-05
  },
  {
    "step": 1470,
    "epoch": 0.6505864129232131,
    "loss": 0.9938,
    "grad_norm": 1.6640625,
    "learning_rate": 4.8062778647098284e-05
  },
  {
    "step": 1480,
    "epoch": 0.6550121708342553,
    "loss": 0.7258,
    "grad_norm": 0.9453125,
    "learning_rate": 4.8035742937588724e-05
  },
  {
    "step": 1490,
    "epoch": 0.6594379287452976,
    "loss": 0.9739,
    "grad_norm": 3.40625,
    "learning_rate": 4.800852759072833e-05
  },
  {
    "step": 1500,
    "epoch": 0.6638636866563399,
    "loss": 0.238,
    "grad_norm": 20.25,
    "learning_rate": 4.7981132818747876e-05
  },
  {
    "step": 1510,
    "epoch": 0.6682894445673822,
    "loss": 0.5278,
    "grad_norm": 2.4375,
    "learning_rate": 4.795355883527727e-05
  },
  {
    "step": 1520,
    "epoch": 0.6727152024784244,
    "loss": 0.638,
    "grad_norm": 5.09375,
    "learning_rate": 4.7925805855343975e-05
  },
  {
    "step": 1530,
    "epoch": 0.6771409603894667,
    "loss": 0.883,
    "grad_norm": 6.53125,
    "learning_rate": 4.789787409537131e-05
  },
  {
    "step": 1540,
    "epoch": 0.681566718300509,
    "loss": 0.8303,
    "grad_norm": 3.671875,
    "learning_rate": 4.7869763773176756e-05
  },
  {
    "step": 1550,
    "epoch": 0.6859924762115512,
    "loss": 0.5889,
    "grad_norm": 1.296875,
    "learning_rate": 4.7841475107970244e-05
  },
  {
    "step": 1560,
    "epoch": 0.6904182341225935,
    "loss": 0.6439,
    "grad_norm": 3.03125,
    "learning_rate": 4.781300832035247e-05
  },
  {
    "step": 1570,
    "epoch": 0.6948439920336358,
    "loss": 0.873,
    "grad_norm": 5.6875,
    "learning_rate": 4.7784363632313166e-05
  },
  {
    "step": 1580,
    "epoch": 0.699269749944678,
    "loss": 0.5761,
    "grad_norm": 2.46875,
    "learning_rate": 4.775554126722935e-05
  },
  {
    "step": 1590,
    "epoch": 0.7036955078557203,
    "loss": 0.6526,
    "grad_norm": 3.515625,
    "learning_rate": 4.772654144986364e-05
  },
  {
    "step": 1600,
    "epoch": 0.7081212657667626,
    "loss": 0.5035,
    "grad_norm": 5.5,
    "learning_rate": 4.769736440636241e-05
  },
  {
    "step": 1610,
    "epoch": 0.7125470236778049,
    "loss": 0.4853,
    "grad_norm": 4.59375,
    "learning_rate": 4.7668010364254124e-05
  },
  {
    "step": 1620,
    "epoch": 0.7169727815888471,
    "loss": 0.4889,
    "grad_norm": 1.9375,
    "learning_rate": 4.763847955244749e-05
  },
  {
    "step": 1630,
    "epoch": 0.7213985394998893,
    "loss": 0.9764,
    "grad_norm": 5.125,
    "learning_rate": 4.760877220122971e-05
  },
  {
    "step": 1640,
    "epoch": 0.7258242974109316,
    "loss": 0.2523,
    "grad_norm": 2.484375,
    "learning_rate": 4.7578888542264686e-05
  },
  {
    "step": 1650,
    "epoch": 0.7302500553219738,
    "loss": 0.6957,
    "grad_norm": 1.6796875,
    "learning_rate": 4.7548828808591195e-05
  },
  {
    "step": 1660,
    "epoch": 0.7346758132330161,
    "loss": 0.6383,
    "grad_norm": 4.09375,
    "learning_rate": 4.7518593234621056e-05
  },
  {
    "step": 1670,
    "epoch": 0.7391015711440584,
    "loss": 0.4969,
    "grad_norm": 6.5,
    "learning_rate": 4.7488182056137374e-05
  },
  {
    "step": 1680,
    "epoch": 0.7435273290551007,
    "loss": 0.4216,
    "grad_norm": 2.109375,
    "learning_rate": 4.745759551029261e-05
  },
  {
    "step": 1690,
    "epoch": 0.7479530869661429,
    "loss": 0.3352,
    "grad_norm": 5.59375,
    "learning_rate": 4.7426833835606806e-05
  },
  {
    "step": 1700,
    "epoch": 0.7523788448771852,
    "loss": 0.4483,
    "grad_norm": 3.53125,
    "learning_rate": 4.739589727196568e-05
  },
  {
    "step": 1710,
    "epoch": 0.7568046027882275,
    "loss": 0.4669,
    "grad_norm": 3.828125,
    "learning_rate": 4.736478606061875e-05
  },
  {
    "step": 1720,
    "epoch": 0.7612303606992697,
    "loss": 0.4418,
    "grad_norm": 2.859375,
    "learning_rate": 4.733350044417752e-05
  },
  {
    "step": 1730,
    "epoch": 0.765656118610312,
    "loss": 0.6368,
    "grad_norm": 1.828125,
    "learning_rate": 4.730204066661349e-05
  },
  {
    "step": 1740,
    "epoch": 0.7700818765213543,
    "loss": 0.8759,
    "grad_norm": 6.78125,
    "learning_rate": 4.727040697325634e-05
  },
  {
    "step": 1750,
    "epoch": 0.7745076344323966,
    "loss": 0.3622,
    "grad_norm": 2.921875,
    "learning_rate": 4.723859961079195e-05
  },
  {
    "step": 1760,
    "epoch": 0.7789333923434388,
    "loss": 0.5866,
    "grad_norm": 2.65625,
    "learning_rate": 4.7206618827260534e-05
  },
  {
    "step": 1770,
    "epoch": 0.7833591502544811,
    "loss": 0.4646,
    "grad_norm": 4.5625,
    "learning_rate": 4.717446487205466e-05
  },
  {
    "step": 1780,
    "epoch": 0.7877849081655234,
    "loss": 0.4109,
    "grad_norm": 4.96875,
    "learning_rate": 4.7142137995917336e-05
  },
  {
    "step": 1790,
    "epoch": 0.7922106660765657,
    "loss": 0.5617,
    "grad_norm": 2.359375,
    "learning_rate": 4.710963845094003e-05
  },
  {
    "step": 1800,
    "epoch": 0.7966364239876079,
    "loss": 0.4069,
    "grad_norm": 0.19140625,
    "learning_rate": 4.707696649056073e-05
  },
  {
    "step": 1810,
    "epoch": 0.8010621818986502,
    "loss": 0.6146,
    "grad_norm": 3.703125,
    "learning_rate": 4.704412236956193e-05
  },
  {
    "step": 1820,
    "epoch": 0.8054879398096924,
    "loss": 0.5851,
    "grad_norm": 2.765625,
    "learning_rate": 4.70111063440687e-05
  },
  {
    "step": 1830,
    "epoch": 0.8099136977207346,
    "loss": 0.9108,
    "grad_norm": 4.5625,
    "learning_rate": 4.697791867154663e-05
  },
  {
    "step": 1840,
    "epoch": 0.8143394556317769,
    "loss": 0.5316,
    "grad_norm": 4.03125,
    "learning_rate": 4.694455961079987e-05
  },
  {
    "step": 1850,
    "epoch": 0.8187652135428192,
    "loss": 0.3788,
    "grad_norm": 2.296875,
    "learning_rate": 4.691102942196906e-05
  },
  {
    "step": 1860,
    "epoch": 0.8231909714538614,
    "loss": 0.4386,
    "grad_norm": 3.78125,
    "learning_rate": 4.6877328366529346e-05
  },
  {
    "step": 1870,
    "epoch": 0.8276167293649037,
    "loss": 0.5243,
    "grad_norm": 4.4375,
    "learning_rate": 4.684345670728834e-05
  },
  {
    "step": 1880,
    "epoch": 0.832042487275946,
    "loss": 0.7953,
    "grad_norm": 24.625,
    "learning_rate": 4.6809414708384046e-05
  },
  {
    "step": 1890,
    "epoch": 0.8364682451869883,
    "loss": 0.3892,
    "grad_norm": 2.75,
    "learning_rate": 4.67752026352828e-05
  },
  {
    "step": 1900,
    "epoch": 0.8408940030980305,
    "loss": 0.5241,
    "grad_norm": 1.0859375,
    "learning_rate": 4.6740820754777235e-05
  },
  {
    "step": 1910,
    "epoch": 0.8453197610090728,
    "loss": 0.4347,
    "grad_norm": 3.453125,
    "learning_rate": 4.670626933498415e-05
  },
  {
    "step": 1920,
    "epoch": 0.8497455189201151,
    "loss": 0.5706,
    "grad_norm": 1.84375,
    "learning_rate": 4.6671548645342456e-05
  },
  {
    "step": 1930,
    "epoch": 0.8541712768311573,
    "loss": 0.4447,
    "grad_norm": 0.90625,
    "learning_rate": 4.663665895661107e-05
  },
  {
    "step": 1940,
    "epoch": 0.8585970347421996,
    "loss": 0.4486,
    "grad_norm": 3.140625,
    "learning_rate": 4.6601600540866794e-05
  },
  {
    "step": 1950,
    "epoch": 0.8630227926532419,
    "loss": 0.79,
    "grad_norm": 4.625,
    "learning_rate": 4.6566373671502196e-05
  },
  {
    "step": 1960,
    "epoch": 0.8674485505642842,
    "loss": 0.8439,
    "grad_norm": 4.84375,
    "learning_rate": 4.653097862322348e-05
  },
  {
    "step": 1970,
    "epoch": 0.8718743084753264,
    "loss": 0.5448,
    "grad_norm": 4.40625,
    "learning_rate": 4.649541567204834e-05
  },
  {
    "step": 1980,
    "epoch": 0.8763000663863687,
    "loss": 0.84,
    "grad_norm": 10.0,
    "learning_rate": 4.645968509530381e-05
  },
  {
    "step": 1990,
    "epoch": 0.880725824297411,
    "loss": 0.2774,
    "grad_norm": 1.03125,
    "learning_rate": 4.6423787171624114e-05
  },
  {
    "step": 2000,
    "epoch": 0.8851515822084532,
    "loss": 0.467,
    "grad_norm": 3.671875,
    "learning_rate": 4.638772218094847e-05
  },
  {
    "step": 2000,
    "epoch": 0.8851515822084532,
    "eval_loss": 0.6056098937988281,
    "eval_runtime": 103.4997,
    "eval_samples_per_second": 10.918,
    "eval_steps_per_second": 10.918
  },
  {
    "step": 2010,
    "epoch": 0.8895773401194955,
    "loss": 0.8735,
    "grad_norm": 0.9375,
    "learning_rate": 4.635149040451891e-05
  },
  {
    "step": 2020,
    "epoch": 0.8940030980305377,
    "loss": 0.1465,
    "grad_norm": 2.578125,
    "learning_rate": 4.631509212487811e-05
  },
  {
    "step": 2030,
    "epoch": 0.89842885594158,
    "loss": 0.4885,
    "grad_norm": 8.5,
    "learning_rate": 4.627852762586718e-05
  },
  {
    "step": 2040,
    "epoch": 0.9028546138526222,
    "loss": 0.3318,
    "grad_norm": 1.4375,
    "learning_rate": 4.624179719262342e-05
  },
  {
    "step": 2050,
    "epoch": 0.9072803717636645,
    "loss": 0.3513,
    "grad_norm": 4.21875,
    "learning_rate": 4.62049011115781e-05
  },
  {
    "step": 2060,
    "epoch": 0.9117061296747068,
    "loss": 0.9292,
    "grad_norm": 5.40625,
    "learning_rate": 4.6167839670454315e-05
  },
  {
    "step": 2070,
    "epoch": 0.916131887585749,
    "loss": 0.45,
    "grad_norm": 7.09375,
    "learning_rate": 4.613061315826461e-05
  },
  {
    "step": 2080,
    "epoch": 0.9205576454967913,
    "loss": 0.3449,
    "grad_norm": 1.9765625,
    "learning_rate": 4.6093221865308786e-05
  },
  {
    "step": 2090,
    "epoch": 0.9249834034078336,
    "loss": 0.6496,
    "grad_norm": 2.234375,
    "learning_rate": 4.605566608317169e-05
  },
  {
    "step": 2100,
    "epoch": 0.9294091613188759,
    "loss": 0.4016,
    "grad_norm": 1.4453125,
    "learning_rate": 4.6017946104720836e-05
  },
  {
    "step": 2110,
    "epoch": 0.9338349192299181,
    "loss": 1.0338,
    "grad_norm": 7.25,
    "learning_rate": 4.598006222410419e-05
  },
  {
    "step": 2120,
    "epoch": 0.9382606771409604,
    "loss": 0.5119,
    "grad_norm": 3.453125,
    "learning_rate": 4.5942014736747875e-05
  },
  {
    "step": 2130,
    "epoch": 0.9426864350520027,
    "loss": 0.3036,
    "grad_norm": 1.3046875,
    "learning_rate": 4.590380393935383e-05
  },
  {
    "step": 2140,
    "epoch": 0.9471121929630449,
    "loss": 0.4973,
    "grad_norm": 2.890625,
    "learning_rate": 4.5865430129897536e-05
  },
  {
    "step": 2150,
    "epoch": 0.9515379508740872,
    "loss": 0.4948,
    "grad_norm": 3.546875,
    "learning_rate": 4.5826893607625665e-05
  },
  {
    "step": 2160,
    "epoch": 0.9559637087851295,
    "loss": 0.2886,
    "grad_norm": 1.0078125,
    "learning_rate": 4.5788194673053756e-05
  },
  {
    "step": 2170,
    "epoch": 0.9603894666961718,
    "loss": 0.5288,
    "grad_norm": 3.796875,
    "learning_rate": 4.5749333627963884e-05
  },
  {
    "step": 2180,
    "epoch": 0.964815224607214,
    "loss": 0.6451,
    "grad_norm": 1.0703125,
    "learning_rate": 4.5710310775402274e-05
  },
  {
    "step": 2190,
    "epoch": 0.9692409825182563,
    "loss": 0.6705,
    "grad_norm": 4.8125,
    "learning_rate": 4.567112641967697e-05
  },
  {
    "step": 2200,
    "epoch": 0.9736667404292986,
    "loss": 0.6843,
    "grad_norm": 5.78125,
    "learning_rate": 4.5631780866355454e-05
  },
  {
    "step": 2210,
    "epoch": 0.9780924983403407,
    "loss": 0.4023,
    "grad_norm": 1.5234375,
    "learning_rate": 4.559227442226226e-05
  },
  {
    "step": 2220,
    "epoch": 0.982518256251383,
    "loss": 0.731,
    "grad_norm": 2.203125,
    "learning_rate": 4.555260739547657e-05
  },
  {
    "step": 2230,
    "epoch": 0.9869440141624253,
    "loss": 0.5684,
    "grad_norm": 2.046875,
    "learning_rate": 4.551278009532981e-05
  },
  {
    "step": 2240,
    "epoch": 0.9913697720734675,
    "loss": 0.2852,
    "grad_norm": 6.125,
    "learning_rate": 4.547279283240329e-05
  },
  {
    "step": 2250,
    "epoch": 0.9957955299845098,
    "loss": 0.6621,
    "grad_norm": 1.8828125,
    "learning_rate": 4.543264591852572e-05
  },
  {
    "step": 2260,
    "epoch": 1.0,
    "loss": 0.8038,
    "grad_norm": 9.375,
    "learning_rate": 4.539233966677078e-05
  },
  {
    "step": 2270,
    "epoch": 1.0044257579110423,
    "loss": 0.364,
    "grad_norm": 2.46875,
    "learning_rate": 4.535187439145473e-05
  },
  {
    "step": 2280,
    "epoch": 1.0088515158220845,
    "loss": 0.6983,
    "grad_norm": 5.84375,
    "learning_rate": 4.531125040813392e-05
  },
  {
    "step": 2290,
    "epoch": 1.0132772737331268,
    "loss": 0.579,
    "grad_norm": 23.0,
    "learning_rate": 4.527046803360232e-05
  },
  {
    "step": 2300,
    "epoch": 1.017703031644169,
    "loss": 0.6394,
    "grad_norm": 14.3125,
    "learning_rate": 4.522952758588909e-05
  },
  {
    "step": 2310,
    "epoch": 1.0221287895552114,
    "loss": 0.4755,
    "grad_norm": 3.328125,
    "learning_rate": 4.518842938425605e-05
  },
  {
    "step": 2320,
    "epoch": 1.0265545474662536,
    "loss": 0.543,
    "grad_norm": 4.625,
    "learning_rate": 4.5147173749195255e-05
  },
  {
    "step": 2330,
    "epoch": 1.030980305377296,
    "loss": 0.6369,
    "grad_norm": 5.5,
    "learning_rate": 4.5105761002426415e-05
  },
  {
    "step": 2340,
    "epoch": 1.0354060632883382,
    "loss": 0.709,
    "grad_norm": 3.234375,
    "learning_rate": 4.506419146689446e-05
  },
  {
    "step": 2350,
    "epoch": 1.0398318211993804,
    "loss": 0.6051,
    "grad_norm": 2.59375,
    "learning_rate": 4.5022465466766974e-05
  },
  {
    "step": 2360,
    "epoch": 1.0442575791104227,
    "loss": 0.7338,
    "grad_norm": 6.34375,
    "learning_rate": 4.498058332743168e-05
  },
  {
    "step": 2370,
    "epoch": 1.048683337021465,
    "loss": 0.7736,
    "grad_norm": 4.21875,
    "learning_rate": 4.4938545375493934e-05
  },
  {
    "step": 2380,
    "epoch": 1.0531090949325073,
    "loss": 0.4327,
    "grad_norm": 2.859375,
    "learning_rate": 4.489635193877411e-05
  },
  {
    "step": 2390,
    "epoch": 1.0575348528435495,
    "loss": 0.389,
    "grad_norm": 6.21875,
    "learning_rate": 4.485400334630511e-05
  },
  {
    "step": 2400,
    "epoch": 1.0619606107545918,
    "loss": 0.6579,
    "grad_norm": 0.4921875,
    "learning_rate": 4.481149992832977e-05
  },
  {
    "step": 2410,
    "epoch": 1.066386368665634,
    "loss": 0.7082,
    "grad_norm": 4.5625,
    "learning_rate": 4.4768842016298275e-05
  },
  {
    "step": 2420,
    "epoch": 1.0708121265766763,
    "loss": 0.4693,
    "grad_norm": 5.75,
    "learning_rate": 4.472602994286559e-05
  },
  {
    "step": 2430,
    "epoch": 1.0752378844877186,
    "loss": 0.2945,
    "grad_norm": 0.0034942626953125,
    "learning_rate": 4.468306404188887e-05
  },
  {
    "step": 2440,
    "epoch": 1.079663642398761,
    "loss": 0.6822,
    "grad_norm": 4.09375,
    "learning_rate": 4.463994464842484e-05
  },
  {
    "step": 2450,
    "epoch": 1.0840894003098032,
    "loss": 0.4992,
    "grad_norm": 2.265625,
    "learning_rate": 4.4596672098727195e-05
  },
  {
    "step": 2460,
    "epoch": 1.0885151582208454,
    "loss": 1.2013,
    "grad_norm": 2.40625,
    "learning_rate": 4.455324673024396e-05
  },
  {
    "step": 2470,
    "epoch": 1.0929409161318875,
    "loss": 0.5499,
    "grad_norm": 3.34375,
    "learning_rate": 4.4509668881614894e-05
  },
  {
    "step": 2480,
    "epoch": 1.0973666740429298,
    "loss": 0.7199,
    "grad_norm": 2.59375,
    "learning_rate": 4.4465938892668814e-05
  },
  {
    "step": 2490,
    "epoch": 1.101792431953972,
    "loss": 0.5252,
    "grad_norm": 1.0703125,
    "learning_rate": 4.4422057104420946e-05
  },
  {
    "step": 2500,
    "epoch": 1.1062181898650143,
    "loss": 0.7761,
    "grad_norm": 4.84375,
    "learning_rate": 4.437802385907029e-05
  },
  {
    "step": 2510,
    "epoch": 1.1106439477760566,
    "loss": 0.7824,
    "grad_norm": 4.1875,
    "learning_rate": 4.4333839499996954e-05
  },
  {
    "step": 2520,
    "epoch": 1.1150697056870988,
    "loss": 0.8153,
    "grad_norm": 3.78125,
    "learning_rate": 4.428950437175944e-05
  },
  {
    "step": 2530,
    "epoch": 1.1194954635981411,
    "loss": 0.6121,
    "grad_norm": 2.40625,
    "learning_rate": 4.424501882009198e-05
  },
  {
    "step": 2540,
    "epoch": 1.1239212215091834,
    "loss": 0.2332,
    "grad_norm": 7.28125,
    "learning_rate": 4.420038319190184e-05
  },
  {
    "step": 2550,
    "epoch": 1.1283469794202257,
    "loss": 0.5476,
    "grad_norm": 3.375,
    "learning_rate": 4.4155597835266616e-05
  },
  {
    "step": 2560,
    "epoch": 1.132772737331268,
    "loss": 0.4489,
    "grad_norm": 2.890625,
    "learning_rate": 4.4110663099431514e-05
  },
  {
    "step": 2570,
    "epoch": 1.1371984952423102,
    "loss": 0.6577,
    "grad_norm": 6.25,
    "learning_rate": 4.406557933480664e-05
  },
  {
    "step": 2580,
    "epoch": 1.1416242531533525,
    "loss": 0.5985,
    "grad_norm": 4.90625,
    "learning_rate": 4.4020346892964246e-05
  },
  {
    "step": 2590,
    "epoch": 1.1460500110643947,
    "loss": 0.5548,
    "grad_norm": 3.71875,
    "learning_rate": 4.397496612663599e-05
  },
  {
    "step": 2600,
    "epoch": 1.150475768975437,
    "loss": 0.7675,
    "grad_norm": 11.0625,
    "learning_rate": 4.392943738971021e-05
  },
  {
    "step": 2610,
    "epoch": 1.1549015268864793,
    "loss": 0.4784,
    "grad_norm": 4.78125,
    "learning_rate": 4.3883761037229146e-05
  },
  {
    "step": 2620,
    "epoch": 1.1593272847975216,
    "loss": 0.7022,
    "grad_norm": 18.75,
    "learning_rate": 4.383793742538616e-05
  },
  {
    "step": 2630,
    "epoch": 1.1637530427085638,
    "loss": 0.6709,
    "grad_norm": 3.375,
    "learning_rate": 4.379196691152298e-05
  },
  {
    "step": 2640,
    "epoch": 1.168178800619606,
    "loss": 0.3498,
    "grad_norm": 3.46875,
    "learning_rate": 4.374584985412692e-05
  },
  {
    "step": 2650,
    "epoch": 1.1726045585306484,
    "loss": 0.5729,
    "grad_norm": 3.1875,
    "learning_rate": 4.369958661282805e-05
  },
  {
    "step": 2660,
    "epoch": 1.1770303164416906,
    "loss": 0.5509,
    "grad_norm": 2.1875,
    "learning_rate": 4.3653177548396426e-05
  },
  {
    "step": 2670,
    "epoch": 1.181456074352733,
    "loss": 0.4137,
    "grad_norm": 11.1875,
    "learning_rate": 4.360662302273925e-05
  },
  {
    "step": 2680,
    "epoch": 1.1858818322637752,
    "loss": 0.4661,
    "grad_norm": 1.703125,
    "learning_rate": 4.355992339889806e-05
  },
  {
    "step": 2690,
    "epoch": 1.1903075901748175,
    "loss": 0.5531,
    "grad_norm": 3.109375,
    "learning_rate": 4.351307904104592e-05
  },
  {
    "step": 2700,
    "epoch": 1.1947333480858597,
    "loss": 0.4484,
    "grad_norm": 4.34375,
    "learning_rate": 4.346609031448452e-05
  },
  {
    "step": 2710,
    "epoch": 1.199159105996902,
    "loss": 0.4897,
    "grad_norm": 2.859375,
    "learning_rate": 4.341895758564141e-05
  },
  {
    "step": 2720,
    "epoch": 1.2035848639079443,
    "loss": 0.5928,
    "grad_norm": 1.15625,
    "learning_rate": 4.337168122206706e-05
  },
  {
    "step": 2730,
    "epoch": 1.2080106218189866,
    "loss": 0.5365,
    "grad_norm": 10.0,
    "learning_rate": 4.3324261592432056e-05
  },
  {
    "step": 2740,
    "epoch": 1.2124363797300288,
    "loss": 0.8811,
    "grad_norm": 5.78125,
    "learning_rate": 4.327669906652421e-05
  },
  {
    "step": 2750,
    "epoch": 1.216862137641071,
    "loss": 0.4261,
    "grad_norm": 3.953125,
    "learning_rate": 4.322899401524563e-05
  },
  {
    "step": 2760,
    "epoch": 1.2212878955521134,
    "loss": 1.3695,
    "grad_norm": 0.97265625,
    "learning_rate": 4.31811468106099e-05
  },
  {
    "step": 2770,
    "epoch": 1.2257136534631556,
    "loss": 0.4347,
    "grad_norm": 2.40625,
    "learning_rate": 4.313315782573913e-05
  },
  {
    "step": 2780,
    "epoch": 1.230139411374198,
    "loss": 0.5816,
    "grad_norm": 3.140625,
    "learning_rate": 4.308502743486107e-05
  },
  {
    "step": 2790,
    "epoch": 1.2345651692852402,
    "loss": 0.7317,
    "grad_norm": 11.9375,
    "learning_rate": 4.303675601330618e-05
  },
  {
    "step": 2800,
    "epoch": 1.2389909271962825,
    "loss": 0.4792,
    "grad_norm": 1.65625,
    "learning_rate": 4.2988343937504686e-05
  },
  {
    "step": 2810,
    "epoch": 1.2434166851073247,
    "loss": 0.6145,
    "grad_norm": 2.25,
    "learning_rate": 4.293979158498369e-05
  },
  {
    "step": 2820,
    "epoch": 1.2478424430183668,
    "loss": 0.6163,
    "grad_norm": 5.53125,
    "learning_rate": 4.289109933436419e-05
  },
  {
    "step": 2830,
    "epoch": 1.252268200929409,
    "loss": 0.5319,
    "grad_norm": 3.125,
    "learning_rate": 4.284226756535814e-05
  },
  {
    "step": 2840,
    "epoch": 1.2566939588404513,
    "loss": 0.3309,
    "grad_norm": 2.421875,
    "learning_rate": 4.279329665876548e-05
  },
  {
    "step": 2850,
    "epoch": 1.2611197167514936,
    "loss": 0.3177,
    "grad_norm": 2.328125,
    "learning_rate": 4.2744186996471174e-05
  },
  {
    "step": 2860,
    "epoch": 1.2655454746625359,
    "loss": 0.6001,
    "grad_norm": 2.65625,
    "learning_rate": 4.269493896144224e-05
  },
  {
    "step": 2870,
    "epoch": 1.2699712325735781,
    "loss": 0.5942,
    "grad_norm": 3.28125,
    "learning_rate": 4.2645552937724744e-05
  },
  {
    "step": 2880,
    "epoch": 1.2743969904846204,
    "loss": 0.6795,
    "grad_norm": 23.125,
    "learning_rate": 4.2596029310440824e-05
  },
  {
    "step": 2890,
    "epoch": 1.2788227483956627,
    "loss": 0.2839,
    "grad_norm": 1.96875,
    "learning_rate": 4.254636846578566e-05
  },
  {
    "step": 2900,
    "epoch": 1.283248506306705,
    "loss": 0.4785,
    "grad_norm": 3.078125,
    "learning_rate": 4.2496570791024513e-05
  },
  {
    "step": 2910,
    "epoch": 1.2876742642177472,
    "loss": 0.3679,
    "grad_norm": 12.875,
    "learning_rate": 4.2446636674489645e-05
  },
  {
    "step": 2920,
    "epoch": 1.2921000221287895,
    "loss": 0.4351,
    "grad_norm": 1.328125,
    "learning_rate": 4.239656650557734e-05
  },
  {
    "step": 2930,
    "epoch": 1.2965257800398318,
    "loss": 0.7262,
    "grad_norm": 21.0,
    "learning_rate": 4.2346360674744815e-05
  },
  {
    "step": 2940,
    "epoch": 1.300951537950874,
    "loss": 0.467,
    "grad_norm": 3.40625,
    "learning_rate": 4.229601957350722e-05
  },
  {
    "step": 2950,
    "epoch": 1.3053772958619163,
    "loss": 0.4076,
    "grad_norm": 1.9296875,
    "learning_rate": 4.224554359443459e-05
  },
  {
    "step": 2960,
    "epoch": 1.3098030537729586,
    "loss": 0.6107,
    "grad_norm": 3.34375,
    "learning_rate": 4.219493313114875e-05
  },
  {
    "step": 2970,
    "epoch": 1.3142288116840009,
    "loss": 0.3492,
    "grad_norm": 3.25,
    "learning_rate": 4.214418857832025e-05
  },
  {
    "step": 2980,
    "epoch": 1.3186545695950431,
    "loss": 0.394,
    "grad_norm": 0.609375,
    "learning_rate": 4.209331033166531e-05
  },
  {
    "step": 2990,
    "epoch": 1.3230803275060854,
    "loss": 0.66,
    "grad_norm": 6.15625,
    "learning_rate": 4.204229878794273e-05
  },
  {
    "step": 3000,
    "epoch": 1.3275060854171277,
    "loss": 0.3201,
    "grad_norm": 1.578125,
    "learning_rate": 4.199115434495076e-05
  },
  {
    "step": 3000,
    "epoch": 1.3275060854171277,
    "eval_loss": 0.5453869104385376,
    "eval_runtime": 104.4117,
    "eval_samples_per_second": 10.823,
    "eval_steps_per_second": 10.823
  },
  {
    "step": 3010,
    "epoch": 1.33193184332817,
    "loss": 0.276,
    "grad_norm": 3.953125,
    "learning_rate": 4.193987740152404e-05
  },
  {
    "step": 3020,
    "epoch": 1.3363576012392122,
    "loss": 0.5858,
    "grad_norm": 4.53125,
    "learning_rate": 4.1888468357530476e-05
  },
  {
    "step": 3030,
    "epoch": 1.3407833591502545,
    "loss": 0.6035,
    "grad_norm": 6.28125,
    "learning_rate": 4.183692761386813e-05
  },
  {
    "step": 3040,
    "epoch": 1.3452091170612968,
    "loss": 0.4759,
    "grad_norm": 9.5625,
    "learning_rate": 4.1785255572462066e-05
  },
  {
    "step": 3050,
    "epoch": 1.349634874972339,
    "loss": 0.3917,
    "grad_norm": 3.46875,
    "learning_rate": 4.1733452636261244e-05
  },
  {
    "step": 3060,
    "epoch": 1.3540606328833813,
    "loss": 0.6664,
    "grad_norm": 1.6171875,
    "learning_rate": 4.168151920923536e-05
  },
  {
    "step": 3070,
    "epoch": 1.3584863907944236,
    "loss": 0.6572,
    "grad_norm": 2.875,
    "learning_rate": 4.1629455696371734e-05
  },
  {
    "step": 3080,
    "epoch": 1.3629121487054658,
    "loss": 0.5216,
    "grad_norm": 2.40625,
    "learning_rate": 4.157726250367207e-05
  },
  {
    "step": 3090,
    "epoch": 1.3673379066165081,
    "loss": 0.6852,
    "grad_norm": 2.203125,
    "learning_rate": 4.1524940038149384e-05
  },
  {
    "step": 3100,
    "epoch": 1.3717636645275504,
    "loss": 0.5275,
    "grad_norm": 6.15625,
    "learning_rate": 4.147248870782477e-05
  },
  {
    "step": 3110,
    "epoch": 1.3761894224385927,
    "loss": 0.5538,
    "grad_norm": 2.0625,
    "learning_rate": 4.141990892172424e-05
  },
  {
    "step": 3120,
    "epoch": 1.380615180349635,
    "loss": 0.3287,
    "grad_norm": 6.96875,
    "learning_rate": 4.136720108987552e-05
  },
  {
    "step": 3130,
    "epoch": 1.3850409382606772,
    "loss": 0.3086,
    "grad_norm": 3.015625,
    "learning_rate": 4.131436562330487e-05
  },
  {
    "step": 3140,
    "epoch": 1.3894666961717195,
    "loss": 0.774,
    "grad_norm": 2.578125,
    "learning_rate": 4.1261402934033886e-05
  },
  {
    "step": 3150,
    "epoch": 1.3938924540827617,
    "loss": 0.5797,
    "grad_norm": 1.328125,
    "learning_rate": 4.120831343507625e-05
  },
  {
    "step": 3160,
    "epoch": 1.398318211993804,
    "loss": 0.5468,
    "grad_norm": 4.46875,
    "learning_rate": 4.115509754043454e-05
  },
  {
    "step": 3170,
    "epoch": 1.4027439699048463,
    "loss": 0.3276,
    "grad_norm": 3.109375,
    "learning_rate": 4.1101755665096996e-05
  },
  {
    "step": 3180,
    "epoch": 1.4071697278158886,
    "loss": 0.6296,
    "grad_norm": 4.40625,
    "learning_rate": 4.104828822503427e-05
  },
  {
    "step": 3190,
    "epoch": 1.4115954857269308,
    "loss": 0.5066,
    "grad_norm": 3.125,
    "learning_rate": 4.09946956371962e-05
  },
  {
    "step": 3200,
    "epoch": 1.416021243637973,
    "loss": 0.5459,
    "grad_norm": 4.71875,
    "learning_rate": 4.094097831950855e-05
  },
  {
    "step": 3210,
    "epoch": 1.4204470015490154,
    "loss": 0.7258,
    "grad_norm": 4.84375,
    "learning_rate": 4.088713669086977e-05
  },
  {
    "step": 3220,
    "epoch": 1.4248727594600576,
    "loss": 0.4358,
    "grad_norm": 3.078125,
    "learning_rate": 4.083317117114768e-05
  },
  {
    "step": 3230,
    "epoch": 1.4292985173711,
    "loss": 0.4387,
    "grad_norm": 1.703125,
    "learning_rate": 4.077908218117625e-05
  },
  {
    "step": 3240,
    "epoch": 1.4337242752821422,
    "loss": 0.4906,
    "grad_norm": 1.8671875,
    "learning_rate": 4.0724870142752284e-05
  },
  {
    "step": 3250,
    "epoch": 1.4381500331931845,
    "loss": 0.4494,
    "grad_norm": 3.625,
    "learning_rate": 4.067053547863215e-05
  },
  {
    "step": 3260,
    "epoch": 1.4425757911042267,
    "loss": 0.6557,
    "grad_norm": 3.578125,
    "learning_rate": 4.061607861252847e-05
  },
  {
    "step": 3270,
    "epoch": 1.4470015490152688,
    "loss": 0.696,
    "grad_norm": 2.265625,
    "learning_rate": 4.056149996910683e-05
  },
  {
    "step": 3280,
    "epoch": 1.451427306926311,
    "loss": 0.506,
    "grad_norm": 3.375,
    "learning_rate": 4.0506799973982465e-05
  },
  {
    "step": 3290,
    "epoch": 1.4558530648373533,
    "loss": 0.748,
    "grad_norm": 2.828125,
    "learning_rate": 4.0451979053716906e-05
  },
  {
    "step": 3300,
    "epoch": 1.4602788227483956,
    "loss": 0.529,
    "grad_norm": 2.46875,
    "learning_rate": 4.039703763581472e-05
  },
  {
    "step": 3310,
    "epoch": 1.4647045806594379,
    "loss": 0.713,
    "grad_norm": 8.1875,
    "learning_rate": 4.0341976148720095e-05
  },
  {
    "step": 3320,
    "epoch": 1.4691303385704801,
    "loss": 0.5378,
    "grad_norm": 3.03125,
    "learning_rate": 4.0286795021813594e-05
  },
  {
    "step": 3330,
    "epoch": 1.4735560964815224,
    "loss": 0.644,
    "grad_norm": 3.625,
    "learning_rate": 4.023149468540871e-05
  },
  {
    "step": 3340,
    "epoch": 1.4779818543925647,
    "loss": 0.4074,
    "grad_norm": 4.84375,
    "learning_rate": 4.0176075570748596e-05
  },
  {
    "step": 3350,
    "epoch": 1.482407612303607,
    "loss": 0.715,
    "grad_norm": 4.8125,
    "learning_rate": 4.012053811000262e-05
  },
  {
    "step": 3360,
    "epoch": 1.4868333702146492,
    "loss": 0.3943,
    "grad_norm": 16.375,
    "learning_rate": 4.006488273626307e-05
  },
  {
    "step": 3370,
    "epoch": 1.4912591281256915,
    "loss": 0.4106,
    "grad_norm": 2.46875,
    "learning_rate": 4.0009109883541715e-05
  },
  {
    "step": 3380,
    "epoch": 1.4956848860367338,
    "loss": 0.343,
    "grad_norm": 3.84375,
    "learning_rate": 3.995321998676648e-05
  },
  {
    "step": 3390,
    "epoch": 1.500110643947776,
    "loss": 0.6473,
    "grad_norm": 3.53125,
    "learning_rate": 3.9897213481778006e-05
  },
  {
    "step": 3400,
    "epoch": 1.5045364018588183,
    "loss": 0.5542,
    "grad_norm": 4.03125,
    "learning_rate": 3.9841090805326264e-05
  },
  {
    "step": 3410,
    "epoch": 1.5089621597698606,
    "loss": 0.7169,
    "grad_norm": 3.40625,
    "learning_rate": 3.9784852395067166e-05
  },
  {
    "step": 3420,
    "epoch": 1.5133879176809029,
    "loss": 0.6862,
    "grad_norm": 2.109375,
    "learning_rate": 3.9728498689559126e-05
  },
  {
    "step": 3430,
    "epoch": 1.5178136755919451,
    "loss": 0.5664,
    "grad_norm": 3.109375,
    "learning_rate": 3.967203012825965e-05
  },
  {
    "step": 3440,
    "epoch": 1.5222394335029874,
    "loss": 0.1651,
    "grad_norm": 2.625,
    "learning_rate": 3.9615447151521945e-05
  },
  {
    "step": 3450,
    "epoch": 1.5266651914140297,
    "loss": 0.4386,
    "grad_norm": 0.70703125,
    "learning_rate": 3.955875020059141e-05
  },
  {
    "step": 3460,
    "epoch": 1.531090949325072,
    "loss": 0.7943,
    "grad_norm": 7.8125,
    "learning_rate": 3.950193971760226e-05
  },
  {
    "step": 3470,
    "epoch": 1.5355167072361142,
    "loss": 0.4551,
    "grad_norm": 6.21875,
    "learning_rate": 3.9445016145574074e-05
  },
  {
    "step": 3480,
    "epoch": 1.5399424651471565,
    "loss": 0.5331,
    "grad_norm": 5.53125,
    "learning_rate": 3.938797992840828e-05
  },
  {
    "step": 3490,
    "epoch": 1.5443682230581988,
    "loss": 0.5519,
    "grad_norm": 4.3125,
    "learning_rate": 3.9330831510884755e-05
  },
  {
    "step": 3500,
    "epoch": 1.548793980969241,
    "loss": 0.4455,
    "grad_norm": 4.78125,
    "learning_rate": 3.927357133865836e-05
  },
  {
    "step": 3510,
    "epoch": 1.553219738880283,
    "loss": 0.4547,
    "grad_norm": 2.90625,
    "learning_rate": 3.92161998582554e-05
  },
  {
    "step": 3520,
    "epoch": 1.5576454967913254,
    "loss": 0.388,
    "grad_norm": 1.3984375,
    "learning_rate": 3.9158717517070214e-05
  },
  {
    "step": 3530,
    "epoch": 1.5620712547023676,
    "loss": 0.4911,
    "grad_norm": 3.375,
    "learning_rate": 3.910112476336164e-05
  },
  {
    "step": 3540,
    "epoch": 1.56649701261341,
    "loss": 0.3756,
    "grad_norm": 2.890625,
    "learning_rate": 3.9043422046249544e-05
  },
  {
    "step": 3550,
    "epoch": 1.5709227705244522,
    "loss": 0.3474,
    "grad_norm": 2.1875,
    "learning_rate": 3.898560981571131e-05
  },
  {
    "step": 3560,
    "epoch": 1.5753485284354944,
    "loss": 0.5887,
    "grad_norm": 6.5625,
    "learning_rate": 3.892768852257831e-05
  },
  {
    "step": 3570,
    "epoch": 1.5797742863465367,
    "loss": 0.3682,
    "grad_norm": 3.875,
    "learning_rate": 3.886965861853244e-05
  },
  {
    "step": 3580,
    "epoch": 1.584200044257579,
    "loss": 0.8011,
    "grad_norm": 3.6875,
    "learning_rate": 3.8811520556102535e-05
  },
  {
    "step": 3590,
    "epoch": 1.5886258021686213,
    "loss": 0.4482,
    "grad_norm": 2.859375,
    "learning_rate": 3.8753274788660894e-05
  },
  {
    "step": 3600,
    "epoch": 1.5930515600796635,
    "loss": 0.5711,
    "grad_norm": 2.28125,
    "learning_rate": 3.869492177041971e-05
  },
  {
    "step": 3610,
    "epoch": 1.5974773179907058,
    "loss": 0.6288,
    "grad_norm": 6.6875,
    "learning_rate": 3.863646195642754e-05
  },
  {
    "step": 3620,
    "epoch": 1.601903075901748,
    "loss": 0.545,
    "grad_norm": 4.75,
    "learning_rate": 3.857789580256575e-05
  },
  {
    "step": 3630,
    "epoch": 1.6063288338127903,
    "loss": 0.4695,
    "grad_norm": 11.375,
    "learning_rate": 3.851922376554499e-05
  },
  {
    "step": 3640,
    "epoch": 1.6107545917238326,
    "loss": 0.7096,
    "grad_norm": 3.8125,
    "learning_rate": 3.846044630290158e-05
  },
  {
    "step": 3650,
    "epoch": 1.615180349634875,
    "loss": 0.4513,
    "grad_norm": 5.0625,
    "learning_rate": 3.8401563872993966e-05
  },
  {
    "step": 3660,
    "epoch": 1.6196061075459172,
    "loss": 0.6312,
    "grad_norm": 2.484375,
    "learning_rate": 3.8342576934999184e-05
  },
  {
    "step": 3670,
    "epoch": 1.6240318654569594,
    "loss": 0.4064,
    "grad_norm": 2.9375,
    "learning_rate": 3.8283485948909224e-05
  },
  {
    "step": 3680,
    "epoch": 1.6284576233680017,
    "loss": 0.3411,
    "grad_norm": 1.734375,
    "learning_rate": 3.8224291375527464e-05
  },
  {
    "step": 3690,
    "epoch": 1.632883381279044,
    "loss": 0.5142,
    "grad_norm": 1.5625,
    "learning_rate": 3.8164993676465074e-05
  },
  {
    "step": 3700,
    "epoch": 1.6373091391900862,
    "loss": 0.6242,
    "grad_norm": 0.76171875,
    "learning_rate": 3.810559331413743e-05
  },
  {
    "step": 3710,
    "epoch": 1.6417348971011285,
    "loss": 0.5108,
    "grad_norm": 3.328125,
    "learning_rate": 3.804609075176049e-05
  },
  {
    "step": 3720,
    "epoch": 1.6461606550121708,
    "loss": 0.6912,
    "grad_norm": 3.03125,
    "learning_rate": 3.798648645334718e-05
  },
  {
    "step": 3730,
    "epoch": 1.650586412923213,
    "loss": 0.3351,
    "grad_norm": 1.9609375,
    "learning_rate": 3.792678088370379e-05
  },
  {
    "step": 3740,
    "epoch": 1.6550121708342553,
    "loss": 0.3672,
    "grad_norm": 1.84375,
    "learning_rate": 3.7866974508426354e-05
  },
  {
    "step": 3750,
    "epoch": 1.6594379287452976,
    "loss": 0.544,
    "grad_norm": 5.53125,
    "learning_rate": 3.780706779389701e-05
  },
  {
    "step": 3760,
    "epoch": 1.6638636866563399,
    "loss": 0.3821,
    "grad_norm": 3.8125,
    "learning_rate": 3.774706120728032e-05
  },
  {
    "step": 3770,
    "epoch": 1.6682894445673822,
    "loss": 0.4732,
    "grad_norm": 8.5,
    "learning_rate": 3.768695521651973e-05
  },
  {
    "step": 3780,
    "epoch": 1.6727152024784244,
    "loss": 0.5524,
    "grad_norm": 4.3125,
    "learning_rate": 3.7626750290333824e-05
  },
  {
    "step": 3790,
    "epoch": 1.6771409603894667,
    "loss": 0.4019,
    "grad_norm": 3.609375,
    "learning_rate": 3.75664468982127e-05
  },
  {
    "step": 3800,
    "epoch": 1.681566718300509,
    "loss": 0.4904,
    "grad_norm": 5.09375,
    "learning_rate": 3.7506045510414335e-05
  },
  {
    "step": 3810,
    "epoch": 1.6859924762115512,
    "loss": 0.3192,
    "grad_norm": 1.609375,
    "learning_rate": 3.744554659796088e-05
  },
  {
    "step": 3820,
    "epoch": 1.6904182341225935,
    "loss": 0.6197,
    "grad_norm": 2.59375,
    "learning_rate": 3.7384950632634995e-05
  },
  {
    "step": 3830,
    "epoch": 1.6948439920336358,
    "loss": 0.5386,
    "grad_norm": 1.9375,
    "learning_rate": 3.732425808697622e-05
  },
  {
    "step": 3840,
    "epoch": 1.699269749944678,
    "loss": 0.4416,
    "grad_norm": 2.703125,
    "learning_rate": 3.726346943427719e-05
  },
  {
    "step": 3850,
    "epoch": 1.7036955078557203,
    "loss": 0.5422,
    "grad_norm": 5.75,
    "learning_rate": 3.7202585148580036e-05
  },
  {
    "step": 3860,
    "epoch": 1.7081212657667626,
    "loss": 0.8424,
    "grad_norm": 6.46875,
    "learning_rate": 3.714160570467266e-05
  },
  {
    "step": 3870,
    "epoch": 1.7125470236778049,
    "loss": 0.7843,
    "grad_norm": 0.0732421875,
    "learning_rate": 3.7080531578085e-05
  },
  {
    "step": 3880,
    "epoch": 1.7169727815888471,
    "loss": 0.582,
    "grad_norm": 9.4375,
    "learning_rate": 3.701936324508537e-05
  },
  {
    "step": 3890,
    "epoch": 1.7213985394998894,
    "loss": 0.5118,
    "grad_norm": 3.3125,
    "learning_rate": 3.6958101182676726e-05
  },
  {
    "step": 3900,
    "epoch": 1.7258242974109317,
    "loss": 0.7004,
    "grad_norm": 1.8125,
    "learning_rate": 3.689674586859292e-05
  },
  {
    "step": 3910,
    "epoch": 1.730250055321974,
    "loss": 0.5652,
    "grad_norm": 7.40625,
    "learning_rate": 3.683529778129503e-05
  },
  {
    "step": 3920,
    "epoch": 1.7346758132330162,
    "loss": 0.3514,
    "grad_norm": 4.71875,
    "learning_rate": 3.677375739996759e-05
  },
  {
    "step": 3930,
    "epoch": 1.7391015711440585,
    "loss": 0.6213,
    "grad_norm": 3.078125,
    "learning_rate": 3.671212520451484e-05
  },
  {
    "step": 3940,
    "epoch": 1.7435273290551008,
    "loss": 0.6189,
    "grad_norm": 7.0625,
    "learning_rate": 3.665040167555702e-05
  },
  {
    "step": 3950,
    "epoch": 1.747953086966143,
    "loss": 0.4782,
    "grad_norm": 5.125,
    "learning_rate": 3.658858729442662e-05
  },
  {
    "step": 3960,
    "epoch": 1.7523788448771853,
    "loss": 0.2948,
    "grad_norm": 3.265625,
    "learning_rate": 3.6526682543164595e-05
  },
  {
    "step": 3970,
    "epoch": 1.7568046027882276,
    "loss": 0.3615,
    "grad_norm": 2.25,
    "learning_rate": 3.646468790451663e-05
  },
  {
    "step": 3980,
    "epoch": 1.7612303606992699,
    "loss": 0.6412,
    "grad_norm": 1.9765625,
    "learning_rate": 3.6402603861929374e-05
  },
  {
    "step": 3990,
    "epoch": 1.7656561186103121,
    "loss": 0.4798,
    "grad_norm": 2.15625,
    "learning_rate": 3.6340430899546656e-05
  },
  {
    "step": 4000,
    "epoch": 1.7700818765213544,
    "loss": 0.3848,
    "grad_norm": 2.8125,
    "learning_rate": 3.6278169502205736e-05
  },
  {
    "step": 4000,
    "epoch": 1.7700818765213544,
    "eval_loss": 0.5555087327957153,
    "eval_runtime": 104.8356,
    "eval_samples_per_second": 10.779,
    "eval_steps_per_second": 10.779
  },
  {
    "step": 4010,
    "epoch": 1.7745076344323967,
    "loss": 0.4902,
    "grad_norm": 4.09375,
    "learning_rate": 3.621582015543348e-05
  },
  {
    "step": 4020,
    "epoch": 1.778933392343439,
    "loss": 0.4116,
    "grad_norm": 3.53125,
    "learning_rate": 3.615338334544265e-05
  },
  {
    "step": 4030,
    "epoch": 1.7833591502544812,
    "loss": 0.4312,
    "grad_norm": 5.65625,
    "learning_rate": 3.6090859559128e-05
  },
  {
    "step": 4040,
    "epoch": 1.7877849081655235,
    "loss": 0.296,
    "grad_norm": 0.2138671875,
    "learning_rate": 3.602824928406259e-05
  },
  {
    "step": 4050,
    "epoch": 1.7922106660765658,
    "loss": 0.5568,
    "grad_norm": 11.5,
    "learning_rate": 3.596555300849392e-05
  },
  {
    "step": 4060,
    "epoch": 1.796636423987608,
    "loss": 0.4649,
    "grad_norm": 0.8671875,
    "learning_rate": 3.590277122134015e-05
  },
  {
    "step": 4070,
    "epoch": 1.8010621818986503,
    "loss": 0.2931,
    "grad_norm": 7.21875,
    "learning_rate": 3.5839904412186256e-05
  },
  {
    "step": 4080,
    "epoch": 1.8054879398096924,
    "loss": 0.8787,
    "grad_norm": 4.625,
    "learning_rate": 3.577695307128024e-05
  },
  {
    "step": 4090,
    "epoch": 1.8099136977207346,
    "loss": 0.3593,
    "grad_norm": 6.9375,
    "learning_rate": 3.571391768952932e-05
  },
  {
    "step": 4100,
    "epoch": 1.814339455631777,
    "loss": 0.4632,
    "grad_norm": 1.5859375,
    "learning_rate": 3.565079875849605e-05
  },
  {
    "step": 4110,
    "epoch": 1.8187652135428192,
    "loss": 0.432,
    "grad_norm": 2.703125,
    "learning_rate": 3.558759677039455e-05
  },
  {
    "step": 4120,
    "epoch": 1.8231909714538614,
    "loss": 0.6558,
    "grad_norm": 3.53125,
    "learning_rate": 3.552431221808661e-05
  },
  {
    "step": 4130,
    "epoch": 1.8276167293649037,
    "loss": 0.3953,
    "grad_norm": 2.34375,
    "learning_rate": 3.546094559507787e-05
  },
  {
    "step": 4140,
    "epoch": 1.832042487275946,
    "loss": 0.4516,
    "grad_norm": 0.84375,
    "learning_rate": 3.5397497395514004e-05
  },
  {
    "step": 4150,
    "epoch": 1.8364682451869883,
    "loss": 0.385,
    "grad_norm": 5.28125,
    "learning_rate": 3.533396811417682e-05
  },
  {
    "step": 4160,
    "epoch": 1.8408940030980305,
    "loss": 0.6806,
    "grad_norm": 5.125,
    "learning_rate": 3.5270358246480386e-05
  },
  {
    "step": 4170,
    "epoch": 1.8453197610090728,
    "loss": 0.4858,
    "grad_norm": 6.09375,
    "learning_rate": 3.520666828846726e-05
  },
  {
    "step": 4180,
    "epoch": 1.849745518920115,
    "loss": 0.4067,
    "grad_norm": 6.34375,
    "learning_rate": 3.514289873680451e-05
  },
  {
    "step": 4190,
    "epoch": 1.8541712768311573,
    "loss": 0.5184,
    "grad_norm": 1.28125,
    "learning_rate": 3.5079050088779926e-05
  },
  {
    "step": 4200,
    "epoch": 1.8585970347421996,
    "loss": 0.6378,
    "grad_norm": 17.125,
    "learning_rate": 3.501512284229807e-05
  },
  {
    "step": 4210,
    "epoch": 1.8630227926532419,
    "loss": 0.4597,
    "grad_norm": 2.84375,
    "learning_rate": 3.495111749587647e-05
  },
  {
    "step": 4220,
    "epoch": 1.8674485505642842,
    "loss": 1.0249,
    "grad_norm": 3.546875,
    "learning_rate": 3.488703454864167e-05
  },
  {
    "step": 4230,
    "epoch": 1.8718743084753264,
    "loss": 0.4602,
    "grad_norm": 2.734375,
    "learning_rate": 3.482287450032536e-05
  },
  {
    "step": 4240,
    "epoch": 1.8763000663863687,
    "loss": 0.4696,
    "grad_norm": 1.5078125,
    "learning_rate": 3.475863785126049e-05
  },
  {
    "step": 4250,
    "epoch": 1.880725824297411,
    "loss": 0.3975,
    "grad_norm": 1.140625,
    "learning_rate": 3.4694325102377355e-05
  },
  {
    "step": 4260,
    "epoch": 1.8851515822084532,
    "loss": 0.6971,
    "grad_norm": 3.578125,
    "learning_rate": 3.462993675519968e-05
  },
  {
    "step": 4270,
    "epoch": 1.8895773401194955,
    "loss": 0.3912,
    "grad_norm": 1.6796875,
    "learning_rate": 3.4565473311840735e-05
  },
  {
    "step": 4280,
    "epoch": 1.8940030980305376,
    "loss": 0.5471,
    "grad_norm": 0.57421875,
    "learning_rate": 3.4500935274999413e-05
  },
  {
    "step": 4290,
    "epoch": 1.8984288559415798,
    "loss": 0.3353,
    "grad_norm": 31.375,
    "learning_rate": 3.443632314795627e-05
  },
  {
    "step": 4300,
    "epoch": 1.9028546138526221,
    "loss": 0.5455,
    "grad_norm": 14.8125,
    "learning_rate": 3.437163743456967e-05
  },
  {
    "step": 4310,
    "epoch": 1.9072803717636644,
    "loss": 0.3304,
    "grad_norm": 2.34375,
    "learning_rate": 3.430687863927178e-05
  },
  {
    "step": 4320,
    "epoch": 1.9117061296747067,
    "loss": 0.7217,
    "grad_norm": 5.03125,
    "learning_rate": 3.4242047267064715e-05
  },
  {
    "step": 4330,
    "epoch": 1.916131887585749,
    "loss": 0.429,
    "grad_norm": 2.609375,
    "learning_rate": 3.417714382351652e-05
  },
  {
    "step": 4340,
    "epoch": 1.9205576454967912,
    "loss": 0.3267,
    "grad_norm": 1.5859375,
    "learning_rate": 3.4112168814757307e-05
  },
  {
    "step": 4350,
    "epoch": 1.9249834034078335,
    "loss": 0.5666,
    "grad_norm": 4.8125,
    "learning_rate": 3.4047122747475224e-05
  },
  {
    "step": 4360,
    "epoch": 1.9294091613188757,
    "loss": 0.5284,
    "grad_norm": 3.640625,
    "learning_rate": 3.3982006128912584e-05
  },
  {
    "step": 4370,
    "epoch": 1.933834919229918,
    "loss": 0.536,
    "grad_norm": 4.8125,
    "learning_rate": 3.391681946686186e-05
  },
  {
    "step": 4380,
    "epoch": 1.9382606771409603,
    "loss": 0.6128,
    "grad_norm": 4.625,
    "learning_rate": 3.3851563269661726e-05
  },
  {
    "step": 4390,
    "epoch": 1.9426864350520026,
    "loss": 0.8749,
    "grad_norm": 5.09375,
    "learning_rate": 3.378623804619313e-05
  },
  {
    "step": 4400,
    "epoch": 1.9471121929630448,
    "loss": 0.459,
    "grad_norm": 13.875,
    "learning_rate": 3.372084430587528e-05
  },
  {
    "step": 4410,
    "epoch": 1.951537950874087,
    "loss": 0.737,
    "grad_norm": 3.109375,
    "learning_rate": 3.3655382558661685e-05
  },
  {
    "step": 4420,
    "epoch": 1.9559637087851294,
    "loss": 0.2863,
    "grad_norm": 2.59375,
    "learning_rate": 3.3589853315036225e-05
  },
  {
    "step": 4430,
    "epoch": 1.9603894666961716,
    "loss": 0.541,
    "grad_norm": 6.25,
    "learning_rate": 3.3524257086009104e-05
  },
  {
    "step": 4440,
    "epoch": 1.964815224607214,
    "loss": 0.5359,
    "grad_norm": 2.484375,
    "learning_rate": 3.345859438311287e-05
  },
  {
    "step": 4450,
    "epoch": 1.9692409825182562,
    "loss": 0.3777,
    "grad_norm": 4.28125,
    "learning_rate": 3.339286571839848e-05
  },
  {
    "step": 4460,
    "epoch": 1.9736667404292985,
    "loss": 0.4441,
    "grad_norm": 5.1875,
    "learning_rate": 3.3327071604431275e-05
  },
  {
    "step": 4470,
    "epoch": 1.9780924983403407,
    "loss": 0.3256,
    "grad_norm": 3.515625,
    "learning_rate": 3.3261212554286975e-05
  },
  {
    "step": 4480,
    "epoch": 1.982518256251383,
    "loss": 0.8768,
    "grad_norm": 2.453125,
    "learning_rate": 3.319528908154766e-05
  },
  {
    "step": 4490,
    "epoch": 1.9869440141624253,
    "loss": 0.6497,
    "grad_norm": 3.75,
    "learning_rate": 3.312930170029783e-05
  },
  {
    "step": 4500,
    "epoch": 1.9913697720734675,
    "loss": 0.499,
    "grad_norm": 1.765625,
    "learning_rate": 3.3063250925120334e-05
  },
  {
    "step": 4510,
    "epoch": 1.9957955299845098,
    "loss": 0.3688,
    "grad_norm": 1.1796875,
    "learning_rate": 3.299713727109239e-05
  },
  {
    "step": 4520,
    "epoch": 2.0,
    "loss": 0.5607,
    "grad_norm": 3.765625,
    "learning_rate": 3.2930961253781554e-05
  },
  {
    "step": 4530,
    "epoch": 2.0044257579110423,
    "loss": 0.4157,
    "grad_norm": 2.765625,
    "learning_rate": 3.28647233892417e-05
  },
  {
    "step": 4540,
    "epoch": 2.0088515158220845,
    "loss": 0.7646,
    "grad_norm": 3.953125,
    "learning_rate": 3.279842419400899e-05
  },
  {
    "step": 4550,
    "epoch": 2.013277273733127,
    "loss": 0.4865,
    "grad_norm": 5.8125,
    "learning_rate": 3.273206418509788e-05
  },
  {
    "step": 4560,
    "epoch": 2.017703031644169,
    "loss": 0.4241,
    "grad_norm": 6.21875,
    "learning_rate": 3.2665643879997056e-05
  },
  {
    "step": 4570,
    "epoch": 2.0221287895552114,
    "loss": 0.462,
    "grad_norm": 7.8125,
    "learning_rate": 3.2599163796665376e-05
  },
  {
    "step": 4580,
    "epoch": 2.0265545474662536,
    "loss": 0.3765,
    "grad_norm": 1.71875,
    "learning_rate": 3.253262445352791e-05
  },
  {
    "step": 4590,
    "epoch": 2.030980305377296,
    "loss": 0.3992,
    "grad_norm": 4.5,
    "learning_rate": 3.24660263694718e-05
  },
  {
    "step": 4600,
    "epoch": 2.035406063288338,
    "loss": 0.4868,
    "grad_norm": 3.3125,
    "learning_rate": 3.2399370063842294e-05
  },
  {
    "step": 4610,
    "epoch": 2.0398318211993804,
    "loss": 0.5155,
    "grad_norm": 6.53125,
    "learning_rate": 3.233265605643866e-05
  },
  {
    "step": 4620,
    "epoch": 2.0442575791104227,
    "loss": 0.6975,
    "grad_norm": 2.390625,
    "learning_rate": 3.226588486751012e-05
  },
  {
    "step": 4630,
    "epoch": 2.048683337021465,
    "loss": 0.2793,
    "grad_norm": 5.40625,
    "learning_rate": 3.219905701775182e-05
  },
  {
    "step": 4640,
    "epoch": 2.0531090949325073,
    "loss": 0.4433,
    "grad_norm": 3.546875,
    "learning_rate": 3.2132173028300756e-05
  },
  {
    "step": 4650,
    "epoch": 2.0575348528435495,
    "loss": 0.446,
    "grad_norm": 5.5625,
    "learning_rate": 3.206523342073172e-05
  },
  {
    "step": 4660,
    "epoch": 2.061960610754592,
    "loss": 0.5922,
    "grad_norm": 2.484375,
    "learning_rate": 3.1998238717053206e-05
  },
  {
    "step": 4670,
    "epoch": 2.066386368665634,
    "loss": 0.5054,
    "grad_norm": 8.5625,
    "learning_rate": 3.193118943970338e-05
  },
  {
    "step": 4680,
    "epoch": 2.0708121265766763,
    "loss": 0.5288,
    "grad_norm": 3.375,
    "learning_rate": 3.186408611154597e-05
  },
  {
    "step": 4690,
    "epoch": 2.0752378844877186,
    "loss": 0.527,
    "grad_norm": 8.25,
    "learning_rate": 3.179692925586622e-05
  },
  {
    "step": 4700,
    "epoch": 2.079663642398761,
    "loss": 0.2845,
    "grad_norm": 1.1953125,
    "learning_rate": 3.1729719396366765e-05
  },
  {
    "step": 4710,
    "epoch": 2.084089400309803,
    "loss": 0.6122,
    "grad_norm": 6.65625,
    "learning_rate": 3.1662457057163604e-05
  },
  {
    "step": 4720,
    "epoch": 2.0885151582208454,
    "loss": 0.3964,
    "grad_norm": 4.03125,
    "learning_rate": 3.159514276278197e-05
  },
  {
    "step": 4730,
    "epoch": 2.0929409161318877,
    "loss": 0.3849,
    "grad_norm": 3.1875,
    "learning_rate": 3.152777703815223e-05
  },
  {
    "step": 4740,
    "epoch": 2.09736667404293,
    "loss": 0.5093,
    "grad_norm": 3.921875,
    "learning_rate": 3.1460360408605866e-05
  },
  {
    "step": 4750,
    "epoch": 2.1017924319539723,
    "loss": 0.3254,
    "grad_norm": 2.671875,
    "learning_rate": 3.1392893399871295e-05
  },
  {
    "step": 4760,
    "epoch": 2.1062181898650145,
    "loss": 0.4589,
    "grad_norm": 2.640625,
    "learning_rate": 3.1325376538069776e-05
  },
  {
    "step": 4770,
    "epoch": 2.110643947776057,
    "loss": 0.5405,
    "grad_norm": 4.65625,
    "learning_rate": 3.125781034971139e-05
  },
  {
    "step": 4780,
    "epoch": 2.115069705687099,
    "loss": 0.3766,
    "grad_norm": 0.3515625,
    "learning_rate": 3.119019536169083e-05
  },
  {
    "step": 4790,
    "epoch": 2.1194954635981413,
    "loss": 0.5029,
    "grad_norm": 0.55859375,
    "learning_rate": 3.112253210128336e-05
  },
  {
    "step": 4800,
    "epoch": 2.1239212215091836,
    "loss": 0.8002,
    "grad_norm": 5.625,
    "learning_rate": 3.1054821096140676e-05
  },
  {
    "step": 4810,
    "epoch": 2.128346979420226,
    "loss": 0.6024,
    "grad_norm": 1.09375,
    "learning_rate": 3.0987062874286804e-05
  },
  {
    "step": 4820,
    "epoch": 2.132772737331268,
    "loss": 0.4064,
    "grad_norm": 3.140625,
    "learning_rate": 3.0919257964113964e-05
  },
  {
    "step": 4830,
    "epoch": 2.1371984952423104,
    "loss": 0.6312,
    "grad_norm": 20.625,
    "learning_rate": 3.085140689437846e-05
  },
  {
    "step": 4840,
    "epoch": 2.1416242531533527,
    "loss": 0.5073,
    "grad_norm": 3.640625,
    "learning_rate": 3.0783510194196576e-05
  },
  {
    "step": 4850,
    "epoch": 2.146050011064395,
    "loss": 0.4142,
    "grad_norm": 5.40625,
    "learning_rate": 3.0715568393040405e-05
  },
  {
    "step": 4860,
    "epoch": 2.1504757689754372,
    "loss": 0.4074,
    "grad_norm": 1.6484375,
    "learning_rate": 3.064758202073377e-05
  },
  {
    "step": 4870,
    "epoch": 2.1549015268864795,
    "loss": 0.5842,
    "grad_norm": 5.21875,
    "learning_rate": 3.0579551607448066e-05
  },
  {
    "step": 4880,
    "epoch": 2.159327284797522,
    "loss": 0.5418,
    "grad_norm": 6.65625,
    "learning_rate": 3.0511477683698108e-05
  },
  {
    "step": 4890,
    "epoch": 2.1637530427085636,
    "loss": 0.4002,
    "grad_norm": 3.921875,
    "learning_rate": 3.044336078033803e-05
  },
  {
    "step": 4900,
    "epoch": 2.1681788006196063,
    "loss": 0.5322,
    "grad_norm": 4.4375,
    "learning_rate": 3.0375201428557132e-05
  },
  {
    "step": 4910,
    "epoch": 2.172604558530648,
    "loss": 0.4602,
    "grad_norm": 3.1875,
    "learning_rate": 3.030700015987573e-05
  },
  {
    "step": 4920,
    "epoch": 2.177030316441691,
    "loss": 0.3719,
    "grad_norm": 2.65625,
    "learning_rate": 3.0238757506141012e-05
  },
  {
    "step": 4930,
    "epoch": 2.1814560743527327,
    "loss": 0.2981,
    "grad_norm": 0.03173828125,
    "learning_rate": 3.0170473999522915e-05
  },
  {
    "step": 4940,
    "epoch": 2.185881832263775,
    "loss": 0.3919,
    "grad_norm": 7.84375,
    "learning_rate": 3.010215017250993e-05
  },
  {
    "step": 4950,
    "epoch": 2.1903075901748172,
    "loss": 0.4453,
    "grad_norm": 3.234375,
    "learning_rate": 3.003378655790498e-05
  },
  {
    "step": 4960,
    "epoch": 2.1947333480858595,
    "loss": 0.4292,
    "grad_norm": 2.984375,
    "learning_rate": 2.996538368882127e-05
  },
  {
    "step": 4970,
    "epoch": 2.199159105996902,
    "loss": 0.4807,
    "grad_norm": 3.625,
    "learning_rate": 2.9896942098678122e-05
  },
  {
    "step": 4980,
    "epoch": 2.203584863907944,
    "loss": 0.3378,
    "grad_norm": 2.65625,
    "learning_rate": 2.9828462321196788e-05
  },
  {
    "step": 4990,
    "epoch": 2.2080106218189863,
    "loss": 0.4185,
    "grad_norm": 9.25,
    "learning_rate": 2.975994489039634e-05
  },
  {
    "step": 5000,
    "epoch": 2.2124363797300286,
    "loss": 0.4874,
    "grad_norm": 3.171875,
    "learning_rate": 2.9691390340589466e-05
  },
  {
    "step": 5000,
    "epoch": 2.2124363797300286,
    "eval_loss": 0.5172871947288513,
    "eval_runtime": 104.2839,
    "eval_samples_per_second": 10.836,
    "eval_steps_per_second": 10.836
  },
  {
    "step": 5010,
    "epoch": 2.216862137641071,
    "loss": 0.4542,
    "grad_norm": 1.234375,
    "learning_rate": 2.9622799206378305e-05
  },
  {
    "step": 5020,
    "epoch": 2.221287895552113,
    "loss": 0.3159,
    "grad_norm": 0.2119140625,
    "learning_rate": 2.9554172022650317e-05
  },
  {
    "step": 5030,
    "epoch": 2.2257136534631554,
    "loss": 0.4724,
    "grad_norm": 0.63671875,
    "learning_rate": 2.948550932457407e-05
  },
  {
    "step": 5040,
    "epoch": 2.2301394113741977,
    "loss": 0.5629,
    "grad_norm": 6.0,
    "learning_rate": 2.9416811647595048e-05
  },
  {
    "step": 5050,
    "epoch": 2.23456516928524,
    "loss": 0.3375,
    "grad_norm": 4.28125,
    "learning_rate": 2.9348079527431567e-05
  },
  {
    "step": 5060,
    "epoch": 2.2389909271962822,
    "loss": 0.3858,
    "grad_norm": 3.0,
    "learning_rate": 2.9279313500070483e-05
  },
  {
    "step": 5070,
    "epoch": 2.2434166851073245,
    "loss": 0.3408,
    "grad_norm": 1.1875,
    "learning_rate": 2.9210514101763113e-05
  },
  {
    "step": 5080,
    "epoch": 2.2478424430183668,
    "loss": 0.4777,
    "grad_norm": 3.828125,
    "learning_rate": 2.914168186902097e-05
  },
  {
    "step": 5090,
    "epoch": 2.252268200929409,
    "loss": 0.562,
    "grad_norm": 4.1875,
    "learning_rate": 2.9072817338611636e-05
  },
  {
    "step": 5100,
    "epoch": 2.2566939588404513,
    "loss": 0.7115,
    "grad_norm": 1.4375,
    "learning_rate": 2.900392104755455e-05
  },
  {
    "step": 5110,
    "epoch": 2.2611197167514936,
    "loss": 0.4938,
    "grad_norm": 2.546875,
    "learning_rate": 2.893499353311683e-05
  },
  {
    "step": 5120,
    "epoch": 2.265545474662536,
    "loss": 0.4518,
    "grad_norm": 4.90625,
    "learning_rate": 2.8866035332809084e-05
  },
  {
    "step": 5130,
    "epoch": 2.269971232573578,
    "loss": 0.6265,
    "grad_norm": 9.375,
    "learning_rate": 2.8797046984381208e-05
  },
  {
    "step": 5140,
    "epoch": 2.2743969904846204,
    "loss": 0.2662,
    "grad_norm": 2.59375,
    "learning_rate": 2.8728029025818204e-05
  },
  {
    "step": 5150,
    "epoch": 2.2788227483956627,
    "loss": 0.3922,
    "grad_norm": 5.875,
    "learning_rate": 2.865898199533597e-05
  },
  {
    "step": 5160,
    "epoch": 2.283248506306705,
    "loss": 0.418,
    "grad_norm": 4.375,
    "learning_rate": 2.8589906431377134e-05
  },
  {
    "step": 5170,
    "epoch": 2.287674264217747,
    "loss": 0.7287,
    "grad_norm": 9.0625,
    "learning_rate": 2.85208028726068e-05
  },
  {
    "step": 5180,
    "epoch": 2.2921000221287895,
    "loss": 0.4651,
    "grad_norm": 6.78125,
    "learning_rate": 2.8451671857908415e-05
  },
  {
    "step": 5190,
    "epoch": 2.2965257800398318,
    "loss": 0.4253,
    "grad_norm": 3.921875,
    "learning_rate": 2.8382513926379504e-05
  },
  {
    "step": 5200,
    "epoch": 2.300951537950874,
    "loss": 0.4732,
    "grad_norm": 8.25,
    "learning_rate": 2.8313329617327537e-05
  },
  {
    "step": 5210,
    "epoch": 2.3053772958619163,
    "loss": 0.3556,
    "grad_norm": 2.953125,
    "learning_rate": 2.824411947026563e-05
  },
  {
    "step": 5220,
    "epoch": 2.3098030537729586,
    "loss": 0.5486,
    "grad_norm": 3.875,
    "learning_rate": 2.817488402490841e-05
  },
  {
    "step": 5230,
    "epoch": 2.314228811684001,
    "loss": 0.3292,
    "grad_norm": 5.71875,
    "learning_rate": 2.8105623821167804e-05
  },
  {
    "step": 5240,
    "epoch": 2.318654569595043,
    "loss": 0.4562,
    "grad_norm": 2.546875,
    "learning_rate": 2.803633939914878e-05
  },
  {
    "step": 5250,
    "epoch": 2.3230803275060854,
    "loss": 0.5224,
    "grad_norm": 8.5625,
    "learning_rate": 2.7967031299145193e-05
  },
  {
    "step": 5260,
    "epoch": 2.3275060854171277,
    "loss": 0.6914,
    "grad_norm": 3.359375,
    "learning_rate": 2.7897700061635517e-05
  },
  {
    "step": 5270,
    "epoch": 2.33193184332817,
    "loss": 0.4684,
    "grad_norm": 1.9765625,
    "learning_rate": 2.7828346227278674e-05
  },
  {
    "step": 5280,
    "epoch": 2.336357601239212,
    "loss": 0.4581,
    "grad_norm": 3.0,
    "learning_rate": 2.7758970336909795e-05
  },
  {
    "step": 5290,
    "epoch": 2.3407833591502545,
    "loss": 0.492,
    "grad_norm": 3.09375,
    "learning_rate": 2.7689572931536017e-05
  },
  {
    "step": 5300,
    "epoch": 2.3452091170612968,
    "loss": 0.9692,
    "grad_norm": 8.75,
    "learning_rate": 2.7620154552332232e-05
  },
  {
    "step": 5310,
    "epoch": 2.349634874972339,
    "loss": 0.5695,
    "grad_norm": 6.3125,
    "learning_rate": 2.7550715740636917e-05
  },
  {
    "step": 5320,
    "epoch": 2.3540606328833813,
    "loss": 0.4037,
    "grad_norm": 2.328125,
    "learning_rate": 2.7481257037947872e-05
  },
  {
    "step": 5330,
    "epoch": 2.3584863907944236,
    "loss": 0.4976,
    "grad_norm": 1.734375,
    "learning_rate": 2.7411778985918006e-05
  },
  {
    "step": 5340,
    "epoch": 2.362912148705466,
    "loss": 0.5981,
    "grad_norm": 1.5546875,
    "learning_rate": 2.7342282126351144e-05
  },
  {
    "step": 5350,
    "epoch": 2.367337906616508,
    "loss": 0.6494,
    "grad_norm": 3.765625,
    "learning_rate": 2.7272767001197742e-05
  },
  {
    "step": 5360,
    "epoch": 2.3717636645275504,
    "loss": 0.7062,
    "grad_norm": 1.9375,
    "learning_rate": 2.7203234152550712e-05
  },
  {
    "step": 5370,
    "epoch": 2.3761894224385927,
    "loss": 0.3519,
    "grad_norm": 4.125,
    "learning_rate": 2.713368412264118e-05
  },
  {
    "step": 5380,
    "epoch": 2.380615180349635,
    "loss": 0.4672,
    "grad_norm": 1.6484375,
    "learning_rate": 2.7064117453834243e-05
  },
  {
    "step": 5390,
    "epoch": 2.385040938260677,
    "loss": 0.5789,
    "grad_norm": 8.0,
    "learning_rate": 2.699453468862477e-05
  },
  {
    "step": 5400,
    "epoch": 2.3894666961717195,
    "loss": 0.5208,
    "grad_norm": 1.3515625,
    "learning_rate": 2.6924936369633125e-05
  },
  {
    "step": 5410,
    "epoch": 2.3938924540827617,
    "loss": 0.2601,
    "grad_norm": 4.90625,
    "learning_rate": 2.6855323039601e-05
  },
  {
    "step": 5420,
    "epoch": 2.398318211993804,
    "loss": 0.5554,
    "grad_norm": 22.0,
    "learning_rate": 2.678569524138711e-05
  },
  {
    "step": 5430,
    "epoch": 2.4027439699048463,
    "loss": 0.7589,
    "grad_norm": 0.95703125,
    "learning_rate": 2.671605351796302e-05
  },
  {
    "step": 5440,
    "epoch": 2.4071697278158886,
    "loss": 0.5485,
    "grad_norm": 4.125,
    "learning_rate": 2.664639841240888e-05
  },
  {
    "step": 5450,
    "epoch": 2.411595485726931,
    "loss": 0.349,
    "grad_norm": 2.1875,
    "learning_rate": 2.65767304679092e-05
  },
  {
    "step": 5460,
    "epoch": 2.416021243637973,
    "loss": 0.522,
    "grad_norm": 1.0703125,
    "learning_rate": 2.650705022774859e-05
  },
  {
    "step": 5470,
    "epoch": 2.4204470015490154,
    "loss": 0.3001,
    "grad_norm": 4.21875,
    "learning_rate": 2.6437358235307576e-05
  },
  {
    "step": 5480,
    "epoch": 2.4248727594600576,
    "loss": 0.2986,
    "grad_norm": 4.46875,
    "learning_rate": 2.6367655034058302e-05
  },
  {
    "step": 5490,
    "epoch": 2.4292985173711,
    "loss": 0.4667,
    "grad_norm": 7.5,
    "learning_rate": 2.6297941167560346e-05
  },
  {
    "step": 5500,
    "epoch": 2.433724275282142,
    "loss": 0.3174,
    "grad_norm": 3.546875,
    "learning_rate": 2.6228217179456433e-05
  },
  {
    "step": 5510,
    "epoch": 2.4381500331931845,
    "loss": 0.5903,
    "grad_norm": 8.625,
    "learning_rate": 2.6158483613468227e-05
  },
  {
    "step": 5520,
    "epoch": 2.4425757911042267,
    "loss": 0.5625,
    "grad_norm": 3.578125,
    "learning_rate": 2.6088741013392098e-05
  },
  {
    "step": 5530,
    "epoch": 2.447001549015269,
    "loss": 0.3602,
    "grad_norm": 6.625,
    "learning_rate": 2.6018989923094828e-05
  },
  {
    "step": 5540,
    "epoch": 2.4514273069263113,
    "loss": 0.5277,
    "grad_norm": 15.125,
    "learning_rate": 2.5949230886509457e-05
  },
  {
    "step": 5550,
    "epoch": 2.4558530648373535,
    "loss": 0.3024,
    "grad_norm": 2.609375,
    "learning_rate": 2.5879464447630946e-05
  },
  {
    "step": 5560,
    "epoch": 2.460278822748396,
    "loss": 0.37,
    "grad_norm": 2.046875,
    "learning_rate": 2.5809691150512012e-05
  },
  {
    "step": 5570,
    "epoch": 2.464704580659438,
    "loss": 0.3591,
    "grad_norm": 7.59375,
    "learning_rate": 2.573991153925883e-05
  },
  {
    "step": 5580,
    "epoch": 2.4691303385704804,
    "loss": 0.4018,
    "grad_norm": 2.953125,
    "learning_rate": 2.5670126158026842e-05
  },
  {
    "step": 5590,
    "epoch": 2.4735560964815226,
    "loss": 0.3582,
    "grad_norm": 5.15625,
    "learning_rate": 2.5600335551016445e-05
  },
  {
    "step": 5600,
    "epoch": 2.477981854392565,
    "loss": 0.5747,
    "grad_norm": 2.96875,
    "learning_rate": 2.5530540262468837e-05
  },
  {
    "step": 5610,
    "epoch": 2.482407612303607,
    "loss": 0.9027,
    "grad_norm": 2.90625,
    "learning_rate": 2.546074083666169e-05
  },
  {
    "step": 5620,
    "epoch": 2.4868333702146495,
    "loss": 0.5675,
    "grad_norm": 3.375,
    "learning_rate": 2.539093781790494e-05
  },
  {
    "step": 5630,
    "epoch": 2.4912591281256917,
    "loss": 0.2511,
    "grad_norm": 1.703125,
    "learning_rate": 2.5321131750536547e-05
  },
  {
    "step": 5640,
    "epoch": 2.4956848860367336,
    "loss": 0.1629,
    "grad_norm": 2.390625,
    "learning_rate": 2.5251323178918268e-05
  },
  {
    "step": 5650,
    "epoch": 2.5001106439477763,
    "loss": 0.3703,
    "grad_norm": 2.0625,
    "learning_rate": 2.518151264743135e-05
  },
  {
    "step": 5660,
    "epoch": 2.504536401858818,
    "loss": 0.6096,
    "grad_norm": 3.828125,
    "learning_rate": 2.5111700700472346e-05
  },
  {
    "step": 5670,
    "epoch": 2.508962159769861,
    "loss": 0.5318,
    "grad_norm": 7.28125,
    "learning_rate": 2.5041887882448844e-05
  },
  {
    "step": 5680,
    "epoch": 2.5133879176809026,
    "loss": 0.4198,
    "grad_norm": 3.53125,
    "learning_rate": 2.4972074737775214e-05
  },
  {
    "step": 5690,
    "epoch": 2.5178136755919454,
    "loss": 0.4196,
    "grad_norm": 2.90625,
    "learning_rate": 2.490226181086838e-05
  },
  {
    "step": 5700,
    "epoch": 2.522239433502987,
    "loss": 0.5569,
    "grad_norm": 2.265625,
    "learning_rate": 2.4832449646143604e-05
  },
  {
    "step": 5710,
    "epoch": 2.52666519141403,
    "loss": 0.5178,
    "grad_norm": 4.28125,
    "learning_rate": 2.4762638788010122e-05
  },
  {
    "step": 5720,
    "epoch": 2.5310909493250717,
    "loss": 0.5716,
    "grad_norm": 8.1875,
    "learning_rate": 2.4692829780867065e-05
  },
  {
    "step": 5730,
    "epoch": 2.5355167072361144,
    "loss": 0.3723,
    "grad_norm": 4.5,
    "learning_rate": 2.4623023169099073e-05
  },
  {
    "step": 5740,
    "epoch": 2.5399424651471563,
    "loss": 0.4321,
    "grad_norm": 7.9375,
    "learning_rate": 2.4553219497072143e-05
  },
  {
    "step": 5750,
    "epoch": 2.544368223058199,
    "loss": 0.2444,
    "grad_norm": 1.5625,
    "learning_rate": 2.4483419309129315e-05
  },
  {
    "step": 5760,
    "epoch": 2.548793980969241,
    "loss": 0.2286,
    "grad_norm": 4.65625,
    "learning_rate": 2.441362314958649e-05
  },
  {
    "step": 5770,
    "epoch": 2.553219738880283,
    "loss": 0.3562,
    "grad_norm": 1.6953125,
    "learning_rate": 2.4343831562728135e-05
  },
  {
    "step": 5780,
    "epoch": 2.5576454967913254,
    "loss": 0.2977,
    "grad_norm": 3.34375,
    "learning_rate": 2.4274045092803056e-05
  },
  {
    "step": 5790,
    "epoch": 2.5620712547023676,
    "loss": 0.5982,
    "grad_norm": 2.765625,
    "learning_rate": 2.4204264284020182e-05
  },
  {
    "step": 5800,
    "epoch": 2.56649701261341,
    "loss": 0.5944,
    "grad_norm": 3.203125,
    "learning_rate": 2.413448968054426e-05
  },
  {
    "step": 5810,
    "epoch": 2.570922770524452,
    "loss": 0.38,
    "grad_norm": 0.42578125,
    "learning_rate": 2.406472182649168e-05
  },
  {
    "step": 5820,
    "epoch": 2.5753485284354944,
    "loss": 0.3158,
    "grad_norm": 1.8984375,
    "learning_rate": 2.3994961265926166e-05
  },
  {
    "step": 5830,
    "epoch": 2.5797742863465367,
    "loss": 0.488,
    "grad_norm": 8.375,
    "learning_rate": 2.3925208542854608e-05
  },
  {
    "step": 5840,
    "epoch": 2.584200044257579,
    "loss": 0.3411,
    "grad_norm": 4.1875,
    "learning_rate": 2.3855464201222716e-05
  },
  {
    "step": 5850,
    "epoch": 2.5886258021686213,
    "loss": 0.4204,
    "grad_norm": 3.5,
    "learning_rate": 2.378572878491091e-05
  },
  {
    "step": 5860,
    "epoch": 2.5930515600796635,
    "loss": 0.5645,
    "grad_norm": 1.953125,
    "learning_rate": 2.3716002837729954e-05
  },
  {
    "step": 5870,
    "epoch": 2.597477317990706,
    "loss": 0.3849,
    "grad_norm": 2.03125,
    "learning_rate": 2.36462869034168e-05
  },
  {
    "step": 5880,
    "epoch": 2.601903075901748,
    "loss": 0.5445,
    "grad_norm": 6.5,
    "learning_rate": 2.35765815256303e-05
  },
  {
    "step": 5890,
    "epoch": 2.6063288338127903,
    "loss": 0.5287,
    "grad_norm": 0.87890625,
    "learning_rate": 2.3506887247947e-05
  },
  {
    "step": 5900,
    "epoch": 2.6107545917238326,
    "loss": 0.6177,
    "grad_norm": 2.859375,
    "learning_rate": 2.343720461385688e-05
  },
  {
    "step": 5910,
    "epoch": 2.615180349634875,
    "loss": 0.4464,
    "grad_norm": 0.953125,
    "learning_rate": 2.3367534166759102e-05
  },
  {
    "step": 5920,
    "epoch": 2.619606107545917,
    "loss": 0.5923,
    "grad_norm": 4.1875,
    "learning_rate": 2.3297876449957834e-05
  },
  {
    "step": 5930,
    "epoch": 2.6240318654569594,
    "loss": 0.51,
    "grad_norm": 4.625,
    "learning_rate": 2.3228232006657923e-05
  },
  {
    "step": 5940,
    "epoch": 2.6284576233680017,
    "loss": 0.3251,
    "grad_norm": 1.359375,
    "learning_rate": 2.315860137996074e-05
  },
  {
    "step": 5950,
    "epoch": 2.632883381279044,
    "loss": 0.5821,
    "grad_norm": 2.859375,
    "learning_rate": 2.3088985112859884e-05
  },
  {
    "step": 5960,
    "epoch": 2.6373091391900862,
    "loss": 0.4033,
    "grad_norm": 4.40625,
    "learning_rate": 2.3019383748237015e-05
  },
  {
    "step": 5970,
    "epoch": 2.6417348971011285,
    "loss": 0.4754,
    "grad_norm": 14.5,
    "learning_rate": 2.2949797828857525e-05
  },
  {
    "step": 5980,
    "epoch": 2.646160655012171,
    "loss": 0.3401,
    "grad_norm": 2.53125,
    "learning_rate": 2.2880227897366422e-05
  },
  {
    "step": 5990,
    "epoch": 2.650586412923213,
    "loss": 0.6951,
    "grad_norm": 7.0,
    "learning_rate": 2.2810674496283984e-05
  },
  {
    "step": 6000,
    "epoch": 2.6550121708342553,
    "loss": 0.327,
    "grad_norm": 6.5625,
    "learning_rate": 2.2741138168001608e-05
  },
  {
    "step": 6000,
    "epoch": 2.6550121708342553,
    "eval_loss": 0.508233904838562,
    "eval_runtime": 104.7344,
    "eval_samples_per_second": 10.789,
    "eval_steps_per_second": 10.789
  },
  {
    "step": 6010,
    "epoch": 2.6594379287452976,
    "loss": 0.5533,
    "grad_norm": 6.0625,
    "learning_rate": 2.2671619454777566e-05
  },
  {
    "step": 6020,
    "epoch": 2.66386368665634,
    "loss": 0.3344,
    "grad_norm": 3.40625,
    "learning_rate": 2.2602118898732736e-05
  },
  {
    "step": 6030,
    "epoch": 2.668289444567382,
    "loss": 0.5123,
    "grad_norm": 2.703125,
    "learning_rate": 2.2532637041846422e-05
  },
  {
    "step": 6040,
    "epoch": 2.6727152024784244,
    "loss": 0.3249,
    "grad_norm": 2.34375,
    "learning_rate": 2.2463174425952084e-05
  },
  {
    "step": 6050,
    "epoch": 2.6771409603894667,
    "loss": 0.4994,
    "grad_norm": 3.484375,
    "learning_rate": 2.239373159273318e-05
  },
  {
    "step": 6060,
    "epoch": 2.681566718300509,
    "loss": 0.341,
    "grad_norm": 5.78125,
    "learning_rate": 2.232430908371885e-05
  },
  {
    "step": 6070,
    "epoch": 2.6859924762115512,
    "loss": 0.6154,
    "grad_norm": 3.21875,
    "learning_rate": 2.2254907440279786e-05
  },
  {
    "step": 6080,
    "epoch": 2.6904182341225935,
    "loss": 0.6754,
    "grad_norm": 4.9375,
    "learning_rate": 2.2185527203623922e-05
  },
  {
    "step": 6090,
    "epoch": 2.694843992033636,
    "loss": 0.5991,
    "grad_norm": 2.515625,
    "learning_rate": 2.2116168914792292e-05
  },
  {
    "step": 6100,
    "epoch": 2.699269749944678,
    "loss": 0.4413,
    "grad_norm": 5.09375,
    "learning_rate": 2.2046833114654773e-05
  },
  {
    "step": 6110,
    "epoch": 2.7036955078557203,
    "loss": 0.6199,
    "grad_norm": 2.84375,
    "learning_rate": 2.197752034390585e-05
  },
  {
    "step": 6120,
    "epoch": 2.7081212657667626,
    "loss": 0.3488,
    "grad_norm": 0.9375,
    "learning_rate": 2.190823114306045e-05
  },
  {
    "step": 6130,
    "epoch": 2.712547023677805,
    "loss": 0.5716,
    "grad_norm": 0.0771484375,
    "learning_rate": 2.183896605244965e-05
  },
  {
    "step": 6140,
    "epoch": 2.716972781588847,
    "loss": 0.4505,
    "grad_norm": 5.3125,
    "learning_rate": 2.1769725612216567e-05
  },
  {
    "step": 6150,
    "epoch": 2.7213985394998894,
    "loss": 0.4881,
    "grad_norm": 3.109375,
    "learning_rate": 2.1700510362312052e-05
  },
  {
    "step": 6160,
    "epoch": 2.7258242974109317,
    "loss": 0.4911,
    "grad_norm": 7.25,
    "learning_rate": 2.1631320842490532e-05
  },
  {
    "step": 6170,
    "epoch": 2.730250055321974,
    "loss": 0.3577,
    "grad_norm": 6.5,
    "learning_rate": 2.156215759230577e-05
  },
  {
    "step": 6180,
    "epoch": 2.7346758132330162,
    "loss": 0.5682,
    "grad_norm": 6.5625,
    "learning_rate": 2.1493021151106703e-05
  },
  {
    "step": 6190,
    "epoch": 2.7391015711440585,
    "loss": 0.3594,
    "grad_norm": 1.1015625,
    "learning_rate": 2.1423912058033174e-05
  },
  {
    "step": 6200,
    "epoch": 2.7435273290551008,
    "loss": 0.3104,
    "grad_norm": 5.625,
    "learning_rate": 2.135483085201177e-05
  },
  {
    "step": 6210,
    "epoch": 2.747953086966143,
    "loss": 0.4474,
    "grad_norm": 1.828125,
    "learning_rate": 2.1285778071751634e-05
  },
  {
    "step": 6220,
    "epoch": 2.7523788448771853,
    "loss": 0.6216,
    "grad_norm": 4.46875,
    "learning_rate": 2.1216754255740193e-05
  },
  {
    "step": 6230,
    "epoch": 2.7568046027882276,
    "loss": 0.5195,
    "grad_norm": 6.78125,
    "learning_rate": 2.1147759942239046e-05
  },
  {
    "step": 6240,
    "epoch": 2.76123036069927,
    "loss": 0.5433,
    "grad_norm": 1.6875,
    "learning_rate": 2.10787956692797e-05
  },
  {
    "step": 6250,
    "epoch": 2.765656118610312,
    "loss": 0.3759,
    "grad_norm": 12.625,
    "learning_rate": 2.1009861974659413e-05
  },
  {
    "step": 6260,
    "epoch": 2.7700818765213544,
    "loss": 0.5731,
    "grad_norm": 4.0,
    "learning_rate": 2.0940959395936975e-05
  },
  {
    "step": 6270,
    "epoch": 2.7745076344323967,
    "loss": 0.3673,
    "grad_norm": 2.265625,
    "learning_rate": 2.0872088470428553e-05
  },
  {
    "step": 6280,
    "epoch": 2.778933392343439,
    "loss": 0.4541,
    "grad_norm": 2.6875,
    "learning_rate": 2.080324973520344e-05
  },
  {
    "step": 6290,
    "epoch": 2.783359150254481,
    "loss": 0.3565,
    "grad_norm": 1.8984375,
    "learning_rate": 2.0734443727079943e-05
  },
  {
    "step": 6300,
    "epoch": 2.7877849081655235,
    "loss": 0.8752,
    "grad_norm": 5.4375,
    "learning_rate": 2.0665670982621105e-05
  },
  {
    "step": 6310,
    "epoch": 2.7922106660765658,
    "loss": 0.5448,
    "grad_norm": 3.5,
    "learning_rate": 2.0596932038130628e-05
  },
  {
    "step": 6320,
    "epoch": 2.796636423987608,
    "loss": 0.2114,
    "grad_norm": 4.1875,
    "learning_rate": 2.0528227429648604e-05
  },
  {
    "step": 6330,
    "epoch": 2.8010621818986503,
    "loss": 0.439,
    "grad_norm": 12.0,
    "learning_rate": 2.0459557692947367e-05
  },
  {
    "step": 6340,
    "epoch": 2.8054879398096926,
    "loss": 0.4511,
    "grad_norm": 4.9375,
    "learning_rate": 2.039092336352732e-05
  },
  {
    "step": 6350,
    "epoch": 2.8099136977207344,
    "loss": 0.4849,
    "grad_norm": 1.734375,
    "learning_rate": 2.0322324976612745e-05
  },
  {
    "step": 6360,
    "epoch": 2.814339455631777,
    "loss": 0.4002,
    "grad_norm": 8.1875,
    "learning_rate": 2.0253763067147657e-05
  },
  {
    "step": 6370,
    "epoch": 2.818765213542819,
    "loss": 0.5048,
    "grad_norm": 4.40625,
    "learning_rate": 2.0185238169791585e-05
  },
  {
    "step": 6380,
    "epoch": 2.8231909714538617,
    "loss": 0.4289,
    "grad_norm": 3.5,
    "learning_rate": 2.011675081891545e-05
  },
  {
    "step": 6390,
    "epoch": 2.8276167293649035,
    "loss": 0.3601,
    "grad_norm": 2.6875,
    "learning_rate": 2.0048301548597363e-05
  },
  {
    "step": 6400,
    "epoch": 2.832042487275946,
    "loss": 0.4649,
    "grad_norm": 7.25,
    "learning_rate": 1.99798908926185e-05
  },
  {
    "step": 6410,
    "epoch": 2.836468245186988,
    "loss": 0.5443,
    "grad_norm": 31.375,
    "learning_rate": 1.9911519384458893e-05
  },
  {
    "step": 6420,
    "epoch": 2.8408940030980308,
    "loss": 0.4963,
    "grad_norm": 2.21875,
    "learning_rate": 1.9843187557293284e-05
  },
  {
    "step": 6430,
    "epoch": 2.8453197610090726,
    "loss": 0.5358,
    "grad_norm": 5.375,
    "learning_rate": 1.9774895943987007e-05
  },
  {
    "step": 6440,
    "epoch": 2.8497455189201153,
    "loss": 0.4093,
    "grad_norm": 0.9375,
    "learning_rate": 1.9706645077091767e-05
  },
  {
    "step": 6450,
    "epoch": 2.854171276831157,
    "loss": 0.3976,
    "grad_norm": 0.8671875,
    "learning_rate": 1.9638435488841546e-05
  },
  {
    "step": 6460,
    "epoch": 2.8585970347422,
    "loss": 0.4953,
    "grad_norm": 1.6640625,
    "learning_rate": 1.9570267711148403e-05
  },
  {
    "step": 6470,
    "epoch": 2.8630227926532417,
    "loss": 0.4981,
    "grad_norm": 2.25,
    "learning_rate": 1.950214227559837e-05
  },
  {
    "step": 6480,
    "epoch": 2.8674485505642844,
    "loss": 0.6607,
    "grad_norm": 2.78125,
    "learning_rate": 1.9434059713447265e-05
  },
  {
    "step": 6490,
    "epoch": 2.871874308475326,
    "loss": 0.3518,
    "grad_norm": 0.97265625,
    "learning_rate": 1.9366020555616603e-05
  },
  {
    "step": 6500,
    "epoch": 2.876300066386369,
    "loss": 0.6168,
    "grad_norm": 2.140625,
    "learning_rate": 1.9298025332689397e-05
  },
  {
    "step": 6510,
    "epoch": 2.8807258242974108,
    "loss": 0.4194,
    "grad_norm": 4.21875,
    "learning_rate": 1.9230074574906042e-05
  },
  {
    "step": 6520,
    "epoch": 2.8851515822084535,
    "loss": 0.5905,
    "grad_norm": 7.59375,
    "learning_rate": 1.9162168812160218e-05
  },
  {
    "step": 6530,
    "epoch": 2.8895773401194953,
    "loss": 0.3628,
    "grad_norm": 2.390625,
    "learning_rate": 1.9094308573994692e-05
  },
  {
    "step": 6540,
    "epoch": 2.8940030980305376,
    "loss": 0.3738,
    "grad_norm": 2.75,
    "learning_rate": 1.9026494389597238e-05
  },
  {
    "step": 6550,
    "epoch": 2.89842885594158,
    "loss": 0.437,
    "grad_norm": 1.890625,
    "learning_rate": 1.8958726787796477e-05
  },
  {
    "step": 6560,
    "epoch": 2.902854613852622,
    "loss": 0.5226,
    "grad_norm": 4.625,
    "learning_rate": 1.8891006297057798e-05
  },
  {
    "step": 6570,
    "epoch": 2.9072803717636644,
    "loss": 0.4131,
    "grad_norm": 2.625,
    "learning_rate": 1.8823333445479174e-05
  },
  {
    "step": 6580,
    "epoch": 2.9117061296747067,
    "loss": 0.2729,
    "grad_norm": 2.96875,
    "learning_rate": 1.8755708760787113e-05
  },
  {
    "step": 6590,
    "epoch": 2.916131887585749,
    "loss": 0.4313,
    "grad_norm": 5.8125,
    "learning_rate": 1.8688132770332476e-05
  },
  {
    "step": 6600,
    "epoch": 2.920557645496791,
    "loss": 0.3058,
    "grad_norm": 3.296875,
    "learning_rate": 1.862060600108642e-05
  },
  {
    "step": 6610,
    "epoch": 2.9249834034078335,
    "loss": 0.5165,
    "grad_norm": 6.9375,
    "learning_rate": 1.8553128979636243e-05
  },
  {
    "step": 6620,
    "epoch": 2.9294091613188757,
    "loss": 0.538,
    "grad_norm": 3.625,
    "learning_rate": 1.848570223218133e-05
  },
  {
    "step": 6630,
    "epoch": 2.933834919229918,
    "loss": 0.343,
    "grad_norm": 8.3125,
    "learning_rate": 1.8418326284528996e-05
  },
  {
    "step": 6640,
    "epoch": 2.9382606771409603,
    "loss": 0.9382,
    "grad_norm": 0.52734375,
    "learning_rate": 1.8351001662090412e-05
  },
  {
    "step": 6650,
    "epoch": 2.9426864350520026,
    "loss": 0.4995,
    "grad_norm": 4.78125,
    "learning_rate": 1.8283728889876513e-05
  },
  {
    "step": 6660,
    "epoch": 2.947112192963045,
    "loss": 0.3004,
    "grad_norm": 3.296875,
    "learning_rate": 1.8216508492493886e-05
  },
  {
    "step": 6670,
    "epoch": 2.951537950874087,
    "loss": 0.3156,
    "grad_norm": 7.625,
    "learning_rate": 1.8149340994140702e-05
  },
  {
    "step": 6680,
    "epoch": 2.9559637087851294,
    "loss": 0.388,
    "grad_norm": 2.59375,
    "learning_rate": 1.8082226918602585e-05
  },
  {
    "step": 6690,
    "epoch": 2.9603894666961716,
    "loss": 0.5624,
    "grad_norm": 3.78125,
    "learning_rate": 1.8015166789248604e-05
  },
  {
    "step": 6700,
    "epoch": 2.964815224607214,
    "loss": 0.7982,
    "grad_norm": 2.984375,
    "learning_rate": 1.7948161129027094e-05
  },
  {
    "step": 6710,
    "epoch": 2.969240982518256,
    "loss": 0.7411,
    "grad_norm": 1.03125,
    "learning_rate": 1.788121046046167e-05
  },
  {
    "step": 6720,
    "epoch": 2.9736667404292985,
    "loss": 0.447,
    "grad_norm": 2.984375,
    "learning_rate": 1.7814315305647093e-05
  },
  {
    "step": 6730,
    "epoch": 2.9780924983403407,
    "loss": 0.6137,
    "grad_norm": 6.65625,
    "learning_rate": 1.774747618624521e-05
  },
  {
    "step": 6740,
    "epoch": 2.982518256251383,
    "loss": 0.5246,
    "grad_norm": 4.03125,
    "learning_rate": 1.768069362348091e-05
  },
  {
    "step": 6750,
    "epoch": 2.9869440141624253,
    "loss": 0.2556,
    "grad_norm": 4.4375,
    "learning_rate": 1.7613968138138026e-05
  },
  {
    "step": 6760,
    "epoch": 2.9913697720734675,
    "loss": 0.4984,
    "grad_norm": 1.2890625,
    "learning_rate": 1.7547300250555304e-05
  },
  {
    "step": 6770,
    "epoch": 2.99579552998451,
    "loss": 0.5407,
    "grad_norm": 9.0625,
    "learning_rate": 1.7480690480622303e-05
  },
  {
    "step": 6780,
    "epoch": 3.0,
    "loss": 0.3937,
    "grad_norm": 2.625,
    "learning_rate": 1.741413934777542e-05
  },
  {
    "step": 6790,
    "epoch": 3.0044257579110423,
    "loss": 0.5398,
    "grad_norm": 6.71875,
    "learning_rate": 1.7347647370993738e-05
  },
  {
    "step": 6800,
    "epoch": 3.0088515158220845,
    "loss": 0.7424,
    "grad_norm": 6.71875,
    "learning_rate": 1.7281215068795055e-05
  },
  {
    "step": 6810,
    "epoch": 3.013277273733127,
    "loss": 0.4532,
    "grad_norm": 5.9375,
    "learning_rate": 1.7214842959231794e-05
  },
  {
    "step": 6820,
    "epoch": 3.017703031644169,
    "loss": 0.2852,
    "grad_norm": 32.75,
    "learning_rate": 1.7148531559887018e-05
  },
  {
    "step": 6830,
    "epoch": 3.0221287895552114,
    "loss": 0.4989,
    "grad_norm": 9.9375,
    "learning_rate": 1.7082281387870338e-05
  },
  {
    "step": 6840,
    "epoch": 3.0265545474662536,
    "loss": 0.4005,
    "grad_norm": 0.8359375,
    "learning_rate": 1.7016092959813893e-05
  },
  {
    "step": 6850,
    "epoch": 3.030980305377296,
    "loss": 0.4867,
    "grad_norm": 1.125,
    "learning_rate": 1.694996679186835e-05
  },
  {
    "step": 6860,
    "epoch": 3.035406063288338,
    "loss": 0.6781,
    "grad_norm": 2.6875,
    "learning_rate": 1.6883903399698833e-05
  },
  {
    "step": 6870,
    "epoch": 3.0398318211993804,
    "loss": 0.4169,
    "grad_norm": 2.640625,
    "learning_rate": 1.681790329848097e-05
  },
  {
    "step": 6880,
    "epoch": 3.0442575791104227,
    "loss": 0.4741,
    "grad_norm": 3.921875,
    "learning_rate": 1.675196700289679e-05
  },
  {
    "step": 6890,
    "epoch": 3.048683337021465,
    "loss": 0.6338,
    "grad_norm": 6.875,
    "learning_rate": 1.6686095027130783e-05
  },
  {
    "step": 6900,
    "epoch": 3.0531090949325073,
    "loss": 0.4497,
    "grad_norm": 4.84375,
    "learning_rate": 1.662028788486583e-05
  },
  {
    "step": 6910,
    "epoch": 3.0575348528435495,
    "loss": 0.4252,
    "grad_norm": 2.3125,
    "learning_rate": 1.6554546089279273e-05
  },
  {
    "step": 6920,
    "epoch": 3.061960610754592,
    "loss": 0.3963,
    "grad_norm": 2.40625,
    "learning_rate": 1.6488870153038815e-05
  },
  {
    "step": 6930,
    "epoch": 3.066386368665634,
    "loss": 0.6541,
    "grad_norm": 5.90625,
    "learning_rate": 1.642326058829861e-05
  },
  {
    "step": 6940,
    "epoch": 3.0708121265766763,
    "loss": 0.3954,
    "grad_norm": 1.890625,
    "learning_rate": 1.635771790669523e-05
  },
  {
    "step": 6950,
    "epoch": 3.0752378844877186,
    "loss": 0.602,
    "grad_norm": 5.0,
    "learning_rate": 1.629224261934366e-05
  },
  {
    "step": 6960,
    "epoch": 3.079663642398761,
    "loss": 0.6039,
    "grad_norm": 3.640625,
    "learning_rate": 1.6226835236833354e-05
  },
  {
    "step": 6970,
    "epoch": 3.084089400309803,
    "loss": 0.7319,
    "grad_norm": 1.6953125,
    "learning_rate": 1.6161496269224208e-05
  },
  {
    "step": 6980,
    "epoch": 3.0885151582208454,
    "loss": 0.4647,
    "grad_norm": 2.890625,
    "learning_rate": 1.6096226226042632e-05
  },
  {
    "step": 6990,
    "epoch": 3.0929409161318877,
    "loss": 0.4977,
    "grad_norm": 3.375,
    "learning_rate": 1.603102561627751e-05
  },
  {
    "step": 7000,
    "epoch": 3.09736667404293,
    "loss": 0.5376,
    "grad_norm": 5.09375,
    "learning_rate": 1.5965894948376326e-05
  },
  {
    "step": 7000,
    "epoch": 3.09736667404293,
    "eval_loss": 0.5004762411117554,
    "eval_runtime": 103.5283,
    "eval_samples_per_second": 10.915,
    "eval_steps_per_second": 10.915
  },
  {
    "step": 7010,
    "epoch": 3.1017924319539723,
    "loss": 0.3205,
    "grad_norm": 2.3125,
    "learning_rate": 1.5900834730241088e-05
  },
  {
    "step": 7020,
    "epoch": 3.1062181898650145,
    "loss": 0.5061,
    "grad_norm": 1.5546875,
    "learning_rate": 1.5835845469224447e-05
  },
  {
    "step": 7030,
    "epoch": 3.110643947776057,
    "loss": 0.6089,
    "grad_norm": 5.0625,
    "learning_rate": 1.5770927672125735e-05
  },
  {
    "step": 7040,
    "epoch": 3.115069705687099,
    "loss": 0.517,
    "grad_norm": 0.78515625,
    "learning_rate": 1.5706081845186954e-05
  },
  {
    "step": 7050,
    "epoch": 3.1194954635981413,
    "loss": 0.4301,
    "grad_norm": 3.890625,
    "learning_rate": 1.56413084940889e-05
  },
  {
    "step": 7060,
    "epoch": 3.1239212215091836,
    "loss": 0.4062,
    "grad_norm": 6.875,
    "learning_rate": 1.5576608123947166e-05
  },
  {
    "step": 7070,
    "epoch": 3.128346979420226,
    "loss": 0.5211,
    "grad_norm": 1.25,
    "learning_rate": 1.5511981239308256e-05
  },
  {
    "step": 7080,
    "epoch": 3.132772737331268,
    "loss": 0.4253,
    "grad_norm": 1.6171875,
    "learning_rate": 1.5447428344145563e-05
  },
  {
    "step": 7090,
    "epoch": 3.1371984952423104,
    "loss": 0.6041,
    "grad_norm": 2.96875,
    "learning_rate": 1.5382949941855574e-05
  },
  {
    "step": 7100,
    "epoch": 3.1416242531533527,
    "loss": 0.3177,
    "grad_norm": 0.08837890625,
    "learning_rate": 1.5318546535253785e-05
  },
  {
    "step": 7110,
    "epoch": 3.146050011064395,
    "loss": 0.3669,
    "grad_norm": 4.65625,
    "learning_rate": 1.5254218626570926e-05
  },
  {
    "step": 7120,
    "epoch": 3.1504757689754372,
    "loss": 0.4639,
    "grad_norm": 6.65625,
    "learning_rate": 1.5189966717448923e-05
  },
  {
    "step": 7130,
    "epoch": 3.1549015268864795,
    "loss": 0.4222,
    "grad_norm": 0.63671875,
    "learning_rate": 1.5125791308937092e-05
  },
  {
    "step": 7140,
    "epoch": 3.159327284797522,
    "loss": 0.5392,
    "grad_norm": 6.90625,
    "learning_rate": 1.5061692901488162e-05
  },
  {
    "step": 7150,
    "epoch": 3.1637530427085636,
    "loss": 0.3661,
    "grad_norm": 6.71875,
    "learning_rate": 1.4997671994954371e-05
  },
  {
    "step": 7160,
    "epoch": 3.1681788006196063,
    "loss": 0.2801,
    "grad_norm": 3.71875,
    "learning_rate": 1.4933729088583626e-05
  },
  {
    "step": 7170,
    "epoch": 3.172604558530648,
    "loss": 0.2623,
    "grad_norm": 2.375,
    "learning_rate": 1.486986468101555e-05
  },
  {
    "step": 7180,
    "epoch": 3.177030316441691,
    "loss": 0.2983,
    "grad_norm": 0.67578125,
    "learning_rate": 1.4806079270277623e-05
  },
  {
    "step": 7190,
    "epoch": 3.1814560743527327,
    "loss": 0.4394,
    "grad_norm": 4.71875,
    "learning_rate": 1.4742373353781285e-05
  },
  {
    "step": 7200,
    "epoch": 3.185881832263775,
    "loss": 0.3344,
    "grad_norm": 7.8125,
    "learning_rate": 1.4678747428318079e-05
  },
  {
    "step": 7210,
    "epoch": 3.1903075901748172,
    "loss": 0.4744,
    "grad_norm": 17.625,
    "learning_rate": 1.461520199005574e-05
  },
  {
    "step": 7220,
    "epoch": 3.1947333480858595,
    "loss": 0.3685,
    "grad_norm": 1.09375,
    "learning_rate": 1.4551737534534383e-05
  },
  {
    "step": 7230,
    "epoch": 3.199159105996902,
    "loss": 0.3714,
    "grad_norm": 5.75,
    "learning_rate": 1.4488354556662554e-05
  },
  {
    "step": 7240,
    "epoch": 3.203584863907944,
    "loss": 0.6242,
    "grad_norm": 2.453125,
    "learning_rate": 1.4425053550713458e-05
  },
  {
    "step": 7250,
    "epoch": 3.2080106218189863,
    "loss": 0.675,
    "grad_norm": 12.625,
    "learning_rate": 1.4361835010321067e-05
  },
  {
    "step": 7260,
    "epoch": 3.2124363797300286,
    "loss": 0.6875,
    "grad_norm": 5.375,
    "learning_rate": 1.4298699428476236e-05
  },
  {
    "step": 7270,
    "epoch": 3.216862137641071,
    "loss": 0.3659,
    "grad_norm": 6.125,
    "learning_rate": 1.4235647297522942e-05
  },
  {
    "step": 7280,
    "epoch": 3.221287895552113,
    "loss": 0.4529,
    "grad_norm": 10.1875,
    "learning_rate": 1.4172679109154349e-05
  },
  {
    "step": 7290,
    "epoch": 3.2257136534631554,
    "loss": 0.4678,
    "grad_norm": 2.640625,
    "learning_rate": 1.4109795354409044e-05
  },
  {
    "step": 7300,
    "epoch": 3.2301394113741977,
    "loss": 0.5079,
    "grad_norm": 7.4375,
    "learning_rate": 1.4046996523667166e-05
  },
  {
    "step": 7310,
    "epoch": 3.23456516928524,
    "loss": 0.3693,
    "grad_norm": 10.1875,
    "learning_rate": 1.3984283106646636e-05
  },
  {
    "step": 7320,
    "epoch": 3.2389909271962822,
    "loss": 0.262,
    "grad_norm": 3.03125,
    "learning_rate": 1.3921655592399254e-05
  },
  {
    "step": 7330,
    "epoch": 3.2434166851073245,
    "loss": 0.4486,
    "grad_norm": 2.15625,
    "learning_rate": 1.3859114469306977e-05
  },
  {
    "step": 7340,
    "epoch": 3.2478424430183668,
    "loss": 0.4544,
    "grad_norm": 4.78125,
    "learning_rate": 1.379666022507804e-05
  },
  {
    "step": 7350,
    "epoch": 3.252268200929409,
    "loss": 0.5666,
    "grad_norm": 4.0625,
    "learning_rate": 1.3734293346743168e-05
  },
  {
    "step": 7360,
    "epoch": 3.2566939588404513,
    "loss": 0.4891,
    "grad_norm": 4.28125,
    "learning_rate": 1.3672014320651832e-05
  },
  {
    "step": 7370,
    "epoch": 3.2611197167514936,
    "loss": 0.4546,
    "grad_norm": 2.453125,
    "learning_rate": 1.3609823632468366e-05
  },
  {
    "step": 7380,
    "epoch": 3.265545474662536,
    "loss": 0.3112,
    "grad_norm": 0.4765625,
    "learning_rate": 1.3547721767168272e-05
  },
  {
    "step": 7390,
    "epoch": 3.269971232573578,
    "loss": 0.4376,
    "grad_norm": 5.4375,
    "learning_rate": 1.3485709209034347e-05
  },
  {
    "step": 7400,
    "epoch": 3.2743969904846204,
    "loss": 0.3426,
    "grad_norm": 1.0390625,
    "learning_rate": 1.3423786441652998e-05
  },
  {
    "step": 7410,
    "epoch": 3.2788227483956627,
    "loss": 0.7916,
    "grad_norm": 6.21875,
    "learning_rate": 1.3361953947910394e-05
  },
  {
    "step": 7420,
    "epoch": 3.283248506306705,
    "loss": 0.5014,
    "grad_norm": 2.96875,
    "learning_rate": 1.330021220998874e-05
  },
  {
    "step": 7430,
    "epoch": 3.287674264217747,
    "loss": 0.5273,
    "grad_norm": 2.46875,
    "learning_rate": 1.3238561709362504e-05
  },
  {
    "step": 7440,
    "epoch": 3.2921000221287895,
    "loss": 0.5563,
    "grad_norm": 4.4375,
    "learning_rate": 1.3177002926794685e-05
  },
  {
    "step": 7450,
    "epoch": 3.2965257800398318,
    "loss": 0.6607,
    "grad_norm": 4.75,
    "learning_rate": 1.311553634233304e-05
  },
  {
    "step": 7460,
    "epoch": 3.300951537950874,
    "loss": 0.459,
    "grad_norm": 8.0,
    "learning_rate": 1.3054162435306336e-05
  },
  {
    "step": 7470,
    "epoch": 3.3053772958619163,
    "loss": 0.5532,
    "grad_norm": 3.234375,
    "learning_rate": 1.2992881684320627e-05
  },
  {
    "step": 7480,
    "epoch": 3.3098030537729586,
    "loss": 0.6066,
    "grad_norm": 1.34375,
    "learning_rate": 1.2931694567255512e-05
  },
  {
    "step": 7490,
    "epoch": 3.314228811684001,
    "loss": 0.7742,
    "grad_norm": 2.453125,
    "learning_rate": 1.2870601561260437e-05
  },
  {
    "step": 7500,
    "epoch": 3.318654569595043,
    "loss": 0.3529,
    "grad_norm": 19.625,
    "learning_rate": 1.280960314275092e-05
  },
  {
    "step": 7510,
    "epoch": 3.3230803275060854,
    "loss": 0.7346,
    "grad_norm": 8.75,
    "learning_rate": 1.2748699787404893e-05
  },
  {
    "step": 7520,
    "epoch": 3.3275060854171277,
    "loss": 0.7,
    "grad_norm": 4.4375,
    "learning_rate": 1.2687891970158949e-05
  },
  {
    "step": 7530,
    "epoch": 3.33193184332817,
    "loss": 0.3747,
    "grad_norm": 7.28125,
    "learning_rate": 1.2627180165204671e-05
  },
  {
    "step": 7540,
    "epoch": 3.336357601239212,
    "loss": 0.3715,
    "grad_norm": 7.25,
    "learning_rate": 1.2566564845984902e-05
  },
  {
    "step": 7550,
    "epoch": 3.3407833591502545,
    "loss": 0.2926,
    "grad_norm": 2.40625,
    "learning_rate": 1.250604648519007e-05
  },
  {
    "step": 7560,
    "epoch": 3.3452091170612968,
    "loss": 0.2652,
    "grad_norm": 3.6875,
    "learning_rate": 1.2445625554754525e-05
  },
  {
    "step": 7570,
    "epoch": 3.349634874972339,
    "loss": 0.2951,
    "grad_norm": 0.703125,
    "learning_rate": 1.2385302525852804e-05
  },
  {
    "step": 7580,
    "epoch": 3.3540606328833813,
    "loss": 0.2119,
    "grad_norm": 1.828125,
    "learning_rate": 1.2325077868896023e-05
  },
  {
    "step": 7590,
    "epoch": 3.3584863907944236,
    "loss": 0.3413,
    "grad_norm": 0.0048828125,
    "learning_rate": 1.2264952053528144e-05
  },
  {
    "step": 7600,
    "epoch": 3.362912148705466,
    "loss": 0.5228,
    "grad_norm": 1.890625,
    "learning_rate": 1.2204925548622353e-05
  },
  {
    "step": 7610,
    "epoch": 3.367337906616508,
    "loss": 0.3301,
    "grad_norm": 7.1875,
    "learning_rate": 1.2144998822277387e-05
  },
  {
    "step": 7620,
    "epoch": 3.3717636645275504,
    "loss": 0.8146,
    "grad_norm": 4.65625,
    "learning_rate": 1.2085172341813911e-05
  },
  {
    "step": 7630,
    "epoch": 3.3761894224385927,
    "loss": 0.3565,
    "grad_norm": 1.7890625,
    "learning_rate": 1.202544657377082e-05
  },
  {
    "step": 7640,
    "epoch": 3.380615180349635,
    "loss": 0.5855,
    "grad_norm": 0.6875,
    "learning_rate": 1.196582198390166e-05
  },
  {
    "step": 7650,
    "epoch": 3.385040938260677,
    "loss": 0.3617,
    "grad_norm": 2.65625,
    "learning_rate": 1.190629903717097e-05
  },
  {
    "step": 7660,
    "epoch": 3.3894666961717195,
    "loss": 0.4649,
    "grad_norm": 4.5625,
    "learning_rate": 1.1846878197750636e-05
  },
  {
    "step": 7670,
    "epoch": 3.3938924540827617,
    "loss": 0.3082,
    "grad_norm": 5.3125,
    "learning_rate": 1.17875599290163e-05
  },
  {
    "step": 7680,
    "epoch": 3.398318211993804,
    "loss": 0.4125,
    "grad_norm": 6.5625,
    "learning_rate": 1.1728344693543731e-05
  },
  {
    "step": 7690,
    "epoch": 3.4027439699048463,
    "loss": 0.6312,
    "grad_norm": 4.59375,
    "learning_rate": 1.1669232953105244e-05
  },
  {
    "step": 7700,
    "epoch": 3.4071697278158886,
    "loss": 0.4537,
    "grad_norm": 3.140625,
    "learning_rate": 1.1610225168666047e-05
  },
  {
    "step": 7710,
    "epoch": 3.411595485726931,
    "loss": 0.3008,
    "grad_norm": 4.9375,
    "learning_rate": 1.155132180038072e-05
  },
  {
    "step": 7720,
    "epoch": 3.416021243637973,
    "loss": 0.7273,
    "grad_norm": 15.25,
    "learning_rate": 1.1492523307589547e-05
  },
  {
    "step": 7730,
    "epoch": 3.4204470015490154,
    "loss": 0.3241,
    "grad_norm": 1.8515625,
    "learning_rate": 1.1433830148814989e-05
  },
  {
    "step": 7740,
    "epoch": 3.4248727594600576,
    "loss": 0.2682,
    "grad_norm": 5.90625,
    "learning_rate": 1.1375242781758077e-05
  },
  {
    "step": 7750,
    "epoch": 3.4292985173711,
    "loss": 0.3001,
    "grad_norm": 6.0,
    "learning_rate": 1.131676166329488e-05
  },
  {
    "step": 7760,
    "epoch": 3.433724275282142,
    "loss": 0.4188,
    "grad_norm": 1.109375,
    "learning_rate": 1.1258387249472916e-05
  },
  {
    "step": 7770,
    "epoch": 3.4381500331931845,
    "loss": 0.3166,
    "grad_norm": 1.6640625,
    "learning_rate": 1.1200119995507572e-05
  },
  {
    "step": 7780,
    "epoch": 3.4425757911042267,
    "loss": 0.3844,
    "grad_norm": 8.375,
    "learning_rate": 1.1141960355778614e-05
  },
  {
    "step": 7790,
    "epoch": 3.447001549015269,
    "loss": 0.4637,
    "grad_norm": 2.96875,
    "learning_rate": 1.1083908783826587e-05
  },
  {
    "step": 7800,
    "epoch": 3.4514273069263113,
    "loss": 0.4441,
    "grad_norm": 3.3125,
    "learning_rate": 1.1025965732349316e-05
  },
  {
    "step": 7810,
    "epoch": 3.4558530648373535,
    "loss": 0.4325,
    "grad_norm": 4.03125,
    "learning_rate": 1.0968131653198343e-05
  },
  {
    "step": 7820,
    "epoch": 3.460278822748396,
    "loss": 0.4131,
    "grad_norm": 4.4375,
    "learning_rate": 1.0910406997375452e-05
  },
  {
    "step": 7830,
    "epoch": 3.464704580659438,
    "loss": 0.3946,
    "grad_norm": 4.8125,
    "learning_rate": 1.0852792215029089e-05
  },
  {
    "step": 7840,
    "epoch": 3.4691303385704804,
    "loss": 0.6399,
    "grad_norm": 4.28125,
    "learning_rate": 1.079528775545092e-05
  },
  {
    "step": 7850,
    "epoch": 3.4735560964815226,
    "loss": 0.2869,
    "grad_norm": 1.921875,
    "learning_rate": 1.0737894067072258e-05
  },
  {
    "step": 7860,
    "epoch": 3.477981854392565,
    "loss": 0.6308,
    "grad_norm": 3.171875,
    "learning_rate": 1.0680611597460608e-05
  },
  {
    "step": 7870,
    "epoch": 3.482407612303607,
    "loss": 0.477,
    "grad_norm": 2.25,
    "learning_rate": 1.0623440793316191e-05
  },
  {
    "step": 7880,
    "epoch": 3.4868333702146495,
    "loss": 0.3374,
    "grad_norm": 2.34375,
    "learning_rate": 1.0566382100468406e-05
  },
  {
    "step": 7890,
    "epoch": 3.4912591281256917,
    "loss": 0.5414,
    "grad_norm": 3.328125,
    "learning_rate": 1.0509435963872422e-05
  },
  {
    "step": 7900,
    "epoch": 3.4956848860367336,
    "loss": 0.4371,
    "grad_norm": 6.34375,
    "learning_rate": 1.045260282760563e-05
  },
  {
    "step": 7910,
    "epoch": 3.5001106439477763,
    "loss": 0.4862,
    "grad_norm": 5.4375,
    "learning_rate": 1.0395883134864259e-05
  },
  {
    "step": 7920,
    "epoch": 3.504536401858818,
    "loss": 0.5369,
    "grad_norm": 1.6328125,
    "learning_rate": 1.0339277327959863e-05
  },
  {
    "step": 7930,
    "epoch": 3.508962159769861,
    "loss": 0.4987,
    "grad_norm": 13.8125,
    "learning_rate": 1.0282785848315894e-05
  },
  {
    "step": 7940,
    "epoch": 3.5133879176809026,
    "loss": 0.8817,
    "grad_norm": 4.0,
    "learning_rate": 1.0226409136464245e-05
  },
  {
    "step": 7950,
    "epoch": 3.5178136755919454,
    "loss": 0.894,
    "grad_norm": 0.88671875,
    "learning_rate": 1.0170147632041856e-05
  },
  {
    "step": 7960,
    "epoch": 3.522239433502987,
    "loss": 0.5568,
    "grad_norm": 3.921875,
    "learning_rate": 1.0114001773787243e-05
  },
  {
    "step": 7970,
    "epoch": 3.52666519141403,
    "loss": 0.6114,
    "grad_norm": 2.671875,
    "learning_rate": 1.0057971999537082e-05
  },
  {
    "step": 7980,
    "epoch": 3.5310909493250717,
    "loss": 0.5701,
    "grad_norm": 4.03125,
    "learning_rate": 1.0002058746222806e-05
  },
  {
    "step": 7990,
    "epoch": 3.5355167072361144,
    "loss": 0.4928,
    "grad_norm": 3.78125,
    "learning_rate": 9.946262449867189e-06
  },
  {
    "step": 8000,
    "epoch": 3.5399424651471563,
    "loss": 0.3504,
    "grad_norm": 1.5859375,
    "learning_rate": 9.890583545580972e-06
  },
  {
    "step": 8000,
    "epoch": 3.5399424651471563,
    "eval_loss": 0.5136820077896118,
    "eval_runtime": 103.1411,
    "eval_samples_per_second": 10.956,
    "eval_steps_per_second": 10.956
  },
  {
    "step": 8010,
    "epoch": 3.544368223058199,
    "loss": 0.1488,
    "grad_norm": 1.3046875,
    "learning_rate": 9.83502246755942e-06
  },
  {
    "step": 8020,
    "epoch": 3.548793980969241,
    "loss": 0.3704,
    "grad_norm": 3.359375,
    "learning_rate": 9.779579649079e-06
  },
  {
    "step": 8030,
    "epoch": 3.553219738880283,
    "loss": 0.4155,
    "grad_norm": 5.96875,
    "learning_rate": 9.72425552249393e-06
  },
  {
    "step": 8040,
    "epoch": 3.5576454967913254,
    "loss": 0.5114,
    "grad_norm": 4.15625,
    "learning_rate": 9.669050519232876e-06
  },
  {
    "step": 8050,
    "epoch": 3.5620712547023676,
    "loss": 0.721,
    "grad_norm": 8.625,
    "learning_rate": 9.61396506979554e-06
  },
  {
    "step": 8060,
    "epoch": 3.56649701261341,
    "loss": 0.5821,
    "grad_norm": 2.09375,
    "learning_rate": 9.558999603749305e-06
  },
  {
    "step": 8070,
    "epoch": 3.570922770524452,
    "loss": 0.5546,
    "grad_norm": 4.46875,
    "learning_rate": 9.504154549725943e-06
  },
  {
    "step": 8080,
    "epoch": 3.5753485284354944,
    "loss": 0.2775,
    "grad_norm": 1.2734375,
    "learning_rate": 9.449430335418183e-06
  },
  {
    "step": 8090,
    "epoch": 3.5797742863465367,
    "loss": 0.4795,
    "grad_norm": 3.640625,
    "learning_rate": 9.394827387576455e-06
  },
  {
    "step": 8100,
    "epoch": 3.584200044257579,
    "loss": 0.6271,
    "grad_norm": 2.90625,
    "learning_rate": 9.340346132005506e-06
  },
  {
    "step": 8110,
    "epoch": 3.5886258021686213,
    "loss": 0.6804,
    "grad_norm": 9.375,
    "learning_rate": 9.285986993561116e-06
  },
  {
    "step": 8120,
    "epoch": 3.5930515600796635,
    "loss": 0.4312,
    "grad_norm": 4.625,
    "learning_rate": 9.231750396146754e-06
  },
  {
    "step": 8130,
    "epoch": 3.597477317990706,
    "loss": 0.6598,
    "grad_norm": 2.875,
    "learning_rate": 9.177636762710321e-06
  },
  {
    "step": 8140,
    "epoch": 3.601903075901748,
    "loss": 0.4189,
    "grad_norm": 0.4375,
    "learning_rate": 9.123646515240783e-06
  },
  {
    "step": 8150,
    "epoch": 3.6063288338127903,
    "loss": 0.407,
    "grad_norm": 20.625,
    "learning_rate": 9.06978007476495e-06
  },
  {
    "step": 8160,
    "epoch": 3.6107545917238326,
    "loss": 0.4997,
    "grad_norm": 4.03125,
    "learning_rate": 9.016037861344129e-06
  },
  {
    "step": 8170,
    "epoch": 3.615180349634875,
    "loss": 0.2354,
    "grad_norm": 2.15625,
    "learning_rate": 8.962420294070914e-06
  },
  {
    "step": 8180,
    "epoch": 3.619606107545917,
    "loss": 0.3806,
    "grad_norm": 2.328125,
    "learning_rate": 8.908927791065861e-06
  },
  {
    "step": 8190,
    "epoch": 3.6240318654569594,
    "loss": 0.4113,
    "grad_norm": 1.8984375,
    "learning_rate": 8.855560769474236e-06
  },
  {
    "step": 8200,
    "epoch": 3.6284576233680017,
    "loss": 0.4887,
    "grad_norm": 2.296875,
    "learning_rate": 8.80231964546282e-06
  },
  {
    "step": 8210,
    "epoch": 3.632883381279044,
    "loss": 0.5376,
    "grad_norm": 3.34375,
    "learning_rate": 8.749204834216568e-06
  },
  {
    "step": 8220,
    "epoch": 3.6373091391900862,
    "loss": 0.5668,
    "grad_norm": 15.75,
    "learning_rate": 8.69621674993547e-06
  },
  {
    "step": 8230,
    "epoch": 3.6417348971011285,
    "loss": 0.5688,
    "grad_norm": 6.875,
    "learning_rate": 8.643355805831243e-06
  },
  {
    "step": 8240,
    "epoch": 3.646160655012171,
    "loss": 0.5233,
    "grad_norm": 6.625,
    "learning_rate": 8.590622414124148e-06
  },
  {
    "step": 8250,
    "epoch": 3.650586412923213,
    "loss": 0.3779,
    "grad_norm": 4.46875,
    "learning_rate": 8.538016986039752e-06
  },
  {
    "step": 8260,
    "epoch": 3.6550121708342553,
    "loss": 0.5939,
    "grad_norm": 4.25,
    "learning_rate": 8.485539931805767e-06
  },
  {
    "step": 8270,
    "epoch": 3.6594379287452976,
    "loss": 0.6192,
    "grad_norm": 6.0625,
    "learning_rate": 8.433191660648807e-06
  },
  {
    "step": 8280,
    "epoch": 3.66386368665634,
    "loss": 0.5302,
    "grad_norm": 3.390625,
    "learning_rate": 8.380972580791191e-06
  },
  {
    "step": 8290,
    "epoch": 3.668289444567382,
    "loss": 0.3897,
    "grad_norm": 2.453125,
    "learning_rate": 8.32888309944781e-06
  },
  {
    "step": 8300,
    "epoch": 3.6727152024784244,
    "loss": 0.4805,
    "grad_norm": 4.34375,
    "learning_rate": 8.276923622822894e-06
  },
  {
    "step": 8310,
    "epoch": 3.6771409603894667,
    "loss": 0.4913,
    "grad_norm": 5.65625,
    "learning_rate": 8.22509455610688e-06
  },
  {
    "step": 8320,
    "epoch": 3.681566718300509,
    "loss": 0.3424,
    "grad_norm": 5.4375,
    "learning_rate": 8.173396303473232e-06
  },
  {
    "step": 8330,
    "epoch": 3.6859924762115512,
    "loss": 0.5632,
    "grad_norm": 8.1875,
    "learning_rate": 8.121829268075328e-06
  },
  {
    "step": 8340,
    "epoch": 3.6904182341225935,
    "loss": 0.7078,
    "grad_norm": 4.53125,
    "learning_rate": 8.070393852043251e-06
  },
  {
    "step": 8350,
    "epoch": 3.694843992033636,
    "loss": 0.3758,
    "grad_norm": 2.125,
    "learning_rate": 8.019090456480726e-06
  },
  {
    "step": 8360,
    "epoch": 3.699269749944678,
    "loss": 0.7475,
    "grad_norm": 9.0,
    "learning_rate": 7.967919481461925e-06
  },
  {
    "step": 8370,
    "epoch": 3.7036955078557203,
    "loss": 0.3293,
    "grad_norm": 2.046875,
    "learning_rate": 7.916881326028386e-06
  },
  {
    "step": 8380,
    "epoch": 3.7081212657667626,
    "loss": 0.3506,
    "grad_norm": 1.6328125,
    "learning_rate": 7.865976388185914e-06
  },
  {
    "step": 8390,
    "epoch": 3.712547023677805,
    "loss": 0.4362,
    "grad_norm": 8.5625,
    "learning_rate": 7.815205064901417e-06
  },
  {
    "step": 8400,
    "epoch": 3.716972781588847,
    "loss": 0.4496,
    "grad_norm": 1.8203125,
    "learning_rate": 7.7645677520999e-06
  },
  {
    "step": 8410,
    "epoch": 3.7213985394998894,
    "loss": 0.4496,
    "grad_norm": 1.4609375,
    "learning_rate": 7.714064844661276e-06
  },
  {
    "step": 8420,
    "epoch": 3.7258242974109317,
    "loss": 0.3162,
    "grad_norm": 0.3046875,
    "learning_rate": 7.663696736417387e-06
  },
  {
    "step": 8430,
    "epoch": 3.730250055321974,
    "loss": 0.4549,
    "grad_norm": 5.78125,
    "learning_rate": 7.613463820148831e-06
  },
  {
    "step": 8440,
    "epoch": 3.7346758132330162,
    "loss": 0.5159,
    "grad_norm": 3.84375,
    "learning_rate": 7.563366487581999e-06
  },
  {
    "step": 8450,
    "epoch": 3.7391015711440585,
    "loss": 0.7093,
    "grad_norm": 5.09375,
    "learning_rate": 7.513405129385942e-06
  },
  {
    "step": 8460,
    "epoch": 3.7435273290551008,
    "loss": 0.32,
    "grad_norm": 6.3125,
    "learning_rate": 7.463580135169379e-06
  },
  {
    "step": 8470,
    "epoch": 3.747953086966143,
    "loss": 0.5707,
    "grad_norm": 3.0,
    "learning_rate": 7.413891893477612e-06
  },
  {
    "step": 8480,
    "epoch": 3.7523788448771853,
    "loss": 0.4296,
    "grad_norm": 6.75,
    "learning_rate": 7.364340791789545e-06
  },
  {
    "step": 8490,
    "epoch": 3.7568046027882276,
    "loss": 0.3955,
    "grad_norm": 6.1875,
    "learning_rate": 7.314927216514616e-06
  },
  {
    "step": 8500,
    "epoch": 3.76123036069927,
    "loss": 0.2557,
    "grad_norm": 3.125,
    "learning_rate": 7.265651552989802e-06
  },
  {
    "step": 8510,
    "epoch": 3.765656118610312,
    "loss": 0.5171,
    "grad_norm": 7.3125,
    "learning_rate": 7.2165141854766385e-06
  },
  {
    "step": 8520,
    "epoch": 3.7700818765213544,
    "loss": 0.4675,
    "grad_norm": 12.25,
    "learning_rate": 7.1675154971581785e-06
  },
  {
    "step": 8530,
    "epoch": 3.7745076344323967,
    "loss": 0.4519,
    "grad_norm": 2.15625,
    "learning_rate": 7.118655870136048e-06
  },
  {
    "step": 8540,
    "epoch": 3.778933392343439,
    "loss": 0.4947,
    "grad_norm": 2.78125,
    "learning_rate": 7.06993568542742e-06
  },
  {
    "step": 8550,
    "epoch": 3.783359150254481,
    "loss": 0.5228,
    "grad_norm": 0.52734375,
    "learning_rate": 7.021355322962103e-06
  },
  {
    "step": 8560,
    "epoch": 3.7877849081655235,
    "loss": 0.5247,
    "grad_norm": 4.09375,
    "learning_rate": 6.972915161579491e-06
  },
  {
    "step": 8570,
    "epoch": 3.7922106660765658,
    "loss": 0.3283,
    "grad_norm": 4.09375,
    "learning_rate": 6.924615579025709e-06
  },
  {
    "step": 8580,
    "epoch": 3.796636423987608,
    "loss": 0.3664,
    "grad_norm": 5.625,
    "learning_rate": 6.876456951950614e-06
  },
  {
    "step": 8590,
    "epoch": 3.8010621818986503,
    "loss": 0.6535,
    "grad_norm": 3.015625,
    "learning_rate": 6.828439655904836e-06
  },
  {
    "step": 8600,
    "epoch": 3.8054879398096926,
    "loss": 0.3063,
    "grad_norm": 3.328125,
    "learning_rate": 6.780564065336911e-06
  },
  {
    "step": 8610,
    "epoch": 3.8099136977207344,
    "loss": 0.5363,
    "grad_norm": 5.8125,
    "learning_rate": 6.732830553590305e-06
  },
  {
    "step": 8620,
    "epoch": 3.814339455631777,
    "loss": 0.3765,
    "grad_norm": 1.90625,
    "learning_rate": 6.68523949290053e-06
  },
  {
    "step": 8630,
    "epoch": 3.818765213542819,
    "loss": 0.7746,
    "grad_norm": 3.84375,
    "learning_rate": 6.637791254392231e-06
  },
  {
    "step": 8640,
    "epoch": 3.8231909714538617,
    "loss": 0.6461,
    "grad_norm": 2.890625,
    "learning_rate": 6.590486208076318e-06
  },
  {
    "step": 8650,
    "epoch": 3.8276167293649035,
    "loss": 0.4918,
    "grad_norm": 3.15625,
    "learning_rate": 6.543324722847027e-06
  },
  {
    "step": 8660,
    "epoch": 3.832042487275946,
    "loss": 0.3174,
    "grad_norm": 3.03125,
    "learning_rate": 6.496307166479112e-06
  },
  {
    "step": 8670,
    "epoch": 3.836468245186988,
    "loss": 0.5055,
    "grad_norm": 7.375,
    "learning_rate": 6.449433905624916e-06
  },
  {
    "step": 8680,
    "epoch": 3.8408940030980308,
    "loss": 0.3442,
    "grad_norm": 4.0625,
    "learning_rate": 6.402705305811529e-06
  },
  {
    "step": 8690,
    "epoch": 3.8453197610090726,
    "loss": 1.0232,
    "grad_norm": 9.5,
    "learning_rate": 6.356121731437983e-06
  },
  {
    "step": 8700,
    "epoch": 3.8497455189201153,
    "loss": 0.3581,
    "grad_norm": 2.21875,
    "learning_rate": 6.309683545772327e-06
  },
  {
    "step": 8710,
    "epoch": 3.854171276831157,
    "loss": 0.4389,
    "grad_norm": 13.125,
    "learning_rate": 6.263391110948877e-06
  },
  {
    "step": 8720,
    "epoch": 3.8585970347422,
    "loss": 0.4018,
    "grad_norm": 5.3125,
    "learning_rate": 6.2172447879653265e-06
  },
  {
    "step": 8730,
    "epoch": 3.8630227926532417,
    "loss": 0.5633,
    "grad_norm": 1.359375,
    "learning_rate": 6.171244936679984e-06
  },
  {
    "step": 8740,
    "epoch": 3.8674485505642844,
    "loss": 0.5334,
    "grad_norm": 5.90625,
    "learning_rate": 6.125391915808926e-06
  },
  {
    "step": 8750,
    "epoch": 3.871874308475326,
    "loss": 0.5646,
    "grad_norm": 9.6875,
    "learning_rate": 6.079686082923219e-06
  },
  {
    "step": 8760,
    "epoch": 3.876300066386369,
    "loss": 0.3175,
    "grad_norm": 0.28125,
    "learning_rate": 6.034127794446121e-06
  },
  {
    "step": 8770,
    "epoch": 3.8807258242974108,
    "loss": 0.2605,
    "grad_norm": 3.046875,
    "learning_rate": 5.988717405650332e-06
  },
  {
    "step": 8780,
    "epoch": 3.8851515822084535,
    "loss": 0.4708,
    "grad_norm": 1.3984375,
    "learning_rate": 5.943455270655171e-06
  },
  {
    "step": 8790,
    "epoch": 3.8895773401194953,
    "loss": 0.7101,
    "grad_norm": 4.5,
    "learning_rate": 5.898341742423865e-06
  },
  {
    "step": 8800,
    "epoch": 3.8940030980305376,
    "loss": 0.4839,
    "grad_norm": 5.78125,
    "learning_rate": 5.853377172760791e-06
  },
  {
    "step": 8810,
    "epoch": 3.89842885594158,
    "loss": 0.4026,
    "grad_norm": 2.609375,
    "learning_rate": 5.808561912308658e-06
  },
  {
    "step": 8820,
    "epoch": 3.902854613852622,
    "loss": 0.4952,
    "grad_norm": 3.59375,
    "learning_rate": 5.763896310545894e-06
  },
  {
    "step": 8830,
    "epoch": 3.9072803717636644,
    "loss": 0.7441,
    "grad_norm": 11.875,
    "learning_rate": 5.71938071578382e-06
  },
  {
    "step": 8840,
    "epoch": 3.9117061296747067,
    "loss": 0.4141,
    "grad_norm": 3.734375,
    "learning_rate": 5.675015475164e-06
  },
  {
    "step": 8850,
    "epoch": 3.916131887585749,
    "loss": 0.6038,
    "grad_norm": 2.03125,
    "learning_rate": 5.63080093465548e-06
  },
  {
    "step": 8860,
    "epoch": 3.920557645496791,
    "loss": 0.3142,
    "grad_norm": 7.0625,
    "learning_rate": 5.586737439052142e-06
  },
  {
    "step": 8870,
    "epoch": 3.9249834034078335,
    "loss": 0.4606,
    "grad_norm": 6.25,
    "learning_rate": 5.542825331969967e-06
  },
  {
    "step": 8880,
    "epoch": 3.9294091613188757,
    "loss": 0.2624,
    "grad_norm": 1.6171875,
    "learning_rate": 5.499064955844382e-06
  },
  {
    "step": 8890,
    "epoch": 3.933834919229918,
    "loss": 0.2838,
    "grad_norm": 0.81640625,
    "learning_rate": 5.455456651927604e-06
  },
  {
    "step": 8900,
    "epoch": 3.9382606771409603,
    "loss": 0.4789,
    "grad_norm": 3.984375,
    "learning_rate": 5.412000760285935e-06
  },
  {
    "step": 8910,
    "epoch": 3.9426864350520026,
    "loss": 0.7169,
    "grad_norm": 21.125,
    "learning_rate": 5.368697619797159e-06
  },
  {
    "step": 8920,
    "epoch": 3.947112192963045,
    "loss": 0.4709,
    "grad_norm": 2.390625,
    "learning_rate": 5.325547568147857e-06
  },
  {
    "step": 8930,
    "epoch": 3.951537950874087,
    "loss": 0.5438,
    "grad_norm": 1.09375,
    "learning_rate": 5.282550941830799e-06
  },
  {
    "step": 8940,
    "epoch": 3.9559637087851294,
    "loss": 0.3393,
    "grad_norm": 0.0595703125,
    "learning_rate": 5.239708076142311e-06
  },
  {
    "step": 8950,
    "epoch": 3.9603894666961716,
    "loss": 0.6118,
    "grad_norm": 4.6875,
    "learning_rate": 5.1970193051796764e-06
  },
  {
    "step": 8960,
    "epoch": 3.964815224607214,
    "loss": 0.5645,
    "grad_norm": 2.875,
    "learning_rate": 5.1544849618384935e-06
  },
  {
    "step": 8970,
    "epoch": 3.969240982518256,
    "loss": 0.6353,
    "grad_norm": 0.9296875,
    "learning_rate": 5.112105377810128e-06
  },
  {
    "step": 8980,
    "epoch": 3.9736667404292985,
    "loss": 0.4506,
    "grad_norm": 7.1875,
    "learning_rate": 5.069880883579078e-06
  },
  {
    "step": 8990,
    "epoch": 3.9780924983403407,
    "loss": 0.6701,
    "grad_norm": 5.75,
    "learning_rate": 5.027811808420446e-06
  },
  {
    "step": 9000,
    "epoch": 3.982518256251383,
    "loss": 0.6779,
    "grad_norm": 1.1875,
    "learning_rate": 4.9858984803973215e-06
  },
  {
    "step": 9000,
    "epoch": 3.982518256251383,
    "eval_loss": 0.519167959690094,
    "eval_runtime": 104.8527,
    "eval_samples_per_second": 10.777,
    "eval_steps_per_second": 10.777
  },
  {
    "step": 9010,
    "epoch": 3.9869440141624253,
    "loss": 0.3262,
    "grad_norm": 4.125,
    "learning_rate": 4.9441412263582535e-06
  },
  {
    "step": 9020,
    "epoch": 3.9913697720734675,
    "loss": 0.3746,
    "grad_norm": 2.84375,
    "learning_rate": 4.902540371934703e-06
  },
  {
    "step": 9030,
    "epoch": 3.99579552998451,
    "loss": 0.3218,
    "grad_norm": 2.078125,
    "learning_rate": 4.861096241538482e-06
  },
  {
    "step": 9040,
    "epoch": 4.0,
    "loss": 0.3043,
    "grad_norm": 1.6328125,
    "learning_rate": 4.819809158359259e-06
  },
  {
    "step": 9050,
    "epoch": 4.004425757911042,
    "loss": 0.5742,
    "grad_norm": 8.625,
    "learning_rate": 4.778679444361989e-06
  },
  {
    "step": 9060,
    "epoch": 4.0088515158220845,
    "loss": 0.5402,
    "grad_norm": 6.71875,
    "learning_rate": 4.737707420284451e-06
  },
  {
    "step": 9070,
    "epoch": 4.013277273733126,
    "loss": 0.3837,
    "grad_norm": 1.109375,
    "learning_rate": 4.6968934056347085e-06
  },
  {
    "step": 9080,
    "epoch": 4.017703031644169,
    "loss": 0.7521,
    "grad_norm": 4.84375,
    "learning_rate": 4.656237718688661e-06
  },
  {
    "step": 9090,
    "epoch": 4.022128789555211,
    "loss": 0.4723,
    "grad_norm": 4.34375,
    "learning_rate": 4.615740676487507e-06
  },
  {
    "step": 9100,
    "epoch": 4.026554547466254,
    "loss": 0.2968,
    "grad_norm": 2.296875,
    "learning_rate": 4.575402594835326e-06
  },
  {
    "step": 9110,
    "epoch": 4.0309803053772955,
    "loss": 0.4002,
    "grad_norm": 4.0,
    "learning_rate": 4.535223788296586e-06
  },
  {
    "step": 9120,
    "epoch": 4.035406063288338,
    "loss": 0.2641,
    "grad_norm": 2.890625,
    "learning_rate": 4.495204570193687e-06
  },
  {
    "step": 9130,
    "epoch": 4.03983182119938,
    "loss": 0.386,
    "grad_norm": 9.25,
    "learning_rate": 4.455345252604531e-06
  },
  {
    "step": 9140,
    "epoch": 4.044257579110423,
    "loss": 0.2195,
    "grad_norm": 0.7890625,
    "learning_rate": 4.415646146360075e-06
  },
  {
    "step": 9150,
    "epoch": 4.0486833370214645,
    "loss": 0.5575,
    "grad_norm": 1.828125,
    "learning_rate": 4.376107561041937e-06
  },
  {
    "step": 9160,
    "epoch": 4.053109094932507,
    "loss": 0.29,
    "grad_norm": 7.8125,
    "learning_rate": 4.336729804979936e-06
  },
  {
    "step": 9170,
    "epoch": 4.057534852843549,
    "loss": 0.3197,
    "grad_norm": 6.75,
    "learning_rate": 4.297513185249738e-06
  },
  {
    "step": 9180,
    "epoch": 4.061960610754592,
    "loss": 0.5053,
    "grad_norm": 2.3125,
    "learning_rate": 4.258458007670413e-06
  },
  {
    "step": 9190,
    "epoch": 4.066386368665634,
    "loss": 0.4177,
    "grad_norm": 8.9375,
    "learning_rate": 4.219564576802074e-06
  },
  {
    "step": 9200,
    "epoch": 4.070812126576676,
    "loss": 1.0179,
    "grad_norm": 6.96875,
    "learning_rate": 4.180833195943523e-06
  },
  {
    "step": 9210,
    "epoch": 4.075237884487718,
    "loss": 0.1588,
    "grad_norm": 1.140625,
    "learning_rate": 4.1422641671298335e-06
  },
  {
    "step": 9220,
    "epoch": 4.079663642398761,
    "loss": 0.2843,
    "grad_norm": 1.921875,
    "learning_rate": 4.103857791130056e-06
  },
  {
    "step": 9230,
    "epoch": 4.084089400309803,
    "loss": 0.4207,
    "grad_norm": 3.46875,
    "learning_rate": 4.065614367444809e-06
  },
  {
    "step": 9240,
    "epoch": 4.088515158220845,
    "loss": 0.6821,
    "grad_norm": 4.6875,
    "learning_rate": 4.027534194304005e-06
  },
  {
    "step": 9250,
    "epoch": 4.092940916131887,
    "loss": 0.6025,
    "grad_norm": 1.1015625,
    "learning_rate": 3.9896175686644805e-06
  },
  {
    "step": 9260,
    "epoch": 4.09736667404293,
    "loss": 0.511,
    "grad_norm": 81.5,
    "learning_rate": 3.951864786207699e-06
  },
  {
    "step": 9270,
    "epoch": 4.101792431953972,
    "loss": 0.399,
    "grad_norm": 3.5,
    "learning_rate": 3.914276141337434e-06
  },
  {
    "step": 9280,
    "epoch": 4.1062181898650145,
    "loss": 0.5907,
    "grad_norm": 7.90625,
    "learning_rate": 3.8768519271775e-06
  },
  {
    "step": 9290,
    "epoch": 4.110643947776056,
    "loss": 0.2338,
    "grad_norm": 7.125,
    "learning_rate": 3.839592435569425e-06
  },
  {
    "step": 9300,
    "epoch": 4.115069705687099,
    "loss": 0.2122,
    "grad_norm": 2.375,
    "learning_rate": 3.802497957070225e-06
  },
  {
    "step": 9310,
    "epoch": 4.119495463598141,
    "loss": 0.3709,
    "grad_norm": 3.90625,
    "learning_rate": 3.765568780950085e-06
  },
  {
    "step": 9320,
    "epoch": 4.123921221509184,
    "loss": 0.3483,
    "grad_norm": 1.921875,
    "learning_rate": 3.7288051951901327e-06
  },
  {
    "step": 9330,
    "epoch": 4.128346979420225,
    "loss": 0.6088,
    "grad_norm": 6.4375,
    "learning_rate": 3.692207486480209e-06
  },
  {
    "step": 9340,
    "epoch": 4.132772737331268,
    "loss": 0.432,
    "grad_norm": 5.6875,
    "learning_rate": 3.6557759402165852e-06
  },
  {
    "step": 9350,
    "epoch": 4.13719849524231,
    "loss": 0.4052,
    "grad_norm": 7.8125,
    "learning_rate": 3.6195108404997873e-06
  },
  {
    "step": 9360,
    "epoch": 4.141624253153353,
    "loss": 0.4129,
    "grad_norm": 10.3125,
    "learning_rate": 3.5834124701323413e-06
  },
  {
    "step": 9370,
    "epoch": 4.1460500110643945,
    "loss": 0.6388,
    "grad_norm": 2.046875,
    "learning_rate": 3.547481110616596e-06
  },
  {
    "step": 9380,
    "epoch": 4.150475768975437,
    "loss": 0.5748,
    "grad_norm": 4.28125,
    "learning_rate": 3.51171704215251e-06
  },
  {
    "step": 9390,
    "epoch": 4.154901526886479,
    "loss": 0.441,
    "grad_norm": 2.265625,
    "learning_rate": 3.4761205436354692e-06
  },
  {
    "step": 9400,
    "epoch": 4.159327284797522,
    "loss": 0.3651,
    "grad_norm": 0.5234375,
    "learning_rate": 3.440691892654116e-06
  },
  {
    "step": 9410,
    "epoch": 4.163753042708564,
    "loss": 0.5968,
    "grad_norm": 0.78515625,
    "learning_rate": 3.4054313654881957e-06
  },
  {
    "step": 9420,
    "epoch": 4.168178800619606,
    "loss": 0.6382,
    "grad_norm": 3.171875,
    "learning_rate": 3.3703392371063845e-06
  },
  {
    "step": 9430,
    "epoch": 4.172604558530648,
    "loss": 0.3606,
    "grad_norm": 9.4375,
    "learning_rate": 3.3354157811641446e-06
  },
  {
    "step": 9440,
    "epoch": 4.177030316441691,
    "loss": 0.5389,
    "grad_norm": 9.0625,
    "learning_rate": 3.300661270001601e-06
  },
  {
    "step": 9450,
    "epoch": 4.181456074352733,
    "loss": 0.509,
    "grad_norm": 8.1875,
    "learning_rate": 3.2660759746414054e-06
  },
  {
    "step": 9460,
    "epoch": 4.185881832263775,
    "loss": 0.5701,
    "grad_norm": 4.375,
    "learning_rate": 3.231660164786651e-06
  },
  {
    "step": 9470,
    "epoch": 4.190307590174817,
    "loss": 0.4137,
    "grad_norm": 1.6640625,
    "learning_rate": 3.1974141088187277e-06
  },
  {
    "step": 9480,
    "epoch": 4.19473334808586,
    "loss": 0.3437,
    "grad_norm": 8.125,
    "learning_rate": 3.1633380737952663e-06
  },
  {
    "step": 9490,
    "epoch": 4.199159105996902,
    "loss": 0.4029,
    "grad_norm": 5.375,
    "learning_rate": 3.1294323254480257e-06
  },
  {
    "step": 9500,
    "epoch": 4.2035848639079445,
    "loss": 0.2588,
    "grad_norm": 6.40625,
    "learning_rate": 3.0956971281808496e-06
  },
  {
    "step": 9510,
    "epoch": 4.208010621818986,
    "loss": 0.3654,
    "grad_norm": 3.15625,
    "learning_rate": 3.062132745067581e-06
  },
  {
    "step": 9520,
    "epoch": 4.212436379730029,
    "loss": 0.5371,
    "grad_norm": 72.5,
    "learning_rate": 3.028739437850017e-06
  },
  {
    "step": 9530,
    "epoch": 4.216862137641071,
    "loss": 0.7158,
    "grad_norm": 6.03125,
    "learning_rate": 2.995517466935882e-06
  },
  {
    "step": 9540,
    "epoch": 4.221287895552114,
    "loss": 0.3836,
    "grad_norm": 3.75,
    "learning_rate": 2.96246709139677e-06
  },
  {
    "step": 9550,
    "epoch": 4.225713653463155,
    "loss": 0.5095,
    "grad_norm": 4.34375,
    "learning_rate": 2.9295885689661574e-06
  },
  {
    "step": 9560,
    "epoch": 4.230139411374198,
    "loss": 0.4536,
    "grad_norm": 3.0,
    "learning_rate": 2.896882156037367e-06
  },
  {
    "step": 9570,
    "epoch": 4.23456516928524,
    "loss": 0.3556,
    "grad_norm": 2.90625,
    "learning_rate": 2.8643481076615712e-06
  },
  {
    "step": 9580,
    "epoch": 4.238990927196283,
    "loss": 0.7754,
    "grad_norm": 1.8203125,
    "learning_rate": 2.8319866775458115e-06
  },
  {
    "step": 9590,
    "epoch": 4.2434166851073245,
    "loss": 0.2801,
    "grad_norm": 3.96875,
    "learning_rate": 2.7997981180510333e-06
  },
  {
    "step": 9600,
    "epoch": 4.247842443018367,
    "loss": 0.6993,
    "grad_norm": 5.09375,
    "learning_rate": 2.767782680190073e-06
  },
  {
    "step": 9610,
    "epoch": 4.252268200929409,
    "loss": 0.5358,
    "grad_norm": 3.015625,
    "learning_rate": 2.7359406136257593e-06
  },
  {
    "step": 9620,
    "epoch": 4.256693958840452,
    "loss": 0.4129,
    "grad_norm": 6.0,
    "learning_rate": 2.7042721666689224e-06
  },
  {
    "step": 9630,
    "epoch": 4.261119716751494,
    "loss": 0.2968,
    "grad_norm": 3.296875,
    "learning_rate": 2.67277758627647e-06
  },
  {
    "step": 9640,
    "epoch": 4.265545474662536,
    "loss": 0.5786,
    "grad_norm": 3.21875,
    "learning_rate": 2.6414571180494702e-06
  },
  {
    "step": 9650,
    "epoch": 4.269971232573578,
    "loss": 0.301,
    "grad_norm": 3.6875,
    "learning_rate": 2.6103110062312143e-06
  },
  {
    "step": 9660,
    "epoch": 4.274396990484621,
    "loss": 0.3982,
    "grad_norm": 4.34375,
    "learning_rate": 2.5793394937053544e-06
  },
  {
    "step": 9670,
    "epoch": 4.278822748395663,
    "loss": 0.3479,
    "grad_norm": 0.37109375,
    "learning_rate": 2.5485428219939542e-06
  },
  {
    "step": 9680,
    "epoch": 4.283248506306705,
    "loss": 0.5626,
    "grad_norm": 7.53125,
    "learning_rate": 2.5179212312556546e-06
  },
  {
    "step": 9690,
    "epoch": 4.287674264217747,
    "loss": 0.2688,
    "grad_norm": 3.328125,
    "learning_rate": 2.4874749602837697e-06
  },
  {
    "step": 9700,
    "epoch": 4.29210002212879,
    "loss": 0.4782,
    "grad_norm": 4.46875,
    "learning_rate": 2.4572042465044355e-06
  },
  {
    "step": 9710,
    "epoch": 4.296525780039832,
    "loss": 0.4902,
    "grad_norm": 4.96875,
    "learning_rate": 2.4271093259747552e-06
  },
  {
    "step": 9720,
    "epoch": 4.3009515379508745,
    "loss": 0.3747,
    "grad_norm": 1.640625,
    "learning_rate": 2.3971904333809637e-06
  },
  {
    "step": 9730,
    "epoch": 4.305377295861916,
    "loss": 0.6752,
    "grad_norm": 3.21875,
    "learning_rate": 2.367447802036607e-06
  },
  {
    "step": 9740,
    "epoch": 4.309803053772959,
    "loss": 0.2477,
    "grad_norm": 2.390625,
    "learning_rate": 2.3378816638806866e-06
  },
  {
    "step": 9750,
    "epoch": 4.314228811684001,
    "loss": 0.445,
    "grad_norm": 6.71875,
    "learning_rate": 2.3084922494758963e-06
  },
  {
    "step": 9760,
    "epoch": 4.318654569595044,
    "loss": 0.4523,
    "grad_norm": 2.4375,
    "learning_rate": 2.2792797880067914e-06
  },
  {
    "step": 9770,
    "epoch": 4.323080327506085,
    "loss": 0.4653,
    "grad_norm": 3.734375,
    "learning_rate": 2.25024450727801e-06
  },
  {
    "step": 9780,
    "epoch": 4.327506085417127,
    "loss": 0.4462,
    "grad_norm": 4.0625,
    "learning_rate": 2.2213866337125022e-06
  },
  {
    "step": 9790,
    "epoch": 4.33193184332817,
    "loss": 0.3938,
    "grad_norm": 6.4375,
    "learning_rate": 2.192706392349772e-06
  },
  {
    "step": 9800,
    "epoch": 4.336357601239213,
    "loss": 0.387,
    "grad_norm": 4.5,
    "learning_rate": 2.1642040068440924e-06
  },
  {
    "step": 9810,
    "epoch": 4.3407833591502545,
    "loss": 0.7525,
    "grad_norm": 9.0,
    "learning_rate": 2.1358796994628005e-06
  },
  {
    "step": 9820,
    "epoch": 4.345209117061296,
    "loss": 0.7102,
    "grad_norm": 6.625,
    "learning_rate": 2.1077336910845293e-06
  },
  {
    "step": 9830,
    "epoch": 4.349634874972339,
    "loss": 0.4914,
    "grad_norm": 7.21875,
    "learning_rate": 2.0797662011975057e-06
  },
  {
    "step": 9840,
    "epoch": 4.354060632883382,
    "loss": 0.297,
    "grad_norm": 3.8125,
    "learning_rate": 2.0519774478978406e-06
  },
  {
    "step": 9850,
    "epoch": 4.358486390794424,
    "loss": 0.4802,
    "grad_norm": 0.93359375,
    "learning_rate": 2.0243676478878037e-06
  },
  {
    "step": 9860,
    "epoch": 4.362912148705465,
    "loss": 0.4789,
    "grad_norm": 0.98828125,
    "learning_rate": 1.9969370164741736e-06
  },
  {
    "step": 9870,
    "epoch": 4.367337906616508,
    "loss": 0.4426,
    "grad_norm": 1.6484375,
    "learning_rate": 1.969685767566512e-06
  },
  {
    "step": 9880,
    "epoch": 4.37176366452755,
    "loss": 0.7075,
    "grad_norm": 2.390625,
    "learning_rate": 1.942614113675545e-06
  },
  {
    "step": 9890,
    "epoch": 4.376189422438593,
    "loss": 0.3993,
    "grad_norm": 2.90625,
    "learning_rate": 1.91572226591146e-06
  },
  {
    "step": 9900,
    "epoch": 4.3806151803496345,
    "loss": 0.6467,
    "grad_norm": 0.09375,
    "learning_rate": 1.889010433982291e-06
  },
  {
    "step": 9910,
    "epoch": 4.385040938260677,
    "loss": 0.3089,
    "grad_norm": 2.34375,
    "learning_rate": 1.86247882619226e-06
  },
  {
    "step": 9920,
    "epoch": 4.389466696171719,
    "loss": 0.3808,
    "grad_norm": 1.234375,
    "learning_rate": 1.8361276494401837e-06
  },
  {
    "step": 9930,
    "epoch": 4.393892454082762,
    "loss": 0.5162,
    "grad_norm": 1.578125,
    "learning_rate": 1.809957109217833e-06
  },
  {
    "step": 9940,
    "epoch": 4.398318211993804,
    "loss": 0.5634,
    "grad_norm": 5.59375,
    "learning_rate": 1.7839674096083336e-06
  },
  {
    "step": 9950,
    "epoch": 4.402743969904846,
    "loss": 0.5961,
    "grad_norm": 4.75,
    "learning_rate": 1.758158753284586e-06
  },
  {
    "step": 9960,
    "epoch": 4.407169727815888,
    "loss": 0.5376,
    "grad_norm": 3.265625,
    "learning_rate": 1.7325313415076705e-06
  },
  {
    "step": 9970,
    "epoch": 4.411595485726931,
    "loss": 0.504,
    "grad_norm": 1.9921875,
    "learning_rate": 1.7070853741253029e-06
  },
  {
    "step": 9980,
    "epoch": 4.416021243637973,
    "loss": 0.5643,
    "grad_norm": 1.5234375,
    "learning_rate": 1.6818210495702342e-06
  },
  {
    "step": 9990,
    "epoch": 4.420447001549015,
    "loss": 0.2957,
    "grad_norm": 1.625,
    "learning_rate": 1.6567385648587564e-06
  },
  {
    "step": 10000,
    "epoch": 4.424872759460057,
    "loss": 0.5691,
    "grad_norm": 3.328125,
    "learning_rate": 1.6318381155891155e-06
  },
  {
    "step": 10000,
    "epoch": 4.424872759460057,
    "eval_loss": 0.4923619031906128,
    "eval_runtime": 102.5709,
    "eval_samples_per_second": 11.017,
    "eval_steps_per_second": 11.017
  },
  {
    "step": 10010,
    "epoch": 4.4292985173711,
    "loss": 0.4885,
    "grad_norm": 2.796875,
    "learning_rate": 1.6071198959400208e-06
  },
  {
    "step": 10020,
    "epoch": 4.433724275282142,
    "loss": 0.3547,
    "grad_norm": 2.75,
    "learning_rate": 1.5825840986691154e-06
  },
  {
    "step": 10030,
    "epoch": 4.4381500331931845,
    "loss": 0.5526,
    "grad_norm": 4.09375,
    "learning_rate": 1.5582309151114666e-06
  },
  {
    "step": 10040,
    "epoch": 4.442575791104226,
    "loss": 0.2515,
    "grad_norm": 8.125,
    "learning_rate": 1.5340605351781028e-06
  },
  {
    "step": 10050,
    "epoch": 4.447001549015269,
    "loss": 0.4121,
    "grad_norm": 5.96875,
    "learning_rate": 1.5100731473544933e-06
  },
  {
    "step": 10060,
    "epoch": 4.451427306926311,
    "loss": 0.3574,
    "grad_norm": 2.75,
    "learning_rate": 1.4862689386991146e-06
  },
  {
    "step": 10070,
    "epoch": 4.4558530648373535,
    "loss": 0.6351,
    "grad_norm": 5.09375,
    "learning_rate": 1.462648094841962e-06
  },
  {
    "step": 10080,
    "epoch": 4.460278822748395,
    "loss": 0.354,
    "grad_norm": 2.34375,
    "learning_rate": 1.4392107999831262e-06
  },
  {
    "step": 10090,
    "epoch": 4.464704580659438,
    "loss": 0.4254,
    "grad_norm": 1.7890625,
    "learning_rate": 1.4159572368913298e-06
  },
  {
    "step": 10100,
    "epoch": 4.46913033857048,
    "loss": 0.5144,
    "grad_norm": 2.140625,
    "learning_rate": 1.3928875869025415e-06
  },
  {
    "step": 10110,
    "epoch": 4.473556096481523,
    "loss": 0.5153,
    "grad_norm": 4.875,
    "learning_rate": 1.3700020299185156e-06
  },
  {
    "step": 10120,
    "epoch": 4.4779818543925645,
    "loss": 0.4496,
    "grad_norm": 4.9375,
    "learning_rate": 1.3473007444054276e-06
  },
  {
    "step": 10130,
    "epoch": 4.482407612303607,
    "loss": 0.337,
    "grad_norm": 7.25,
    "learning_rate": 1.3247839073924684e-06
  },
  {
    "step": 10140,
    "epoch": 4.486833370214649,
    "loss": 0.5369,
    "grad_norm": 5.125,
    "learning_rate": 1.3024516944704496e-06
  },
  {
    "step": 10150,
    "epoch": 4.491259128125692,
    "loss": 0.2772,
    "grad_norm": 3.703125,
    "learning_rate": 1.2803042797904507e-06
  },
  {
    "step": 10160,
    "epoch": 4.4956848860367336,
    "loss": 0.3115,
    "grad_norm": 3.375,
    "learning_rate": 1.2583418360624628e-06
  },
  {
    "step": 10170,
    "epoch": 4.500110643947776,
    "loss": 0.3891,
    "grad_norm": 2.484375,
    "learning_rate": 1.2365645345540384e-06
  },
  {
    "step": 10180,
    "epoch": 4.504536401858818,
    "loss": 0.4921,
    "grad_norm": 0.2490234375,
    "learning_rate": 1.2149725450889438e-06
  },
  {
    "step": 10190,
    "epoch": 4.508962159769861,
    "loss": 0.3372,
    "grad_norm": 2.65625,
    "learning_rate": 1.1935660360458538e-06
  },
  {
    "step": 10200,
    "epoch": 4.513387917680903,
    "loss": 0.4045,
    "grad_norm": 4.65625,
    "learning_rate": 1.1723451743570229e-06
  },
  {
    "step": 10210,
    "epoch": 4.517813675591945,
    "loss": 0.4579,
    "grad_norm": 5.21875,
    "learning_rate": 1.1513101255069963e-06
  },
  {
    "step": 10220,
    "epoch": 4.522239433502987,
    "loss": 0.6021,
    "grad_norm": 2.765625,
    "learning_rate": 1.1304610535313066e-06
  },
  {
    "step": 10230,
    "epoch": 4.52666519141403,
    "loss": 0.507,
    "grad_norm": 3.140625,
    "learning_rate": 1.1097981210152043e-06
  },
  {
    "step": 10240,
    "epoch": 4.531090949325072,
    "loss": 0.5252,
    "grad_norm": 1.640625,
    "learning_rate": 1.0893214890923986e-06
  },
  {
    "step": 10250,
    "epoch": 4.535516707236114,
    "loss": 0.56,
    "grad_norm": 5.28125,
    "learning_rate": 1.0690313174437638e-06
  },
  {
    "step": 10260,
    "epoch": 4.539942465147156,
    "loss": 0.3419,
    "grad_norm": 3.8125,
    "learning_rate": 1.048927764296148e-06
  },
  {
    "step": 10270,
    "epoch": 4.544368223058199,
    "loss": 0.4331,
    "grad_norm": 3.6875,
    "learning_rate": 1.029010986421089e-06
  },
  {
    "step": 10280,
    "epoch": 4.548793980969241,
    "loss": 0.5,
    "grad_norm": 0.9296875,
    "learning_rate": 1.0092811391336248e-06
  },
  {
    "step": 10290,
    "epoch": 4.5532197388802835,
    "loss": 0.8003,
    "grad_norm": 56.75,
    "learning_rate": 9.897383762910605e-07
  },
  {
    "step": 10300,
    "epoch": 4.557645496791325,
    "loss": 0.6502,
    "grad_norm": 6.09375,
    "learning_rate": 9.703828502917983e-07
  },
  {
    "step": 10310,
    "epoch": 4.562071254702368,
    "loss": 0.348,
    "grad_norm": 3.5,
    "learning_rate": 9.512147120741055e-07
  },
  {
    "step": 10320,
    "epoch": 4.56649701261341,
    "loss": 0.3702,
    "grad_norm": 6.09375,
    "learning_rate": 9.322341111149852e-07
  },
  {
    "step": 10330,
    "epoch": 4.570922770524453,
    "loss": 0.3774,
    "grad_norm": 7.125,
    "learning_rate": 9.13441195428974e-07
  },
  {
    "step": 10340,
    "epoch": 4.575348528435494,
    "loss": 0.3202,
    "grad_norm": 0.83203125,
    "learning_rate": 8.948361115669962e-07
  },
  {
    "step": 10350,
    "epoch": 4.579774286346537,
    "loss": 0.4638,
    "grad_norm": 4.21875,
    "learning_rate": 8.764190046152421e-07
  },
  {
    "step": 10360,
    "epoch": 4.584200044257579,
    "loss": 0.4038,
    "grad_norm": 5.09375,
    "learning_rate": 8.581900181940051e-07
  },
  {
    "step": 10370,
    "epoch": 4.588625802168622,
    "loss": 0.2912,
    "grad_norm": 2.6875,
    "learning_rate": 8.40149294456588e-07
  },
  {
    "step": 10380,
    "epoch": 4.5930515600796635,
    "loss": 0.421,
    "grad_norm": 26.375,
    "learning_rate": 8.22296974088177e-07
  },
  {
    "step": 10390,
    "epoch": 4.597477317990706,
    "loss": 0.5062,
    "grad_norm": 1.1875,
    "learning_rate": 8.046331963047494e-07
  },
  {
    "step": 10400,
    "epoch": 4.601903075901748,
    "loss": 0.3628,
    "grad_norm": 6.09375,
    "learning_rate": 7.871580988519977e-07
  },
  {
    "step": 10410,
    "epoch": 4.606328833812791,
    "loss": 0.3014,
    "grad_norm": 3.96875,
    "learning_rate": 7.698718180042392e-07
  },
  {
    "step": 10420,
    "epoch": 4.610754591723833,
    "loss": 0.5563,
    "grad_norm": 5.625,
    "learning_rate": 7.527744885633625e-07
  },
  {
    "step": 10430,
    "epoch": 4.615180349634875,
    "loss": 0.4243,
    "grad_norm": 5.0,
    "learning_rate": 7.358662438577802e-07
  },
  {
    "step": 10440,
    "epoch": 4.619606107545917,
    "loss": 0.4631,
    "grad_norm": 3.59375,
    "learning_rate": 7.191472157413809e-07
  },
  {
    "step": 10450,
    "epoch": 4.62403186545696,
    "loss": 0.6131,
    "grad_norm": 3.453125,
    "learning_rate": 7.026175345925035e-07
  },
  {
    "step": 10460,
    "epoch": 4.628457623368002,
    "loss": 0.5442,
    "grad_norm": 0.322265625,
    "learning_rate": 6.862773293129232e-07
  },
  {
    "step": 10470,
    "epoch": 4.632883381279044,
    "loss": 0.5233,
    "grad_norm": 2.5,
    "learning_rate": 6.701267273268391e-07
  },
  {
    "step": 10480,
    "epoch": 4.637309139190086,
    "loss": 0.3728,
    "grad_norm": 2.625,
    "learning_rate": 6.541658545798968e-07
  },
  {
    "step": 10490,
    "epoch": 4.641734897101129,
    "loss": 0.4699,
    "grad_norm": 6.375,
    "learning_rate": 6.383948355381808e-07
  },
  {
    "step": 10500,
    "epoch": 4.646160655012171,
    "loss": 0.6994,
    "grad_norm": 15.4375,
    "learning_rate": 6.228137931872713e-07
  },
  {
    "step": 10510,
    "epoch": 4.650586412923213,
    "loss": 0.5857,
    "grad_norm": 3.484375,
    "learning_rate": 6.074228490312611e-07
  },
  {
    "step": 10520,
    "epoch": 4.655012170834255,
    "loss": 0.4457,
    "grad_norm": 18.875,
    "learning_rate": 5.922221230918318e-07
  },
  {
    "step": 10530,
    "epoch": 4.659437928745298,
    "loss": 0.7754,
    "grad_norm": 8.375,
    "learning_rate": 5.772117339072902e-07
  },
  {
    "step": 10540,
    "epoch": 4.66386368665634,
    "loss": 0.3859,
    "grad_norm": 3.765625,
    "learning_rate": 5.623917985316696e-07
  },
  {
    "step": 10550,
    "epoch": 4.668289444567382,
    "loss": 0.3054,
    "grad_norm": 8.625,
    "learning_rate": 5.477624325338076e-07
  },
  {
    "step": 10560,
    "epoch": 4.672715202478424,
    "loss": 0.1985,
    "grad_norm": 2.859375,
    "learning_rate": 5.333237499964283e-07
  },
  {
    "step": 10570,
    "epoch": 4.677140960389467,
    "loss": 0.4299,
    "grad_norm": 1.2421875,
    "learning_rate": 5.190758635152893e-07
  },
  {
    "step": 10580,
    "epoch": 4.681566718300509,
    "loss": 0.5471,
    "grad_norm": 2.28125,
    "learning_rate": 5.050188841982662e-07
  },
  {
    "step": 10590,
    "epoch": 4.685992476211551,
    "loss": 0.4912,
    "grad_norm": 1.46875,
    "learning_rate": 4.911529216645088e-07
  },
  {
    "step": 10600,
    "epoch": 4.6904182341225935,
    "loss": 0.3346,
    "grad_norm": 3.671875,
    "learning_rate": 4.774780840435783e-07
  },
  {
    "step": 10610,
    "epoch": 4.694843992033636,
    "loss": 0.4241,
    "grad_norm": 1.78125,
    "learning_rate": 4.639944779746025e-07
  },
  {
    "step": 10620,
    "epoch": 4.699269749944678,
    "loss": 0.8617,
    "grad_norm": 9.875,
    "learning_rate": 4.507022086054524e-07
  },
  {
    "step": 10630,
    "epoch": 4.70369550785572,
    "loss": 0.8104,
    "grad_norm": 2.0625,
    "learning_rate": 4.376013795919176e-07
  },
  {
    "step": 10640,
    "epoch": 4.708121265766763,
    "loss": 0.6009,
    "grad_norm": 5.625,
    "learning_rate": 4.246920930968901e-07
  },
  {
    "step": 10650,
    "epoch": 4.712547023677805,
    "loss": 0.5641,
    "grad_norm": 4.25,
    "learning_rate": 4.1197444978958166e-07
  },
  {
    "step": 10660,
    "epoch": 4.716972781588847,
    "loss": 0.2448,
    "grad_norm": 5.4375,
    "learning_rate": 3.9944854884473026e-07
  },
  {
    "step": 10670,
    "epoch": 4.721398539499889,
    "loss": 0.5078,
    "grad_norm": 4.6875,
    "learning_rate": 3.871144879418226e-07
  },
  {
    "step": 10680,
    "epoch": 4.725824297410932,
    "loss": 0.8448,
    "grad_norm": 5.21875,
    "learning_rate": 3.7497236326434757e-07
  },
  {
    "step": 10690,
    "epoch": 4.7302500553219735,
    "loss": 0.372,
    "grad_norm": 1.5,
    "learning_rate": 3.630222694990304e-07
  },
  {
    "step": 10700,
    "epoch": 4.734675813233016,
    "loss": 0.4219,
    "grad_norm": 7.0625,
    "learning_rate": 3.512642998351079e-07
  },
  {
    "step": 10710,
    "epoch": 4.739101571144058,
    "loss": 0.6557,
    "grad_norm": 18.625,
    "learning_rate": 3.396985459635821e-07
  },
  {
    "step": 10720,
    "epoch": 4.743527329055101,
    "loss": 0.5946,
    "grad_norm": 7.09375,
    "learning_rate": 3.283250980765318e-07
  },
  {
    "step": 10730,
    "epoch": 4.747953086966143,
    "loss": 0.5771,
    "grad_norm": 0.1279296875,
    "learning_rate": 3.171440448663798e-07
  },
  {
    "step": 10740,
    "epoch": 4.752378844877185,
    "loss": 0.6896,
    "grad_norm": 4.65625,
    "learning_rate": 3.0615547352523256e-07
  },
  {
    "step": 10750,
    "epoch": 4.756804602788227,
    "loss": 0.5039,
    "grad_norm": 4.9375,
    "learning_rate": 2.953594697441775e-07
  },
  {
    "step": 10760,
    "epoch": 4.76123036069927,
    "loss": 0.784,
    "grad_norm": 3.25,
    "learning_rate": 2.8475611771261455e-07
  },
  {
    "step": 10770,
    "epoch": 4.765656118610312,
    "loss": 0.6531,
    "grad_norm": 7.21875,
    "learning_rate": 2.743455001176176e-07
  },
  {
    "step": 10780,
    "epoch": 4.770081876521354,
    "loss": 0.2975,
    "grad_norm": 3.203125,
    "learning_rate": 2.641276981432711e-07
  },
  {
    "step": 10790,
    "epoch": 4.774507634432396,
    "loss": 0.5072,
    "grad_norm": 1.921875,
    "learning_rate": 2.541027914700483e-07
  },
  {
    "step": 10800,
    "epoch": 4.778933392343439,
    "loss": 0.4322,
    "grad_norm": 5.625,
    "learning_rate": 2.4427085827418705e-07
  },
  {
    "step": 10810,
    "epoch": 4.783359150254481,
    "loss": 0.2854,
    "grad_norm": 1.953125,
    "learning_rate": 2.3463197522707592e-07
  },
  {
    "step": 10820,
    "epoch": 4.7877849081655235,
    "loss": 0.4011,
    "grad_norm": 6.375,
    "learning_rate": 2.2518621749465785e-07
  },
  {
    "step": 10830,
    "epoch": 4.792210666076565,
    "loss": 0.4806,
    "grad_norm": 3.1875,
    "learning_rate": 2.1593365873685544e-07
  },
  {
    "step": 10840,
    "epoch": 4.796636423987608,
    "loss": 0.4127,
    "grad_norm": 0.96484375,
    "learning_rate": 2.0687437110697706e-07
  },
  {
    "step": 10850,
    "epoch": 4.80106218189865,
    "loss": 0.3399,
    "grad_norm": 5.09375,
    "learning_rate": 1.9800842525116715e-07
  },
  {
    "step": 10860,
    "epoch": 4.805487939809693,
    "loss": 0.4332,
    "grad_norm": 4.6875,
    "learning_rate": 1.8933589030785682e-07
  },
  {
    "step": 10870,
    "epoch": 4.809913697720734,
    "loss": 0.5437,
    "grad_norm": 3.84375,
    "learning_rate": 1.8085683390720865e-07
  },
  {
    "step": 10880,
    "epoch": 4.814339455631777,
    "loss": 0.484,
    "grad_norm": 3.90625,
    "learning_rate": 1.7257132217061157e-07
  },
  {
    "step": 10890,
    "epoch": 4.818765213542819,
    "loss": 0.6963,
    "grad_norm": 1.671875,
    "learning_rate": 1.644794197101507e-07
  },
  {
    "step": 10900,
    "epoch": 4.823190971453862,
    "loss": 0.5185,
    "grad_norm": 0.6640625,
    "learning_rate": 1.5658118962810774e-07
  },
  {
    "step": 10910,
    "epoch": 4.8276167293649035,
    "loss": 0.2849,
    "grad_norm": 2.421875,
    "learning_rate": 1.488766935164615e-07
  },
  {
    "step": 10920,
    "epoch": 4.832042487275946,
    "loss": 0.2683,
    "grad_norm": 1.84375,
    "learning_rate": 1.413659914564297e-07
  },
  {
    "step": 10930,
    "epoch": 4.836468245186988,
    "loss": 0.7828,
    "grad_norm": 3.828125,
    "learning_rate": 1.340491420179668e-07
  },
  {
    "step": 10940,
    "epoch": 4.840894003098031,
    "loss": 0.648,
    "grad_norm": 0.94140625,
    "learning_rate": 1.2692620225933925e-07
  },
  {
    "step": 10950,
    "epoch": 4.845319761009073,
    "loss": 0.3436,
    "grad_norm": 4.375,
    "learning_rate": 1.1999722772666476e-07
  },
  {
    "step": 10960,
    "epoch": 4.849745518920115,
    "loss": 0.2085,
    "grad_norm": 1.234375,
    "learning_rate": 1.1326227245347376e-07
  },
  {
    "step": 10970,
    "epoch": 4.854171276831157,
    "loss": 0.2712,
    "grad_norm": 0.56640625,
    "learning_rate": 1.0672138896030137e-07
  },
  {
    "step": 10980,
    "epoch": 4.8585970347422,
    "loss": 0.6219,
    "grad_norm": 22.875,
    "learning_rate": 1.0037462825427114e-07
  },
  {
    "step": 10990,
    "epoch": 4.863022792653242,
    "loss": 0.3929,
    "grad_norm": 1.5078125,
    "learning_rate": 9.422203982870082e-08
  },
  {
    "step": 11000,
    "epoch": 4.867448550564284,
    "loss": 0.5837,
    "grad_norm": 2.703125,
    "learning_rate": 8.826367166270555e-08
  },
  {
    "step": 11000,
    "epoch": 4.867448550564284,
    "eval_loss": 0.44836607575416565,
    "eval_runtime": 104.9728,
    "eval_samples_per_second": 10.765,
    "eval_steps_per_second": 10.765
  },
  {
    "step": 11010,
    "epoch": 4.871874308475326,
    "loss": 0.576,
    "grad_norm": 3.90625,
    "learning_rate": 8.249957022084254e-08
  },
  {
    "step": 11020,
    "epoch": 4.876300066386369,
    "loss": 0.4773,
    "grad_norm": 2.046875,
    "learning_rate": 7.692978045273081e-08
  },
  {
    "step": 11030,
    "epoch": 4.880725824297411,
    "loss": 0.3131,
    "grad_norm": 2.484375,
    "learning_rate": 7.155434579270703e-08
  },
  {
    "step": 11040,
    "epoch": 4.8851515822084535,
    "loss": 0.4368,
    "grad_norm": 4.59375,
    "learning_rate": 6.637330815949527e-08
  },
  {
    "step": 11050,
    "epoch": 4.889577340119495,
    "loss": 0.6274,
    "grad_norm": 3.09375,
    "learning_rate": 6.138670795586276e-08
  },
  {
    "step": 11060,
    "epoch": 4.894003098030538,
    "loss": 0.3703,
    "grad_norm": 2.625,
    "learning_rate": 5.6594584068322966e-08
  },
  {
    "step": 11070,
    "epoch": 4.89842885594158,
    "loss": 0.5055,
    "grad_norm": 2.484375,
    "learning_rate": 5.1996973866821894e-08
  },
  {
    "step": 11080,
    "epoch": 4.902854613852623,
    "loss": 0.796,
    "grad_norm": 4.5625,
    "learning_rate": 4.7593913204441174e-08
  },
  {
    "step": 11090,
    "epoch": 4.907280371763664,
    "loss": 0.5362,
    "grad_norm": 2.09375,
    "learning_rate": 4.338543641713988e-08
  },
  {
    "step": 11100,
    "epoch": 4.911706129674707,
    "loss": 0.2427,
    "grad_norm": 3.234375,
    "learning_rate": 3.937157632346311e-08
  },
  {
    "step": 11110,
    "epoch": 4.916131887585749,
    "loss": 0.5887,
    "grad_norm": 1.8046875,
    "learning_rate": 3.5552364224292203e-08
  },
  {
    "step": 11120,
    "epoch": 4.920557645496792,
    "loss": 0.6369,
    "grad_norm": 0.002227783203125,
    "learning_rate": 3.192782990261989e-08
  },
  {
    "step": 11130,
    "epoch": 4.9249834034078335,
    "loss": 0.4572,
    "grad_norm": 1.3828125,
    "learning_rate": 2.8498001623286642e-08
  },
  {
    "step": 11140,
    "epoch": 4.929409161318876,
    "loss": 0.2372,
    "grad_norm": 0.310546875,
    "learning_rate": 2.5262906132783594e-08
  },
  {
    "step": 11150,
    "epoch": 4.933834919229918,
    "loss": 0.7282,
    "grad_norm": 35.75,
    "learning_rate": 2.2222568659036047e-08
  },
  {
    "step": 11160,
    "epoch": 4.938260677140961,
    "loss": 0.4863,
    "grad_norm": 5.34375,
    "learning_rate": 1.937701291120364e-08
  },
  {
    "step": 11170,
    "epoch": 4.942686435052003,
    "loss": 0.709,
    "grad_norm": 1.5234375,
    "learning_rate": 1.6726261079505478e-08
  },
  {
    "step": 11180,
    "epoch": 4.947112192963045,
    "loss": 0.5868,
    "grad_norm": 1.5859375,
    "learning_rate": 1.4270333835036952e-08
  },
  {
    "step": 11190,
    "epoch": 4.951537950874087,
    "loss": 0.3506,
    "grad_norm": 7.0,
    "learning_rate": 1.2009250329608757e-08
  },
  {
    "step": 11200,
    "epoch": 4.95596370878513,
    "loss": 0.2466,
    "grad_norm": 1.828125,
    "learning_rate": 9.943028195605331e-09
  },
  {
    "step": 11210,
    "epoch": 4.960389466696172,
    "loss": 0.8587,
    "grad_norm": 5.0625,
    "learning_rate": 8.071683545843312e-09
  },
  {
    "step": 11220,
    "epoch": 4.964815224607214,
    "loss": 0.4111,
    "grad_norm": 5.75,
    "learning_rate": 6.395230973443856e-09
  },
  {
    "step": 11230,
    "epoch": 4.969240982518256,
    "loss": 0.3151,
    "grad_norm": 1.671875,
    "learning_rate": 4.913683551718839e-09
  },
  {
    "step": 11240,
    "epoch": 4.973666740429299,
    "loss": 0.3693,
    "grad_norm": 2.03125,
    "learning_rate": 3.6270528340764943e-09
  },
  {
    "step": 11250,
    "epoch": 4.978092498340341,
    "loss": 0.4198,
    "grad_norm": 11.8125,
    "learning_rate": 2.5353488539187065e-09
  },
  {
    "step": 11260,
    "epoch": 4.9825182562513834,
    "loss": 0.6716,
    "grad_norm": 4.0625,
    "learning_rate": 1.6385801245716315e-09
  },
  {
    "step": 11270,
    "epoch": 4.986944014162425,
    "loss": 0.503,
    "grad_norm": 4.03125,
    "learning_rate": 9.36753639219079e-10
  },
  {
    "step": 11280,
    "epoch": 4.991369772073467,
    "loss": 0.3674,
    "grad_norm": 1.390625,
    "learning_rate": 4.298748708470024e-10
  },
  {
    "step": 11290,
    "epoch": 4.99579552998451,
    "loss": 0.5405,
    "grad_norm": 1.75,
    "learning_rate": 1.1794777219631402e-10
  },
  {
    "step": 11300,
    "epoch": 5.0,
    "loss": 0.4267,
    "grad_norm": 11.1875,
    "learning_rate": 9.747757379052759e-13
  },
  {
    "step": 11300,
    "epoch": 5.0,
    "train_runtime": 9272.7793,
    "train_samples_per_second": 4.873,
    "train_steps_per_second": 1.219,
    "total_flos": 1.899995853014139e+18,
    "train_loss": 0.5183392740983879
  }
]